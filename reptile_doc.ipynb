{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一部分：爬虫\n",
    "我选取了CSDN博客作为爬取对象，由于其首页具有分类功能，并且刷新能够获得新的博客列表，因此我将博客内容爬取分为了两步：第一步，通过selenium.webdriver浏览并刷新主页，获取足够多的不同标签博客，第二步，通过requests方法爬取博客的主要内容及相关信息并存储到本地的json文件\n",
    "\n",
    "以下为第一步代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.remote.webdriver import WebDriver as wd\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait as wdw\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains as AC\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "\n",
    "chrome_options = selenium.webdriver.chrome.options.Options()\n",
    "chrome_options.page_load_strategy='eager'\n",
    "\n",
    "d = selenium.webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get(\"https://blog.csdn.net/nav/web/javascript\")\n",
    "f=open(\"try.txt\",\"a\")\n",
    "for i in range(1,1):\n",
    "    soup1=BS(d.page_source)\n",
    "    divs=soup1.find_all('div',class_=\"active-blog\")\n",
    "    for i in divs:\n",
    "        tmp=i.find('a')['href']\n",
    "        f.write(tmp)\n",
    "        f.write(\"\\n\")\n",
    "        #print(tmp)\n",
    "    d.refresh()\n",
    "    d.implicitly_wait(2)\n",
    "f.close()\n",
    "#获取页面地址并存储到本地try.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：将存储到本地的地址一一打开并get其中的重要信息。我设计了一个处理单个页面的函数dealWebsite，如下图所示。具体细节见其中注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "def dealWebsite(website : str):\n",
    "    blogid=website.split('/')[-1]  #获取页面在原网站独一无二的id\n",
    "\n",
    "    resp1= requests.get(website, headers={\n",
    "        \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:103.0) Gecko/20100101 Firefox/103.0\",\n",
    "        #\"Cookie\":\"log_Id_pv=817; uuid_tt_dd=10_30815930450-1631606403588-112982; log_Id_view=3040; Hm_lvt_6bcd52f51e9b3dce32bec4a3997715ac=1660023423; Hm_up_6bcd52f51e9b3dce32bec4a3997715ac=%7B%22islogin%22%3A%7B%22value%22%3A%221%22%2C%22scope%22%3A1%7D%2C%22isonline%22%3A%7B%22value%22%3A%221%22%2C%22scope%22%3A1%7D%2C%22isvip%22%3A%7B%22value%22%3A%221%22%2C%22scope%22%3A1%7D%2C%22uid_%22%3A%7B%22value%22%3A%22m0_60897957%22%2C%22scope%22%3A1%7D%7D; Hm_ct_6bcd52f51e9b3dce32bec4a3997715ac=6525*1*10_30815930450-1631606403588-112982!5744*1*m0_60897957; __gads=ID=79f4050fafc4d350-226c3361a2cb0048:T=1631606636:RT=1631606636:S=ALNI_MZW5MBbckOmSzJogMbF9rfxXK1fxA; log_Id_click=546; UN=m0_60897957; BT=1654090771391; Hm_lvt_e5ef47b9f471504959267fd614d579cd=1661919860; ssxmod_itna=eq0xgDRDnD0DyGDc0DzEKnx0I7ulWQn7DIowG34GXxP3DZDiqAPGhDC+8FhAR0huGxfI7faqa+7ASRRYYFUmDnQWDCPGnDBKoeWixYAkDt4DTD34DYDixib8xi5GRD0KDFbVztZcDm4i3ExDbDiW8wxGCDeKD0PqFDQKDupKh7hACi45oe3W8D7vmDlae+Ri8Qt8EQlv3wx0kX40OnoHz8xoDU0IzcZ5PdAR4H6PDmZ3Dyrg=Dia8q0iUQD0tEppF8AicDGH9WW23qibxKnDozyDxrC/SK74o3P2xNBiaaCNDi=iZaQGDD==; ssxmod_itna2=eq0xgDRDnD0DyGDc0DzEKnx0I7ulWQn7DIowDA=Weq5D/QYDFOGWrIZPhrzzua4=zqzaT9=+UIr+IKH80rMPux=12CATfbxbwMjj0mr6BjYC=ClM7Lyi0Z1a+ADQKqHDFqG7QeD=; __gpi=UID=0000058ddabffc66:T=1652947691:RT=1662050358:S=ALNI_MZSLyYxJeCTGxyDV8IJ_1REz6BOsA; _ga=GA1.2.1103603590.1653901421; UserName=m0_60897957; UserInfo=74d6c6c3921446faaae7185080c4d866; UserToken=74d6c6c3921446faaae7185080c4d866; UserNick=m0_60897957; AU=A11; p_uid=U110000; c_dl_prid=1660738227694_849081; c_dl_rid=1662051908632_238895; c_dl_fref=https://so.csdn.net/so/search; c_dl_fpage=/download/weixin_38639237/13766436; c_dl_um=distribute.pc_search_result.none-task-blog-2%7Eall%7Etop_positive%7Edefault-2-119912882-null-null.142%5Ev44%5Epc_rank_34_default_23; historyList-new=%5B%22dictionary%20%E8%BD%AC%E5%8C%96%E4%B8%BA%20json%20python%22%2C%22python%20json%20dictionary%22%2C%22python%20json%22%2C%22python%20%E5%82%A8%E5%AD%98%E6%95%B0%E6%8D%AE%E5%88%B0json%22%2C%22C%2B%2B%22%2C%22python%E7%88%AC%E8%99%AB%22%5D; csdn_highschool_close=close; cms_blog_nav_flag=true; c_pref=default; c_ref=default; c_first_ref=default; c_first_page=https%3A//blog.csdn.net/weixin_54696666/article/details/126603219; c_segment=6; Hm_lpvt_6bcd52f51e9b3dce32bec4a3997715ac=1662086441; dc_sid=aaf104ee112f3011cb33ca6d0922c856; SESSION=fe4b6696-0cc1-4808-8889-400605bdfb09; x_dev_cloud_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjcmVkZW50aWFsIjoiIiwiY3NkblVzZXJuYW1lIjoibTBfNjA4OTc5NTciLCJ1c2VySWQiOiI2MmY5ZTgzNTQ2ZDY3ZTIwMDk0ZWNhNTUiLCJ1c2VybmFtZSI6Im0wXzYwODk3OTU3In0.pbgV0w69VSwxPQDvWum3GS8SxKbL_KSGSJSQcFFbNeo; csrfToken=BSRO_7n7e533GISHsdVElWaf; cms_blog_nav={%22cate1%22:%22web%22%2C%22cate2%22:{%22type%22:%22javascript%22%2C%22title%22:%22javascript%22}}; Hm_lpvt_e5ef47b9f471504959267fd614d579cd=1661919872; firstDie=1; FCNEC=[[\"AKsRol_74C20jgrfNJoirUZ55jg24hf8LTHCLZV_PO2rL-LE1SHXbIC5T5hrORsMzkxOd21HICKAwFc8ntpdRtevIh288gguMWE_WwEP3IjMocG2ndStnfljcgkyRUwCT2876Fk0DapNllEVplBqig29Gq1YVs9tVA==\"],null,[]]; dc_session_id=10_1662086377233.161962; c_dsid=11_1662086441212.897845; dc_tos=rhka3t; c_page_id=default\"\n",
    "    })\n",
    "    sp1=BS(resp1.content)  #利用beautifulsoup解析该页面\n",
    "    div_writers=sp1.find(\"div\",id=\"asideProfile\")\n",
    "    if(div_writers==None):\n",
    "        return\n",
    "\n",
    "    #get the writer info\n",
    "    #各个变量含义可直接阅读其变量名\n",
    "\n",
    "    writerPicsURL=div_writers.find('img',class_=\"avatar_pic\").get(\"src\")\n",
    "    writerimagename='profile_'+blogid+'.jpg'\n",
    "    profile_writer=requests.get(writerPicsURL,headers={\n",
    "        \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:103.0) Gecko/20100101 Firefox/103.0\"\n",
    "    })\n",
    "    with open(str(\"writer_image\"+\"\\\\\"+writerimagename),\"wb\") as fprofile:\n",
    "        fprofile.write(profile_writer.content)\n",
    "    \n",
    "    writerName=div_writers.find('a',id=\"uid\",class_=\"\",target=\"_blank\").get(\"title\")\n",
    "    if(div_writers.find('span',class_=\"personal-home-page personal-home-years\")!=None):\n",
    "        writerAge=div_writers.find('span',class_=\"personal-home-page personal-home-years\").string\n",
    "    else:\n",
    "        writerAge=\"None\"\n",
    "    text_centers=div_writers.find_all('dl',class_=\"text-center\")\n",
    "    writerBlogNum=text_centers[0].get(\"title\")\n",
    "    writerRankWeekly=text_centers[1].get(\"title\")\n",
    "    writerRankTotal=text_centers[2].get(\"title\")\n",
    "    writerVisitNum=text_centers[3].get(\"title\")\n",
    "    writerGrade=text_centers[4].get(\"title\").split(',')[0]\n",
    "    writerIntegral=text_centers[5].get(\"title\")\n",
    "    writerFan=text_centers[6].get(\"title\")\n",
    "    writerThumb=text_centers[7].get(\"title\")\n",
    "    writerComment=text_centers[8].get(\"title\")\n",
    "    writerCollect=text_centers[9].get(\"title\")\n",
    "\n",
    "    #get the title\n",
    "\n",
    "    blog_title=sp1.find(\"h1\",id=\"articleContentId\",class_=\"title-article\").string\n",
    "    blog_time=sp1.find(\"span\",class_=\"time\").string\n",
    "    blog_read_count=sp1.find(\"span\",class_=\"read-count\").string\n",
    "\n",
    "    #get the content\n",
    "\n",
    "    content=sp1.find('article',class_=\"baidu_pl\")\n",
    "    content.find('link',rel=\"stylesheet\")['href']=\"style.css\"\n",
    "    images=content.find_all('img')\n",
    "    #置换content中的图片src为本地地址，并将图片保存至本地\n",
    "    for image in images:\n",
    "        srcsite=image.get(\"src\")\n",
    "        if(srcsite==''):\n",
    "            continue\n",
    "        #print(srcsite)\n",
    "        try:\n",
    "            imagenames=srcsite.split('/')\n",
    "            imageself=requests.get(srcsite,headers={\n",
    "                \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:103.0) Gecko/20100101 Firefox/103.0\"\n",
    "            })\n",
    "            for thing in imagenames:\n",
    "                if re.match('.*\\.png',thing):\n",
    "                    imagename=re.match('.*\\.png',thing).group()\n",
    "                    #print(type(imagename))\n",
    "                    #re.sub('.*\\.png',imagename,thing)\n",
    "                    image[\"src\"]=str(\"image\"+\"\\\\\"+imagename)\n",
    "                    with open(str(\"image\"+\"\\\\\"+imagename),\"wb\") as fimage:\n",
    "                        fimage.write(imageself.content)\n",
    "                    break\n",
    "                if re.match('.*\\.jpg',thing):\n",
    "                    imagename=re.match('.*\\.jpg',thing).group()\n",
    "                    #re.sub('.*\\.png',imagename,thing)\n",
    "                    image[\"src\"]=str(\"image\"+\"\\\\\"+imagename)\n",
    "                    with open(str(\"image\\\\\"+imagename),\"wb\") as fimage:\n",
    "                        fimage.write(imageself.content)\n",
    "                    break\n",
    "                if re.match('.*\\.jpeg',thing):\n",
    "                    imagename=re.match('.*\\.jpeg',thing).group()\n",
    "                    #re.sub('.*\\.png',imagename,thing)\n",
    "                    image[\"src\"]=str(\"image\"+\"\\\\\"+imagename)\n",
    "                    with open(str(\"image\\\\\"+imagename),\"wb\") as fimage:\n",
    "                        fimage.write(imageself.content)\n",
    "                    break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    #encoding to json\n",
    "\n",
    "    string_tmp=str(content.contents[1]).replace(u'\\xa0', u' ')\n",
    "    data={\n",
    "        \"blogid\":blogid,\n",
    "        \"writerAge\":writerAge,\n",
    "        \"writerBlogNum\":writerBlogNum,\n",
    "        \"writerCollect\":writerCollect,\n",
    "        \"writerComment\":writerComment,\n",
    "        \"writerFan\":writerFan,\n",
    "        \"writerGrade\":writerGrade,\n",
    "        \"writerIntegral\":writerIntegral,\n",
    "        \"writerName\":writerName,\n",
    "        \"writerProfileAdress\":str(\"writer_image\"+\"\\\\\"+writerimagename),\n",
    "        \"writerRankTotal\":writerRankTotal,\n",
    "        \"writerRankWeekly\":writerRankWeekly,\n",
    "        \"writerThumb\":writerThumb,\n",
    "        \"writerVisitNum\":writerVisitNum,\n",
    "        \"blog_read_count\":blog_read_count,\n",
    "        \"blog_time\":blog_time,\n",
    "        \"blog_title\":blog_title,\n",
    "        \"content\":string_tmp\n",
    "    }\n",
    "    \n",
    "    with open(str(\"blog_json\"+\"\\\\\"+blogid+'.json'),\"w\", encoding = \"utf-8\") as fjson:\n",
    "        json.dump(data,fjson,ensure_ascii = False)\n",
    "    \n",
    "    #最后将这一页面的必要信息储存到json文件中\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用dealWebsite函数获取所有已经获得地址对应的页面数据并进行存储，过程如下一段所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0   #设置num，既是为了实时监测进度，也是为了连接被意外打断后能够在被打断的位置重新开始爬\n",
    "import time\n",
    "with open(\"quchong.txt\",\"r\") as lines:  #quchong.txt是try.txt去重后的版本\n",
    "    for line in lines:\n",
    "        num+=1\n",
    "        if(num>0):\n",
    "            line=line[:-1]\n",
    "            #print(ord(line[-1]))\n",
    "            dealWebsite(line)\n",
    "            print(\"now already finished: \",num)  #实时检测具体已经爬到第几个页面\n",
    "            #print(line)\n",
    "            time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一些后续完善部分，是为了适应网页设计需要。\n",
    "\n",
    "例如：通过正则表达式匹配获取每条博客的标签属性，获取博客的中文文字前一段作为页面展示时的提示语，设置一个“firsttag”展示每条博客的基本标签，如下部分的代码所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from turtle import title\n",
    "import datetime\n",
    "\n",
    "with open(\"quchong.txt\",\"r\") as lines:\n",
    "    for line in lines:\n",
    "        blogid=line.split('/')[-1][:-1]\n",
    "        try:\n",
    "            loadingf=open(str(\"blog_json\\\\\"+blogid+\".json\"),'r',encoding='utf-8')\n",
    "            data=json.load(loadingf)\n",
    "            aaa=data[\"content\"]\n",
    "            aaa=aaa.replace(\"image\\\\\",\"..\\\\..\\\\static\\\\image\\\\\")\n",
    "            #aaa=aaa.replace(\"writer_image\\\\\",\"..\\\\..\\\\static\\\\writer_image\\\\\")\n",
    "            aaa=aaa.replace(\"href=\\\"style.css\\\"\",\"href=\\\"../../static/bootstrap/css/csdnstyle.css\\\"\")\n",
    "            #print(aaa)\n",
    "            data[\"content\"]=aaa\n",
    "\n",
    "            aaa=data[\"writerProfileAdress\"]\n",
    "            aaa=aaa.replace(\"writer_image\\\\\",\"..\\\\..\\\\static\\\\writer_image\\\\\")\n",
    "            data[\"writerProfileAdress\"]=aaa\n",
    "\n",
    "            data[\"first_tag\"]=\"Others\"\n",
    "\n",
    "            pipei=re.compile(r'C\\+\\+',re.IGNORECASE)\n",
    "            if (re.search(pipei,data[\"content\"])==None and re.search(pipei,data[\"blog_title\"])==None):\n",
    "                data[\"cpp\"]=0\n",
    "            else:\n",
    "                data[\"cpp\"]=1\n",
    "                data[\"first_tag\"]=\"C++\"\n",
    "\n",
    "            pipei=re.compile(r'C#',re.IGNORECASE)\n",
    "            if (re.search(pipei,data[\"content\"])==None and re.search(pipei,data[\"blog_title\"])==None):\n",
    "                data[\"csharp\"]=0\n",
    "            else:\n",
    "                data[\"csharp\"]=1\n",
    "                data[\"first_tag\"]=\"C#\"\n",
    "            \n",
    "            pipei=re.compile(r'Python',re.IGNORECASE)\n",
    "            if (re.search(pipei,data[\"content\"])==None and re.search(pipei,data[\"blog_title\"])==None):\n",
    "                data[\"python\"]=0\n",
    "            else:\n",
    "                data[\"python\"]=1\n",
    "                data[\"first_tag\"]=\"Python\"\n",
    "\n",
    "            pipei=re.compile(r'JavaScript',re.IGNORECASE)\n",
    "            if (re.search(pipei,data[\"content\"])==None and re.search(pipei,data[\"blog_title\"])==None):\n",
    "                data[\"javascript\"]=0\n",
    "            else:\n",
    "                data[\"javascript\"]=1\n",
    "                data[\"first_tag\"]=\"JavaScript\"\n",
    "\n",
    "            pipei=re.compile(r'java',re.IGNORECASE)\n",
    "            if (re.search(pipei,data[\"content\"])==None and re.search(pipei,data[\"blog_title\"])==None):\n",
    "                data[\"java\"]=0\n",
    "            elif data[\"javascript\"] == 0 :\n",
    "                data[\"java\"]=1    \n",
    "                data[\"first_tag\"]=\"Java\" \n",
    "            else:\n",
    "                data[\"java\"]=0   \n",
    "\n",
    "            pipei=re.compile(r'SQL',re.IGNORECASE)\n",
    "            if (re.search(pipei,data[\"content\"])==None and re.search(pipei,data[\"blog_title\"])==None):\n",
    "                data[\"sql\"]=0\n",
    "            else:\n",
    "                data[\"sql\"]=1        \n",
    "                data[\"first_tag\"]=\"SQL\"\n",
    "\n",
    "            pipei=re.compile(r'PHP',re.IGNORECASE)\n",
    "            if (re.search(pipei,data[\"content\"])==None and re.search(pipei,data[\"blog_title\"])==None):\n",
    "                data[\"php\"]=0\n",
    "            else:\n",
    "                data[\"php\"]=1\n",
    "                data[\"first_tag\"]=\"PHP\"\n",
    "                \n",
    "            re_time=re.compile('[1-9]\\d{3}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1])\\s+(20|21|22|23|[0-1]\\d):[0-5]\\d:[0-5]\\d')\n",
    "            thistime=re_time.search(data[\"blog_time\"]).group()  \n",
    "            data['time']=thistime\n",
    "\n",
    "            re_ch=re.compile(u'[\\u4e00-\\u9fa5\\，\\。\\！\\？\\、\\；\\》\\《\\：]')\n",
    "            stringzh=data[\"content\"][:5000]\n",
    "            zhresult=re.findall(re_ch,stringzh)\n",
    "            zhresult=''.join(zhresult)[:100]\n",
    "            data['summary']=zhresult\n",
    "\n",
    "            with open(str(\"new_blog_json\\\\\"+blogid+\".json\"),\"w\",encoding=\"utf-8\") as fbloghtml:\n",
    "                json.dump(data,fbloghtml,ensure_ascii = False)\n",
    "        \n",
    "        except:\n",
    "            print(\"error!\")\n",
    "            continue\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以及一开始的网页网址去重工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diction={}\n",
    "fwr=open(\"quchong.txt\",\"a\")\n",
    "with open(\"try.txt\",\"r\") as lines:\n",
    "    for line in lines:\n",
    "        blogid=line.split('/')[-1][:-1]\n",
    "        #print(blogid)\n",
    "        if (blogid in diction) == False :\n",
    "            fwr.write(line)\n",
    "            diction[blogid]=1\n",
    "            print(diction[blogid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬虫的工作到这里就告一段落了，但是这一部分的设计在后面搭建Django的数据库时另外起到了很大作用，因此将数据传输的代码（写在manage.py shell中）也贴在此处方便查阅。（将数据从json中导入到django的model中）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "from blog.models import BlogInfo\n",
    "\n",
    "with open(\"..\\\\Python\\\\quchong.txt\",\"r\") as lines:\n",
    "    point = list([])\n",
    "    for line in lines:\n",
    "        blogid1=line.split('/')[-1][:-1]\n",
    "        if(int(blogid1) in point == True):\n",
    "            print(\"11111111111111111111111111111111111111111\")\n",
    "            continue\n",
    "        else:\n",
    "            point.append(int(blogid1))\n",
    "            try:\n",
    "                loadingf=open(str(\"..\\\\Python\\\\new_blog_json\\\\\"+blogid1+\".json\"),'r',encoding='utf-8')\n",
    "                data=json.load(loadingf)\n",
    "                loadingf.close\n",
    "                print(data['blogid'])\n",
    "                try:\n",
    "                    BlogInfo.objects.create(blogid=data['blogid'],\n",
    "                                            writerAge=data['writerAge'],\n",
    "                                            writerBlogNum=data['writerBlogNum'],\n",
    "                                            writerCollect=data['writerCollect'],\n",
    "                                            writerComment=data['writerComment'],\n",
    "                                            writerFan=data['writerFan'],\n",
    "                                            writerGrade=data['writerGrade'],\n",
    "                                            writerIntegral=data['writerIntegral'],\n",
    "                                            writerName=data['writerName'],\n",
    "                                            writerRankWeekly=data['writerRankWeekly'],\n",
    "                                            writerRankTotal=data['writerRankTotal'],\n",
    "                                            writerProfileAdress=data['writerProfileAdress'],\n",
    "                                            writerThumb=data['writerThumb'],\n",
    "                                            writerVisitNum=data['writerVisitNum'],\n",
    "                                            blog_read_count=data['blog_read_count'],\n",
    "                                            blog_time=data['blog_time'],\n",
    "                                            blog_title=data['blog_title'],\n",
    "                                            content=data['content'],\n",
    "                                            cpp=data['cpp'],\n",
    "                                            csharp=data['csharp'],\n",
    "                                            java=data['java'],\n",
    "                                            javascript=data['javascript'],\n",
    "                                            php=data['php'],\n",
    "                                            sqll=data['sql'],\n",
    "                                            python=data['python'],\n",
    "                                            first_tag=data['first_tag']\n",
    "                                            ,summa=data[\"summary\"][:50],\n",
    "                                            time=data[\"time\"]\n",
    "                                            )\n",
    "                except:\n",
    "                    print(\"Error!\")\n",
    "            except:\n",
    "                continue\n",
    "lines.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1b07af814b6ccfbbee2deb077045b7faf001288fa52b0400ff390fb270d9286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
