<div class="article_content clearfix" id="article_content">
<link href="style.css" rel="stylesheet"/>
<div class="markdown_views prism-kimbie-light" id="content_views">
<svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
<path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
</svg>
<p><font color="brown" size="5">多元线性回归_梯度下降法实现【Python机器学习系列（六）】<br/> </font></p>
<div class="toc">
<h3>文章目录</h3>
<ul><li><a href="#1__14">1. 读取数据</a></li><li><a href="#2_29">2.定义代价函数</a></li><li><a href="#3__60">3. 梯度下降</a></li><li><a href="#4_98">4.可视化展示</a></li></ul>
</div>
<p></p>
<hr/>
<p><font size="5">      ʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞ</font><br/>                  <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/7cc703968fc94dd7a64c66270f4a6193.png"/><img alt="请添加图片描述" src="https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif"/><img alt="请添加图片描述" src="https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif"/><img alt="请添加图片描述" src="https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif"/><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/f9d8225c11fe42508a5bd938f8d99e24.png"/><br/> <font size="5">    ʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞ</font></p>
<hr/>
<p><font color="purple" size="4">大家好，我是侯小啾！</font><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/3ed5e82a513a4a41b019a3b3d747ef7f.png"/></p>
<p><font color="brown" size="4"> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/4e3c21f29fd0442a8bf8b90dcf152434.png"/>本期blog分享的是，python实现多元线性回归的梯度下降法。</font></p>
<hr/>
<h1><a id="1__14"></a>1. 读取数据</h1>
<p><font color="purple" size="4">首先要做的就是读取数据，请自行准备一组适合做多元回归的数据即可。这里以<code>data.csv</code>为例，这里做的是二元回归。导入相关库，及相关代码如下。</font></p>
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">from</span> mpl_toolkits<span class="token punctuation">.</span>mplot3d <span class="token keyword">import</span> Axes3D


data <span class="token operator">=</span> np<span class="token punctuation">.</span>loadtxt<span class="token punctuation">(</span><span class="token string">"data.csv"</span><span class="token punctuation">,</span> delimiter<span class="token operator">=</span><span class="token string">","</span><span class="token punctuation">)</span>
<span class="token comment"># 提取特征数据与标签</span>
x_data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
y_data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
</code></pre>
<hr/>
<h1><a id="2_29"></a>2.定义代价函数</h1>
<p><font size="4">回归模型形如：<br/>  <br/> <font size="5">      <span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          h
         
         
          (
         
         
          x
         
         
          )
         
         
          =
         
         
          t
         
         
          h
         
         
          e
         
         
          t
         
         
          a
         
         
          0
         
         
          +
         
         
          t
         
         
          h
         
         
          e
         
         
          t
         
         
          a
         
         
          1
         
         
          ∗
         
         
          x
         
         
          1
         
         
          +
         
         
          t
         
         
          h
         
         
          e
         
         
          t
         
         
          a
         
         
          2
         
         
          ∗
         
         
          x
         
         
          2
         
        
        
         h(x)=theta0 + theta1*x1 + theta2*x2
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7778em; vertical-align: -0.0833em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord">0</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord mathnormal">x</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord">2</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord mathnormal">x</span><span class="mord">2</span></span></span></span></span></font></font></p>
<p><font color="brown" size="4">接下来我们需要初始化相关参数，并定义出代价函数。因为存在多个系数参数，这里代价函数的写法与一元回归时的情况略有不同，稍微有所调整。具体如下：</font></p>
<pre><code class="prism language-python"><span class="token comment"># 初始化一系列参数</span>
<span class="token comment"># 截距</span>
theta0 <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># 系数</span>
theta1 <span class="token operator">=</span> <span class="token number">0</span>
theta2 <span class="token operator">=</span> <span class="token number">0</span>

<span class="token comment"># 学习率</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.0001</span>
<span class="token comment"># 初始化迭代次数</span>
n_iterables <span class="token operator">=</span> <span class="token number">1000</span>


<span class="token comment"># 定义代价函数（损失函数）</span>
<span class="token keyword">def</span> <span class="token function">compute_mse</span><span class="token punctuation">(</span>theta0<span class="token punctuation">,</span> theta1<span class="token punctuation">,</span> theta2<span class="token punctuation">,</span> x_data<span class="token punctuation">,</span> y_data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    total_error <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>x_data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 计算损失 真实值:y_data  预测值h(x)=theta0 + theta1*x1 + theta2*x2</span>
        total_error <span class="token operator">+=</span> <span class="token punctuation">(</span>y_data<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token punctuation">(</span>theta0 <span class="token operator">+</span> theta1 <span class="token operator">*</span> x_data<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> theta2 <span class="token operator">*</span> x_data<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>

    mse_ <span class="token operator">=</span> total_error <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x_data<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>
    <span class="token keyword">return</span> mse_
</code></pre>
<hr/>
<h1><a id="3__60"></a>3. 梯度下降</h1>
<p><font color="brown" size="4"> 多元回归的梯度下降与一元回归的差不多，在一元回归中只需要求一个导数，而现在求多个偏导数。代码过程如下：</font></p>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">gradient_descent</span><span class="token punctuation">(</span>x_data<span class="token punctuation">,</span> y_data<span class="token punctuation">,</span> theta0<span class="token punctuation">,</span> theta1<span class="token punctuation">,</span> theta2<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> n_iterables<span class="token punctuation">)</span><span class="token punctuation">:</span>
    m <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x_data<span class="token punctuation">)</span>

    <span class="token comment"># 循环 --&gt; 迭代次数</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_iterables<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 初始化 theta0 theta1 theta2 的偏导值</span>
        theta0_grad <span class="token operator">=</span> <span class="token number">0</span>
        theta1_grad <span class="token operator">=</span> <span class="token number">0</span>
        theta2_grad <span class="token operator">=</span> <span class="token number">0</span>

        <span class="token comment"># 计算偏导的总和再平均</span>
        <span class="token comment"># 遍历m次</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
            theta0_grad <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> m<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>theta1 <span class="token operator">*</span> x_data<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> theta2 <span class="token operator">*</span> x_data<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> theta0<span class="token punctuation">)</span> <span class="token operator">-</span> y_data<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span>
            theta1_grad <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> m<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>theta1 <span class="token operator">*</span> x_data<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> theta2 <span class="token operator">*</span> x_data<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> theta0<span class="token punctuation">)</span> <span class="token operator">-</span> y_data<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_data<span class="token punctuation">[</span>
                j<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
            theta2_grad <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> m<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>theta1 <span class="token operator">*</span> x_data<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> theta2 <span class="token operator">*</span> x_data<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> theta0<span class="token punctuation">)</span> <span class="token operator">-</span> y_data<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_data<span class="token punctuation">[</span>
                j<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>

        <span class="token comment"># 更新theta</span>
        theta0 <span class="token operator">=</span> theta0 <span class="token operator">-</span> <span class="token punctuation">(</span>learning_rate <span class="token operator">*</span> theta0_grad<span class="token punctuation">)</span>
        theta1 <span class="token operator">=</span> theta1 <span class="token operator">-</span> <span class="token punctuation">(</span>learning_rate <span class="token operator">*</span> theta1_grad<span class="token punctuation">)</span>
        theta2 <span class="token operator">=</span> theta2 <span class="token operator">-</span> <span class="token punctuation">(</span>learning_rate <span class="token operator">*</span> theta2_grad<span class="token punctuation">)</span>
    <span class="token keyword">return</span> theta0<span class="token punctuation">,</span> theta1<span class="token punctuation">,</span> theta2


<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"开始：截距theta0=</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>theta0<span class="token punctuation">}</span></span><span class="token string">,theta1=</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>theta1<span class="token punctuation">}</span></span><span class="token string">,theta2=</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>theta2<span class="token punctuation">}</span></span><span class="token string">,损失=</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>compute_mse<span class="token punctuation">(</span>theta0<span class="token punctuation">,</span>theta1<span class="token punctuation">,</span>theta2<span class="token punctuation">,</span>x_data<span class="token punctuation">,</span>y_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"开始运行"</span><span class="token punctuation">)</span>
theta0<span class="token punctuation">,</span>theta1<span class="token punctuation">,</span>theta2 <span class="token operator">=</span> gradient_descent<span class="token punctuation">(</span>x_data<span class="token punctuation">,</span>y_data<span class="token punctuation">,</span>theta0<span class="token punctuation">,</span>theta1<span class="token punctuation">,</span>theta2<span class="token punctuation">,</span>learning_rate<span class="token punctuation">,</span>n_iterables<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"迭代</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>n_iterables<span class="token punctuation">}</span></span><span class="token string">次后：截距theta0=</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>theta0<span class="token punctuation">}</span></span><span class="token string">,theta1=</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>theta1<span class="token punctuation">}</span></span><span class="token string">,theta2=</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>theta2<span class="token punctuation">}</span></span><span class="token string">,损失=</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>compute_mse<span class="token punctuation">(</span>theta0<span class="token punctuation">,</span>theta1<span class="token punctuation">,</span>theta2<span class="token punctuation">,</span>x_data<span class="token punctuation">,</span>y_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre>
<p><font color="brown" size="4">执行结果输出如下：<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/0dc1d8e37df7420a9f870ecd377e21ed.png"/><br/> <font color="brown" size="4">1000次迭代之后，损失值由23.64变为0.3865。</font></font></p>
<hr/>
<h1><a id="4_98"></a>4.可视化展示</h1>
<p><font color="brown" size="4">可视化展示常常作为机器学习过程的补充，可以使得机器学习的效果更为生动，直观。</font></p>
<pre><code class="prism language-python"><span class="token comment"># 可视化散点分布</span>
fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>
ax <span class="token operator">=</span> Axes3D<span class="token punctuation">(</span>fig<span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>x_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>y_data<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token comment"># 可视化散点分布</span>
fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>
ax <span class="token operator">=</span> Axes3D<span class="token punctuation">(</span>fig<span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>x_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>y_data<span class="token punctuation">)</span>

<span class="token comment"># 绘制预期平面</span>
<span class="token comment"># 构建x</span>
x_0 <span class="token operator">=</span> x_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span>
x_1 <span class="token operator">=</span> x_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>

<span class="token comment"># 生成网格矩阵</span>
x_0<span class="token punctuation">,</span>x_1 <span class="token operator">=</span> np<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>x_0<span class="token punctuation">,</span>x_1<span class="token punctuation">)</span>

y_hat <span class="token operator">=</span> theta0 <span class="token operator">+</span> theta1<span class="token operator">*</span>x_0 <span class="token operator">+</span> theta2<span class="token operator">*</span>x_1

<span class="token comment"># 绘制3D图</span>
ax<span class="token punctuation">.</span>plot_surface<span class="token punctuation">(</span>x_0<span class="token punctuation">,</span>x_1<span class="token punctuation">,</span>y_hat<span class="token punctuation">)</span>

<span class="token comment"># 设置标签</span>
ax<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"Miles"</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"nums"</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>set_zlabel<span class="token punctuation">(</span><span class="token string">"Time"</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p>散点图输出如下：<br/>         <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/902a21b8100345bba324e88d78aa86b4.png"/></p>
<p><font size="4">加上拟合回归面后如图所示：<br/>         <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/29133f314cd54942b89825dbec7eff32.png"/></font></p>
<hr/>
<p><font color="red" size="5">本次分享就到这里，小啾感谢您的关注与支持！<br/> 🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ</font></p>
</div>
<link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css" rel="stylesheet"/>
<link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css" rel="stylesheet"/>
</div>