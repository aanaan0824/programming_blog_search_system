<div class="article_content clearfix" id="article_content">
<link href="style.css" rel="stylesheet"/>
<div class="markdown_views prism-atom-one-light" id="content_views">
<svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
<path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
</svg>
<p>随着我们爬虫的速度越来越快，很多时候，有人发现，数据爬不了啦，打印出来一看。</p>
<p>不返回数据，而且还甩一句话<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/c8ec302659cb494896ed736d7be6b79b.png"/></p>
<p>是不是很熟悉啊？</p>
<p>要想想看，人是怎么访问网站的？ 发请求，对，那么就会带有</p>
<p>request.headers，</p>
<p>那么当你疯狂请求别人的网站时候，人家网站的管理人员就会 觉得有点不对劲了，</p>
<p>他看看请求的 header 信息，一看吓一跳，结果看到的 headers 信息是这样的：</p>
<p>Host: 127.0.0.1:3369<br/> User-Agent: python-requests/3.21.0<br/> Accept-Encoding: gzip, deflate<br/> Accept: <em>/</em><br/> Connection: keep-alive</p>
<p>看到:</p>
<p>User-Agent: python-requests/3.21.0</p>
<p>居然使用 python 的库来请求，说明你已经暴露了，人家不封你才怪呢？</p>
<p>那么怎么办呢？伪装自己呗。</p>
<p>python 不可以伪装，浏览器可以伪装，所以可以修改浏览器的请求头。</p>
<p>简单来说，就是让自己的 python 爬虫假装是浏览器。</p>
<p>伪装 Header的哪个地方？<br/> 要让自己的 python 爬虫假装是浏览器，我们要伪装headers，那么headers里面有很多字段，我们主要注意那几个呢？<br/> headers数据通常用这两个即可，强烈推荐在爬虫中为每个request都配个user-agent，而’Referer’如果需要就加，不需要就不用。（Referer是什么？后面补充知识点）</p>
<p>图示：<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/c38a5f881dd8417bb3dc2517bab85bab.png"/></p>
<p>上面几个重要点解释如下：</p>
<p>Requests Headers：<br/> • “吾是人！”——修改user-agent：里面储存的是系统和浏览器的型号版本，通过修改它来假装自己是人。</p>
<p>• “我从台湾省来”——修改referer：告诉服务器你是通过哪个网址点进来的而不是凭空出现的，有些网站会检查。</p>
<p>• “饼干！”：——带上cookie，有时带不带饼干得到的结果是不同的，试着带饼干去“贿赂”服务器让她给你完整的信息。<br/> 3.headers的伪装—随机User-Agent<br/> 爬虫机制：很多网站都会对Headers的User-Agent进行检测，还有一部分网站会对Referer进行检测（一些资源网站的防盗链就是检测Referer）</p>
<p>随机User-Agent生成 ：生成一个随机的User-Agent，这样你就可以是很多不同的浏览器模样。</p>
<p>（代码现成的，复制拿去用即可）</p>
<p>#!/usr/bin/python3</p>
<p>#@Readme : 反爬之headers的伪装</p>
<h1><a id="Headers_59"></a>对于检测Headers的反爬虫</h1>
<p>from fake_useragent import UserAgent # 下载：pip install fake-useragent</p>
<p>ua = UserAgent() # 实例化，需要联网但是网站不太稳定-可能耗时会长一些</p>
<h1><a id="1_64"></a>1.生成指定浏览器的请求头</h1>
<p>print(ua.ie)<br/> print(ua.opera)<br/> print(ua.chrome)<br/> print(ua.google)<br/> print(ua.firefox)<br/> print(ua.safari)</p>
<h1><a id="UserAgent_71"></a>随机打印一个浏览器的User-Agent</h1>
<p>print(ua.random)<br/> print(‘完毕。’)</p>
<h1><a id="2uarandom_75"></a>2.在工作中常用的则是ua.random方式</h1>
<p>import requests<br/> ua = UserAgent()<br/> print(ua.random) # 随机产生</p>
<p>headers = {<!-- --><br/> ‘User-Agent’: ua.random # 伪装<br/> }</p>
<h1><a id="_84"></a>请求</h1>
<p>url = ‘https://www.baidu.com/’<br/> response = requests.get(url, headers=headers)<br/> print(response.status_code)</p>
<p>Referer的伪装：</p>
<p>如果想爬图片，图片反盗链的话就要用到Referer了。<br/> headers = {‘User-Agent’:ua.random,‘Referer’:‘这里放入图片的主页面’}<br/> 如果遇到防盗链的图片，一般思路就是先爬到所有图片的地址.jpg —–&gt;将它们储存在列表中 —–&gt;遍历访问图片地址，然后用 ‘wb’的格式打开文件写入，文件名根据图片地址动态改变。</p>
<p>这个基本上如果你的爬虫对象不是很严肃的图片网站，都不会用到。</p>
<p>4.2.1 自建的IP代理池<br/> 多线程爬虫<br/> 就是自己去收集网上公开的免费ip，自建起 自己的ip代理池。<br/> 就是通过 python 程序去抓取网上大量免费的代理 ip ， 然后定时的去检测这些 ip 可不可以用，那么下次你要使用代理 ip 的时候，你只需要去自己的 ip 代理池里面拿就行了。<br/> 简单来说：访问免费代理的网站 —&gt; 正则/xpath提取 ip和端口—&gt; 测试ip是否可用 》》可用则保存 》》使用ip爬虫 &gt; 过期，抛弃ip。</p>
<p>这个过程可以使用多线程或异步的方式，因为检测代理是个很慢的过程。</p>
<p>这是来源于网络的一个西刺代理的多线程ip代理爬虫：（我不用）</p>
<p>#!/usr/bin/python3</p>
<p>#@Readme : IP代理==模拟一个ip地址去访问某个网站（爬的次数太多，ip被屏蔽）</p>
<h1><a id="ip_111"></a>多线程的方式构造ip代理池。</h1>
<p>from bs4 import BeautifulSoup<br/> import requests<br/> from urllib import request, error<br/> import threading</p>
<p>import os<br/> from fake_useragent import UserAgent</p>
<p>inFile = open(‘proxy.txt’) # 存放爬虫下来的ip<br/> verifiedtxt = open(‘verified.txt’) # 存放已证实的可用的ip</p>
<p>lock = threading.Lock()</p>
<p>def getProxy(url):<br/> # 打开我们创建的txt文件<br/> proxyFile = open(‘proxy.txt’, ‘a’)</p>
<pre><code># 伪装
ua = UserAgent()
headers = {
    'User-Agent': ua.random
}

# page是我们需要获取多少页的ip，这里我们获取到第９页
for page in range(1, 10):
    # 通过观察ＵＲＬ，我们发现原网址+页码就是我们需要的网址了，这里的page需要转换成str类型
    urls = url + str(page)
    # 通过requests来获取网页源码
    rsp = requests.get(urls, headers=headers)
    html = rsp.text
    # 通过BeautifulSoup，来解析html页面
    soup = BeautifulSoup(html,'html.parser')
    # 通过分析我们发现数据在　id为ip_list的table标签中的tr标签中
    trs = soup.find('table', id='ip_list').find_all('tr')  # 这里获得的是一个list列表
    # 我们循环这个列表
    for item in trs[1:]:
        # 并至少出每个tr中的所有td标签
        tds = item.find_all('td')
        # 我们会发现有些img标签里面是空的，所以这里我们需要加一个判断
        if tds[0].find('img') is None:
            nation = '未知'
            locate = '未知'
        else:
            nation = tds[0].find('img')['alt'].strip()
            locate = tds[3].text.strip()
        # 通过td列表里面的数据，我们分别把它们提取出来
        ip = tds[1].text.strip()
        port = tds[2].text.strip()
        anony = tds[4].text.strip()
        protocol = tds[5].text.strip()
        speed = tds[6].find('div')['title'].strip()
        time = tds[8].text.strip()
        # 将获取到的数据按照规定格式写入txt文本中，这样方便我们获取
        proxyFile.write('%s|%s|%s|%s|%s|%s|%s|%s\n' % (nation, ip, port, locate, anony, protocol, speed, time))
</code></pre>
<p>def verifyProxyList():<br/> verifiedFile = open(‘verified.txt’, ‘a’)</p>
<pre><code>while True:
    lock.acquire()
    ll = inFile.readline().strip()
    lock.release()
    if len(ll) == 0: break
    line = ll.strip().split('|')
    ip = line[1]
    port = line[2]
    realip = ip + ':' + port
    code = verifyProxy(realip)
    if code == 200:
        lock.acquire()
        print("---Success成功:" + ip + ":" + port)
        verifiedFile.write(ll + "\n")
        lock.release()
    else:
        print("---Failure失败:" + ip + ":" + port)
</code></pre>
<p>def verifyProxy(ip):<br/> ‘’’<br/> 验证代理的有效性<br/> ‘’’<br/> ua = UserAgent()<br/> requestHeader = {<!-- --><br/> ‘User-Agent’: ua.random<br/> }<br/> url = “http://www.baidu.com”<br/> # 填写代理地址<br/> proxy = {‘http’: ip}<br/> # 创建proxyHandler<br/> proxy_handler = request.ProxyHandler(proxy)<br/> # 创建ｏｐｅｎｅｒ<br/> proxy_opener = request.build_opener(proxy_handler)<br/> # 安装opener<br/> request.install_opener(proxy_opener)</p>
<pre><code>try:
    req = request.Request(url, headers=requestHeader)
    rsq = request.urlopen(req, timeout=5.0)
    code = rsq.getcode()
    return code
except error.URLError as e:
    return e
</code></pre>
<p>if <strong>name</strong> == ‘<strong>main</strong>’:<br/> # 手动新建两个文件<br/> filename = ‘proxy.txt’<br/> filename2 = ‘verified.txt’<br/> if not os.path.isfile(filename):<br/> inFile = open(filename, mode=“w”, encoding=“utf-8”)<br/> if not os.path.isfile(filename2):<br/> verifiedtxt = open(filename2, mode=“w”, encoding=“utf-8”)<br/> tmp = open(‘proxy.txt’, ‘w’)<br/> tmp.write(“”)<br/> tmp.close()<br/> tmp1 = open(‘verified.txt’, ‘w’)<br/> tmp1.write(“”)<br/> tmp1.close()<br/> # 多线程爬虫西刺代理网，找可用ip<br/> getProxy(“http://www.xicidaili.com/nn/”)<br/> getProxy(“http://www.xicidaili.com/nt/”)<br/> getProxy(“http://www.xicidaili.com/wn/”)<br/> getProxy(“http://www.xicidaili.com/wt/”)</p>
<pre><code>all_thread = []
for i in range(30):
    t = threading.Thread(target=verifyProxyList)
    all_thread.append(t)
    t.start()

for t in all_thread:
    t.join()

inFile.close()
verifiedtxt.close()
</code></pre>
<p>运行一下，效果：<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/6d72f0c2c19e42ddb9f52d48147b4369.png"/></p>
<p>爬出来的可用的很少或者很短：</p>
<p>重点来了！！！！！！！！！<br/> 4.2.3 开源 ip代理池—ProxyPool（吐血推荐）<br/> 类比线程池，进程池，懂了吧？<br/> 这是俺发现的一个不错的开源 ip 代理池ProxyPool，可以用windows系统的，至少Python3.5以上环境哟，还需要将Redis服务开启。</p>
<p>现成的代理池，还不用起来？</p>
<p>ProxyPool下载地址：</p>
<p>https://github.com/Python3WebSpider/ProxyPool.git</p>
<p>（可以手动下载也可以使用git下来。）</p>
<p>1.ProxyPool的使用：</p>
<p>首先使用 git clone 将源代码拉到你本地，<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/2c1d43be0ffa40a0851ff6dca5e44be8.png"/></p>
<p>3.进入proxypool目录，修改settings.py文件，PASSWORD为Redis密码，如果为空，则设置为None。（新装的redis一般没有密码。）</p>
<p>(如果你没 redis 的话，可以先去下载了安装了再来看吧。)</p>
<p>（假设你的redis已经安装完成。）</p>
<p>4.接着在你 clone 下来的文件目录中（就是这个ProxyPool存的电脑路径 ）</p>
<p>5.安装相关所需的 依赖包：<br/> （pip或pip3）</p>
<p>pip install -r requirements.txt</p>
<p>（如果你把ProxyPool导入在pycharm里面，那就一切都在pycharm里面搞就可以了。<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/838be7cce654407ca87fbf957b9c0d7e.png"/></p>
<p>6.接下来开启你的 redis服务，</p>
<p>直接cmd 打开dos窗口，运行：redis-server.exe<br/> 即可开启redis服务器。redis 的默认端口就是 6379<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/99032eec440c44b8b29115abf2c90161.png"/></p>
<p>7.接着就可以运行 run.py 了。</p>
<p>可以在cmd里面命令方式运行，也可以导入pycharm里面运行。</p>
<p>图示：<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/f2ca576e135a4130b666958dc6ba1193.png"/></p>
<p>8.运行 run.py 以后，你可以打开你的redis管理工具，或者进入redis里面查看，这时候在你的 redis 中就会存入很多已经爬取到的代理 ip 了：<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/559a4095768d4cc7acc6a75ef6158c2d.png"/></p>
<p>9.项目跑起来之后，【不要停止】，此时redis里面存了ip，就可以访问这个代理池了。</p>
<p>在上面的图中，可以看到有这么一句话</p>
<p>Running on http://0.0.0.0:5555/ (Press CTRL+C to quit)<br/> 这就是告诉我们随机访问地址URL是多少。<br/> 10.在浏览器中随机获取一个代理 ip 地址：</p>
<p>你就浏览器输入：</p>
<p>http://0.0.0.0:5555/random<br/> 1<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/8f8e26e525d7403e9fcf90ffad4a7755.png"/></p>
<p>11.在代码中随机获取一个ip代理</p>
<p>就这样：</p>
<p>import requests</p>
<h1><a id="ip_326"></a>随机ip代理获取</h1>
<p>PROXY_POOL_URL = ‘http://localhost:5555/random’<br/> def get_proxy():<br/> try:<br/> response = requests.get(PROXY_POOL_URL)<br/> if response.status_code == 200:<br/> return response.text<br/> except ConnectionError:<br/> return None</p>
<p>if <strong>name</strong> == ‘<strong>main</strong>’:<br/> print(get_proxy())</p>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/92c3c0a6a9e14d649f405e7f5f346416.png"/></p>
<p>好了。到此结束了。</p>
<p>使用这个 ip代理池，目前来说是最好的了，又免费又高效唉~~~</p>
<p>5.报错解决<br/> 安装的时候，如果报错类似于如下：</p>
<p>AttributeError: ‘int’ object has no attribute 'items</p>
<p>更新一下 对应的xxx软件版本，比如redis 版本：</p>
<p>pip install redis==3.33.1</p>
<p>最后给一个REDIS DESKTOP MANAGER安装教程：<br/> Redis 服务安装与配置</p>
<p>1.下载解压</p>
<p>先下载Redis：</p>
<p>https://github.com/ServiceStack/redis-windows/tree/master/downloads<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/b39b616000ad4e0ba578e9bdd1a9e84e.png"/><br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/ac63728071354663926953f40e787cae.png"/></p>
<p>下载完后解压到任意路径下（我的是D:\SoftWare\Redis-3.0）</p>
<p>2.启动redis服务器</p>
<p>进入解压后的文件夹，然后运行redis-server.exe文件。<br/> 　<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/ddb2d4c8122a47edbee3002a88b8b89f.png"/><br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/55247e262e864e0b9b2e62b7324a87c8.png"/></p>
<p>注意：该窗口不可关闭，否则，Redis服务不可用！</p>
<p>现在再从Redis Desktop Manager进行连接就可以成功了！<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/e15d8281f9d241ffb85d1cbf276d7a1a.png"/></p>
<p>3.启动redis客户端</p>
<p>直接双击D:\SoftWare\Redis-3.0目录下的redis-cli.exe文件（redis客户端），如果显示127.0.0.1:6379&gt; ，就说明客户端运行成功。<br/> 　<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/3140c5a0acbd4017a0afc024244eaa69.png"/></p>
<p>4.修改密码</p>
<p>redis默认是空密码，但是这样在项目上线后是不安全的，容易被入侵，所以要设置密码。</p>
<p>1）打开redis.windows.conf文件，找到# requirepass foobared 这行，在此行下增加一行requirepass 所设置的密码 ，保存。</p>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/4a14ff9283b84e4e904913da5dc436fd.png"/></p>
<p>//此处注意，密码自定义就行，并且行前不能有空格！</p>
<p>2）打开cmd(windows命令窗口)，切换到redis-server.exe目录下。</p>
<p>3）输入命令：redis-server.exe redis.windows.conf启动redis，即可使用密码了。<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/27f956dc843f4973807d2ab623cbe1ff.png"/></p>
<p>4）客户端测试：</p>
<p>127.0.0.1:6379&gt; keys *<br/> (empty list or set)<br/> 127.0.0.1:6379&gt; auth redis //会报以下错误<br/> (error) ERR Client sent AUTH, but no password is set<br/> 127.0.0.1:6379&gt; CONFIG SET requirepass “redis” //执行此行指令即可解决错误<br/> OK<br/> 127.0.0.1:6379&gt; auth redis<br/> OK</p>
<p>5.将Redis服务安装到本地服务</p>
<p>由于上述启动Redis服务器的方式有点复杂，且redis服务窗口不可关闭。故这里介绍如何将Redis服务安装到Windows系统的本地服务。</p>
<p>在cmd下输入以下命令：</p>
<p>redis-server --service-install redis.conf --loglevel verbose //安装redis本地服务，指定配置文件redis.windows.conf</p>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/22cd85e57e794e5880e3958021371632.png"/></p>
<p>6.如何卸载Redis本地服务</p>
<p>打开win系统命令行，依次输入下列命令：</p>
<p>C:\Users\lenovo&gt;cd /d D:</p>
<p>D:&gt;cd D:\SoftWare\Redis-3.0</p>
<p>D:\SoftWare\Redis-3.0&gt; redis-server --service-uninstall</p>
</div>
<link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css" rel="stylesheet"/>
<link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css" rel="stylesheet"/>
</div>