<div class="article_content clearfix" id="article_content">
<link href="style.css" rel="stylesheet"/>
<div class="htmledit_views" id="content_views">
<h1 id="%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E5%92%8C%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB" style="text-align:center;">人体姿态估计(人体关键点检测)2D Pose训练代码和Android源码</h1>
<p id="main-toc"><strong>目录</strong></p>
<p id="%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E5%92%8C%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB-toc" style="margin-left:0px;"><a href="#%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E5%92%8C%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB">人体姿态估计(人体关键点检测)2D Pose训练代码和Android源码</a></p>
<p id="1.%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E7%9A%84%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#1.%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E7%9A%84%E6%96%B9%E6%B3%95">1.人体姿态估计2D Pose方法</a></p>
<p id="%C2%A02.%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:40px;"><a href="#%C2%A02.%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E6%95%B0%E6%8D%AE%E9%9B%86">2.人体姿态估计数据集</a></p>
<p id="%EF%BC%881%EF%BC%89COCO%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89COCO%E6%95%B0%E6%8D%AE%E9%9B%86">（1）COCO数据集</a></p>
<p id="%EF%BC%882%EF%BC%89MPII%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:80px;"><a href="#%EF%BC%882%EF%BC%89MPII%E6%95%B0%E6%8D%AE%E9%9B%86">（2）MPII数据集</a></p>
<p id="%EF%BC%883%EF%BC%89%E5%85%B3%E9%94%AE%E7%82%B9%E7%A4%BA%E6%84%8F%E5%9B%BE-toc" style="margin-left:80px;"><a href="#%EF%BC%883%EF%BC%89%E5%85%B3%E9%94%AE%E7%82%B9%E7%A4%BA%E6%84%8F%E5%9B%BE">（3）关键点示意图（ID序号）</a></p>
<p id="2.%E4%BA%BA%E4%BD%93%E5%85%B3%E9%94%AE%E7%82%B9Demo%EF%BC%88Python%E7%89%88%E6%9C%AC%EF%BC%89-toc" style="margin-left:40px;"><a href="#2.%E4%BA%BA%E4%BD%93%E5%85%B3%E9%94%AE%E7%82%B9Demo%EF%BC%88Python%E7%89%88%E6%9C%AC%EF%BC%89">3.人体(行人)检测</a></p>
<p id="4.%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-toc" style="margin-left:40px;"><a href="#4.%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB">4.人体姿态估计训练Training Pipeline</a></p>
<p id="%EF%BC%881%EF%BC%89%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE-toc" style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE">（1）Environment</a></p>
<p id="%EF%BC%882%EF%BC%89%E6%88%B4%E5%8F%A3%E7%BD%A9%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-toc" style="margin-left:80px;"><a href="#%EF%BC%882%EF%BC%89%E6%88%B4%E5%8F%A3%E7%BD%A9%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">（2）数据准备：COCO和MPII数据集</a></p>
<p id="%EF%BC%883%EF%BC%89%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B-toc" style="margin-left:80px;"><a href="#%EF%BC%883%EF%BC%89%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B">（3）模型训练</a></p>
<p id="%EF%BC%884%EF%BC%89%20%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E6%95%88%E6%9E%9C-toc" style="margin-left:80px;"><a href="#%EF%BC%884%EF%BC%89%20%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E6%95%88%E6%9E%9C">（4） 测试Demo</a></p>
<p id="%EF%BC%885%EF%BC%89%20%E6%A3%80%E6%B5%8B%E6%95%88%E6%9E%9C%E5%B1%95%E7%A4%BA-toc" style="margin-left:80px;"><a href="#%EF%BC%885%EF%BC%89%20%E6%A3%80%E6%B5%8B%E6%95%88%E6%9E%9C%E5%B1%95%E7%A4%BA">（5） 检测效果展示</a></p>
<p id="5.%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8BAndroid%E9%83%A8%E7%BD%B2-toc" style="margin-left:40px;"><a href="#5.%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8BAndroid%E9%83%A8%E7%BD%B2">5.人体姿态估计模型Android部署</a></p>
<p id="%EF%BC%884%EF%BC%89%20%E5%B0%86Pytorch%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2ONNX%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#%EF%BC%884%EF%BC%89%20%E5%B0%86Pytorch%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2ONNX%E6%A8%A1%E5%9E%8B">（1） 将Pytorch模型转换ONNX模型</a></p>
<p id="%EF%BC%885%EF%BC%89%20%E5%B0%86ONNX%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BATNN%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#%EF%BC%885%EF%BC%89%20%E5%B0%86ONNX%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BATNN%E6%A8%A1%E5%9E%8B">（2） 将ONNX模型转换为TNN模型</a></p>
<p id="%EF%BC%886%EF%BC%89%20Android%E7%AB%AF%E4%B8%8A%E9%83%A8%E7%BD%B2%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB-toc" style="margin-left:80px;"><a href="#%EF%BC%886%EF%BC%89%20Android%E7%AB%AF%E4%B8%8A%E9%83%A8%E7%BD%B2%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB">（3） Android端上部署人体关键点检测</a></p>
<p id="6.%E9%A1%B9%E7%9B%AE%E6%BA%90%E7%A0%81%E4%B8%8B%E8%BD%BD-toc" style="margin-left:40px;"><a href="#6.%E9%A1%B9%E7%9B%AE%E6%BA%90%E7%A0%81%E4%B8%8B%E8%BD%BD">6.项目源码下载</a></p>
<hr/>
<p><strong>人体关键点检测</strong>（Human Keypoints Detection）又称为<strong>人体姿态估计2D Pose</strong>，是计算机视觉中一个相对基础的任务，是人体动作识别、行为分析、人机交互等的前置任务。一般情况下可以将人体关键点检测细分为单人/多人关键点检测、2D/3D关键点检测，同时有算法在完成关键点检测之后还会进行关键点的跟踪，也被称为人体姿态跟踪。</p>
<p>来，先看个Android Demo的效果图：</p>
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td style="text-align:center;"><strong>视频测试</strong></td><td style="text-align:center;"><strong>图片测试</strong></td><td style="text-align:center;"><strong>图片测试</strong></td></tr><tr><td><img alt="" height="556" src="https://img-blog.csdnimg.cn/20210723091643896.gif" width="250"/></td><td><img alt="" height="557" src="image\e5287531a7604f738dd57013222977cb.png" width="250"/></td><td><img alt="" height="556" src="image\922d670de4dd4d46aee4ea2273681300.png" width="250"/></td></tr></tbody></table>
<p> 整套项目，支持的主要内容主要有： <a href="https://mp.weixin.qq.com/s/iyx937RO3zXXfCaB7nSZrg" title="人体姿态估计(人体关键点检测)2D Pose训练代码和Android源码">人体姿态估计(人体关键点检测)2D Pose训练代码和Android源码</a></p>
<blockquote>
<ol><li>2D Pose人体关键点检测的<strong>训练和测试</strong>代码（Pytorch版本）</li><li>支持COCO数据集训练</li><li>支持MPII数据集训练</li><li>支持轻量化模型mobilenet-v2</li><li>支持高精度模型HRNet</li><li> <p id="3.%E4%BA%BA%E4%BD%93%E5%85%B3%E9%94%AE%E7%82%B9Demo%EF%BC%88C%2B%2B%E7%89%88%E6%9C%AC%EF%BC%89">人体关键点检测C++版本：<a href="https://github.com/PanJinquan/Human-Pose-Estimation-Lite-cpp" title="https://github.com/PanJinquan/Human-Pose-Estimation-Lite-cpp">https://github.com/PanJinquan/Human-Pose-Estimation-Lite-cpp</a></p> </li><li> <p>人体关键点检测Android Demo源码，已经集成了轻量化版本的<code>人体检测模型</code>和<code>人体姿态估计模型</code>，在普通手机可实时检测</p> </li></ol>
</blockquote>
<p>一些项目推荐： </p>
<ol><li> 个人Repo(Python版，不含有训练代码): <a href="https://github.com/PanJinquan/Human-Keypoints-Detection" title="https://github.com/PanJinquan/Human-Keypoints-Detection">https://github.com/PanJinquan/Human-Keypoints-Detection</a></li><li>个人Repo(C++版):<a href="https://github.com/PanJinquan/Human-Pose-Estimation-Lite-cpp" title="https://github.com/PanJinquan/Human-Pose-Estimation-Lite-cpp">https://github.com/PanJinquan/Human-Pose-Estimation-Lite-cpp</a></li><li>2D Pose人体关键点检测Android Demo APP体检（支持CPU多线程和GPU加速，可实时检测）：<a href="https://download.csdn.net/download/guyuealian/85723745" title="Android人体检测和人体关键点检测APPDemo安装包-Android文档类资源-CSDN下载">Android人体检测和人体关键点检测APPDemo安装包-Android文档类资源-CSDN下载</a></li><li>2D Pose人体关键点检测(Human Keypoints Detection)训练代码和Android源码：<a href="https://mp.weixin.qq.com/s/iyx937RO3zXXfCaB7nSZrg" title="人体姿态估计(人体关键点检测)2D Pose训练代码和Android源码">人体姿态估计(人体关键点检测)2D Pose训练代码和Android源码</a></li></ol>
<hr/>
<h2 id="1.%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E7%9A%84%E6%96%B9%E6%B3%95">1.<strong>人体姿态估计2D Pose方法</strong></h2>
<div>
<p><span style="color:#000000;">目前，人体姿态估计的主流框架，主要有两个方法</span></p>
<p><span style="color:#000000;"><strong><strong>（1）Top-Down</strong></strong>（自上而下）方法</span></p>
<blockquote>
<p><span style="color:#000000;">将人体检测和关键点检测分离，在图像上首先进行人体检测，找到所有的人体框，对每个人体框图再使用关键点检测，这类方法往往比较慢，但姿态估计准确度较高。目前的主流是CPN，Hourglass，CPM，Alpha Pose等。</span></p>
</blockquote>
<p><span style="color:#000000;"><strong><strong>（2）Bottom-Up</strong></strong>（自下而上）方法</span></p>
<blockquote>
<p><span style="color:#000000;">先检测图像中人体部件，然后将图像中多人人体的部件分别组合成人体，因此这类方法在测试推断的时候往往更快速，准确度稍低。典型就是COCO2016年人体关键点检测冠军Open Pose。</span></p>
</blockquote>
</div>
<p>就目前调研而言， <span style="color:#000000;">Top-Down的方法研究较多，精度也比<strong><strong>Bottom-Up</strong></strong>（自下而上）方法好。本项目也主要分享基于<strong><strong>Top-Down</strong></strong>（自上而下）的方法。</span></p>
<hr/>
<h2 id="%C2%A02.%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E6%95%B0%E6%8D%AE%E9%9B%86">2.<strong>人体姿态估计</strong>数据集</h2>
<h3 id="%EF%BC%881%EF%BC%89COCO%E6%95%B0%E6%8D%AE%E9%9B%86">（1）COCO数据集</h3>
<blockquote>
<p>下载地址：http://cocodataset.org/</p>
<p>COCO人体关键点标注，最多标注全身的17个关键点，平均一幅图像2个人，最多有13个人；</p>
<p>人体关键点标注，每个人体关键点个数的分布情况，其中11-15这个范围的人体是最多的，有接近70000人，6-10其次，超过40000人，后面依次为16-17,2-5,1.</p>
</blockquote>
<h3 id="%EF%BC%882%EF%BC%89MPII%E6%95%B0%E6%8D%AE%E9%9B%86">（2）MPII数据集</h3>
<blockquote>
<p>下载地址：http://human-pose.mpi-inf.mpg.de/#download</p>
<p> 人体关键点标注了全身16个关键点及其是否可见的信息，人数：train有28821，test有11701，有409种人类活动；使用mat的struct格式；行人框使用center和scale标注，人体尺度关于200像素高度。也就是除过了200</p>
</blockquote>
<h3 id="%EF%BC%883%EF%BC%89%E5%85%B3%E9%94%AE%E7%82%B9%E7%A4%BA%E6%84%8F%E5%9B%BE"><strong>（3）关键点示意图（ID序号）</strong></h3>
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:700px;"><tbody><tr><td>数据集</td><td style="width:198px;">关键点示意图          </td><td style="width:394px;">关键点说明</td></tr><tr><td>COCO</td><td style="width:198px;"><img alt="" height="584" src="image\20210416164539283.png" width="300"/>​</td><td style="width:394px;"> <p># 图像左右翻转时，成对的关键点（训练时用于数据增强）</p> <p>flip_pairs=[[1, 2], [3, 4], [5, 6], [7, 8],[9, 10], [11, 12], [13, 14], [15, 16]]</p> <p></p> <p># 关键点连接线序号（用于绘制图像）</p> <p>skeleton =[[15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7], [6, 8], [7, 9], [8, 10], [0, 1], [0, 2], [1, 3], [2, 4]]</p> <p></p> <p># 每个关键点序号对应人体关键点的意义</p> <p>"keypoints": { 0: "nose", 1: "left_eye", 2: "right_eye", 3: "left_ear", 4: "right_ear", 5: "left_shoulder", 6: "right_shoulder", 7: "left_elbow", 8: "right_elbow", 9: "left_wrist", 10: "right_wrist", 11: "left_hip", 12: "right_hip", 13: "left_knee", 14: "right_knee", 15: "left_ankle", 16: "right_ankle" }</p> </td></tr><tr><td>MPII</td><td style="width:198px;"><img alt="" height="606" src="image\20210416165414715.png" width="300"/>​</td><td style="width:394px;"> <p># 图像左右翻转时，成对的关键点（训练时用于数据增强）</p> <p>flip_pairs = [[0, 5], [1, 4], [2, 3], [10, 15], [11, 14], [12, 13]]</p> <p># 关键点连接线序号（用于绘制图像）</p> <p>skeleton=[[0, 1], [1, 2], [3, 4], [4, 5], [2, 6], [6, 3], [12, 11], [7, 12], [11, 10], [13, 14], [14, 15], [8, 9], [8, 7], [6, 7], [7, 13]]</p> <p># 每个关键点序号对应人体关键点的意义</p> <p>"keypoints": {0: "r_ankle", 1: "r_knee", 2: "r_hip", 3: "l_hip", 4: "l_knee", 5: "l_ankle", 6: "pelvis", 7: "thorax", 8: "upper_neck", 9: "head_top", 10: " r_wrist", 11: "r_elbow", 12: "r_shoulder", 13: "l_shoulder", 14: "l_elbow", 15: "l_wrist" }</p> </td></tr><tr><td>human3.6M</td><td style="width:198px;"><img alt="" height="314" src="image\20210416165708163.png" width="300"/>​</td><td style="width:394px;"></td></tr><tr><td>kinect</td><td style="width:198px;"><img alt="" height="299" src="image\20210416165742294.png" width="300"/>​</td><td style="width:394px;">
<div>
<pre><code class="language-html">JointType_SpineBase = 0 # 脊柱底
JointType_SpineMid = 1 # 脊柱中间
JointType_Neck = 2 # 脖子
JointType_Head = 3 # 额头
JointType_ShoulderLeft = 4
JointType_ElbowLeft = 5
JointType_WristLeft = 6
JointType_HandLeft = 7
JointType_ShoulderRight = 8
JointType_ElbowRight = 9
JointType_WristRight = 10
JointType_HandRight = 11
JointType_HipLeft = 12
JointType_KneeLeft = 13
JointType_AnkleLeft = 14
JointType_FootLeft = 15
JointType_HipRight = 16
JointType_KneeRight = 17
JointType_AnkleRight = 18
JointType_FootRight = 19
JointType_SpineShoulder = 20
JointType_HandTipLeft = 21
JointType_ThumbLeft = 22
JointType_HandTipRight = 23
JointType_ThumbRight = 24
JointType_Count = 25</code></pre>
</div> </td></tr></tbody></table>
<hr/>
<h2 id="2.%E4%BA%BA%E4%BD%93%E5%85%B3%E9%94%AE%E7%82%B9Demo%EF%BC%88Python%E7%89%88%E6%9C%AC%EF%BC%89">3.人体(行人)检测</h2>
<p><span style="color:#000000;"><strong>基于Top-Down（自上而下）人体关键点检测方法</strong>，</span><span style="color:#000000;">将人体检测和关键点检测分离；首先在图像上进行人体检测，找到所有的人体框，然后对每个人体框区域进行关键点检测，这类方法往往比较慢，但姿态估计准确度较高。目前的主流是CPN，Hourglass，CPM，Alpha Pose等。</span></p>
<p>人脸检测的方法很多，你可以基于YOLO，SSD模型训练一个通用人体检测模型即可，本篇博客不做重点介绍，感兴趣的话，可以看一下我的另一篇博客《<strong>人检测（人体检测）和人脸检测和人脸关键点检测（C++/Android）</strong>》</p>
<p><a class="has-card" href="https://blog.csdn.net/guyuealian/article/details/125348189" title="又快又好，行人检测和人脸检测和人脸关键点检测（C++/Android源码）_pan_jinquan的博客-CSDN博客"><span class="link-card-box"><span class="link-title">又快又好，行人检测和人脸检测和人脸关键点检测（C++/Android源码）_pan_jinquan的博客-CSDN博客</span><span class="link-desc">考虑到人脸人体检测的需求，本人开发了一套轻量化的，高精度的，可实时的人脸/人体检测Android Demo，主要支持功能如下：支持人脸检测算法模型支持人脸检测和人脸关键点检测(5个人脸关键点)算法模型支持人体检测(行人检测)算法模型支持人脸和人体同时检测算法模型所有算法模型都使用C++开发，推理框架采用TNN，Android通过JNI接口进行算法调用；所有算法模型都可在普通Android手机实时跑，在普通Android手机，CPU和GPU都可以达到实时检测的效果（CPU约25毫秒左右，GPU约1</span><span class="link-link"><img alt="" class="link-link-icon" src="https://g.csdnimg.cn/static/logo/favicon32.ico"/>https://blog.csdn.net/guyuealian/article/details/125348189</span></span></a></p>
<hr/>
<h2 id="4.%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB">4.<strong>人体姿态估计</strong>训练Training Pipeline</h2>
<p><strong>人体姿态估计</strong>训练Pipeline,整套工程项目,基本结构如下：</p>
<div>
<div>
<pre><code class="language-bash">├── configs                          # 配置文件
├── data                             # 数据文件等
├── models                           # models核心代码
├── docker                           # docker配置文件
├── docs                             # 说明文档
├── libs                             # 第三方库
├── work_dir                         # 训练输出保存目录
├── scripts                          # 脚本文件
├── demo.py                          # 推理的demo文件
├── test.py                          # 测试文件
├── train.py                         # 训练文件
├── requirements.txt                 # 依赖包
└── README.md                        # README文件</code></pre>
</div>
</div>
<h3 id="%EF%BC%881%EF%BC%89%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE">（1）Environment</h3>
<p>python依赖包，可参考[requirements.txt]</p>
<div>
<pre><code class="language-bash">numpy==1.16.3
matplotlib==3.1.0
Pillow==6.0.0
bcolz==1.2.1
easydict==1.9
opencv-contrib-python==4.5.2.52
opencv-python==4.5.1.48
pandas==1.1.5
PyYAML==5.3.1
scikit-image==0.17.2
scikit-learn==0.24.0
scipy==1.5.4
seaborn==0.11.2
sklearn==0.0
tensorboard==2.5.0
tensorboardX==2.1
torch==1.7.1+cu110
torchvision==0.8.2+cu110
tqdm==4.55.1
xmltodict==0.12.0
memory_profiler
dldtrainer</code></pre>
</div>
<h3 id="%EF%BC%882%EF%BC%89%E6%88%B4%E5%8F%A3%E7%BD%A9%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">（2）数据准备：<strong>COCO和MPII数据集</strong></h3>
<p><strong>COCO数据集</strong></p>
<ul><li>下载地址： <a href="https://cocodataset.org/#download" title="COCO - Common Objects in Context">COCO - Common Objects in Context</a></li><li>解压后，并保存如下的文件结构</li></ul>
<div>
<pre><code class="language-bash">COCO
├── train2017
│     ├── annotations
│     │       ├── instances_train2017.json
│     │       └── person_keypoints_train2017.json
│     └── images
│              ├── 000000001.jpg
│              ├── 000000002.jpg
│              ├── 000000003.jpg
└── val2017
      ├── annotations
      │        ├── instances_val2017.json
      │        └── person_keypoints_val2017.json
      └── images
                ├── 000000001.jpg
                ├── 000000002.jpg
                ├── 000000003.jpg</code></pre>
</div>
<ul><li>修改配置文件的数据根目录，如train_model_mbv2_penson.yaml</li><li>其他配置文件，也可以如下修改</li></ul>
<div>
<pre><code class="language-bash">DATASET:
  DATASET: 'person_coco'
  ROOT: '/path/to/yours/dataset/COCO'  
.....</code></pre>
</div>
<p><strong>MPII数据集</strong></p>
<ul><li>下载地址： <a href="http://human-pose.mpi-inf.mpg.de/#download" title="MPII Human Pose Database">MPII Human Pose Database</a></li><li>原始数据集的标注是Matlab格式，使用前需要转换json格式: 下载地址： <a href="https://drive.google.com/drive/folders/1En_VqmStnsXMdldXA6qpqEyDQulnmS3a" title="GoogleDrive">GoogleDrive</a></li><li>解压后，并保存如下的文件结构</li></ul>
<div>
<pre><code class="language-bash">MPII
├── annot
│     ├── gt_valid.mat
│     ├── test.json
│     ├── train.json
│     ├── trainval.json
│     └── valid.json
└── images
       ├── 000000001.jpg
       ├── 000000002.jpg
       ├── 000000003.jpg</code></pre>
</div>
<ul><li>修改配置文件的数据根目录，如train_model_mbv2_penson.yaml</li><li>其他配置文件，也可以如下修改</li></ul>
<div>
<pre><code class="language-bash">DATASET:
  DATASET: 'custom_mpii'
  ROOT: '/path/to/yours/dataset/MPII'  
....</code></pre>
</div>
<h3 id="%EF%BC%883%EF%BC%89%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B">（3）模型训练</h3>
<pre>修改配置文件的数据根目录后，就可以开始训练，训练的基本命令如下
</pre>
<div>
<pre><code class="language-bash">bash build.sh # 训练之前，需要先编译工程
python train.py  -c path/to/config.yaml --gpu_id 0 </code></pre>
</div>
<p><strong>训练COCO人体关键点：</strong></p>
<div>
<pre><code class="language-bash"># 轻量化模型：mobilenet
python train.py  -c configs/coco/mobilenet/train_model_mbv2_penson.yaml --gpu_id 0 
# 高精度模型：HRNet
python train.py  -c configs/coco/hrnet/w48_adam_penson.yaml --gpu_id 0</code></pre>
</div>
<p><strong>训练MPII人体关键点：</strong></p>
<div>
<pre><code class="language-bash"># 轻量化模型：mobilenet
python train.py  -c configs/mpii/mobilenet/train_model_mbv2_penson.yaml --gpu_id 0
# 高精度模型：HRNet
python train.py  --c configs/mpii/hrnet/w48_adam_penson.yaml --gpu_id 0</code></pre>
</div>
<h3 id="%EF%BC%884%EF%BC%89%20%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E6%95%88%E6%9E%9C">（4） 测试Demo</h3>
<p>运行Demo测试效果</p>
<div>
<pre><code class="language-bash"># 运行Demo测试效果
bash demo.sh
# 或者通过命令行
# 测试图片
python demo.py \
  -c data/pretrained/model_mobilenet_v2/train_model_mbv2_penson.yaml \
  -m data/pretrained/model_mobilenet_v2/best_model_178_0.6272.pth \
  --skeleton coco \
  --image_dir data/test_image \
  --save_dir data/result_image

# 测试视频
python demo.py \
  -c data/pretrained/model_mobilenet_v2/train_model_mbv2_penson.yaml \
  -m data/pretrained/model_mobilenet_v2/best_model_178_0.6272.pth \
  --skeleton coco \
  --video_path data/videos/kunkun_cut.mp4 \
  --video_save data/videos/kunkun_cut_result.mp4</code></pre>
</div>
<ul><li>可根据自己的需要，修改demo.sh</li><li>Demo参数说明如下</li></ul>
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:700px;"><tbody><tr><td style="text-align:center;"><strong>参数</strong></td><td style="text-align:center;"><strong>类型</strong></td><td style="text-align:center;width:164px;"><strong>参考值</strong></td><td style="text-align:center;width:212px;"><strong>说明</strong></td></tr><tr><td>c,config_file</td><td>str</td><td style="width:164px;"></td><td style="width:212px;">配置文件</td></tr><tr><td>m,model_file </td><td>str</td><td style="width:164px;"></td><td style="width:212px;">模型文件</td></tr><tr><td>skeleton</td><td>str,list</td><td style="width:164px;">mpii</td><td style="width:212px;">骨骼点类型，如mpii,coco</td></tr><tr><td>image_dir</td><td>str</td><td style="width:164px;">data/test_image</td><td style="width:212px;">测试图片的路径</td></tr><tr><td>save_dir</td><td>str</td><td style="width:164px;">data/result_image</td><td style="width:212px;">保存结果，为空不保存</td></tr><tr><td>video_path</td><td>str</td><td style="width:164px;"></td><td style="width:212px;">测试的视频文件</td></tr><tr><td>video_save</td><td>str</td><td style="width:164px;"></td><td style="width:212px;">保存视频文件路径</td></tr><tr><td>detect_person</td><td>bool</td><td style="width:164px;">True</td><td style="width:212px;">是否检测人体检测，默认True</td></tr><tr><td>threshhold</td><td>float</td><td style="width:164px;">0.3</td><td style="width:212px;">关键点检测置信度</td></tr><tr><td>device</td><td>str</td><td style="width:164px;">cuda:0 </td><td style="width:212px;">GPU ID</td></tr></tbody></table>
<h3 id="%EF%BC%885%EF%BC%89%20%E6%A3%80%E6%B5%8B%E6%95%88%E6%9E%9C%E5%B1%95%E7%A4%BA">（5） 检测效果展示</h3>
<table border="1" cellpadding="1" cellspacing="1" style="width:700px;"><tbody><tr><td style="width:382px;"><img alt="" height="573" src="image\3c278a81f6284c639b08ba06d8c1247b.png" width="350"/>​</td><td style="width:316px;"><img alt="" height="495" src="image\9a5cf079157a4a25b76dafc302e1e921.png" width="350"/>​</td></tr><tr><td style="width:382px;"><img alt="" height="756" src="image\14bd0959494545379f7a6366912a018f.png" width="948"/>​</td><td style="width:316px;"><img alt="" src="image\20210416162608200.png"/>​</td></tr></tbody></table>
<p></p>
<h2 id="5.%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8BAndroid%E9%83%A8%E7%BD%B2">5.<strong>人体姿态估计</strong>模型Android部署</h2>
<h3 id="%EF%BC%884%EF%BC%89%20%E5%B0%86Pytorch%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2ONNX%E6%A8%A1%E5%9E%8B">（1） 将Pytorch模型转换ONNX模型</h3>
<p>训练好Pytorch模型后，你可以将模型转换为ONNX模型，方便后续模型部署</p>
<div>
<div>
<pre><code class="language-bash">python utils/convert_tools/convert_torch_to_onnx.py</code></pre>
</div>
</div>
<h3 id="%EF%BC%885%EF%BC%89%20%E5%B0%86ONNX%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BATNN%E6%A8%A1%E5%9E%8B">（2） 将ONNX模型转换为TNN模型</h3>
<p>目前CNN模型有多种部署方式，可以采用TNN，MNN,NCNN，以及TensorRT等部署工具，鄙人采用TNN进行Android端上部署：</p>
<blockquote>
<p>将ONNX模型转换为TNN模型，请参考TNN官方说明：</p>
<p><a href="https://github.com/Tencent/TNN/blob/master/doc/cn/user/onnx2tnn.md" title="TNN/onnx2tnn.md at master · Tencent/TNN · GitHub">TNN/onnx2tnn.md at master · Tencent/TNN · GitHub</a></p>
</blockquote>
<h3 id="%EF%BC%886%EF%BC%89%20Android%E7%AB%AF%E4%B8%8A%E9%83%A8%E7%BD%B2%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB">（3） Android端上部署人体关键点检测</h3>
<p>项目实现了Android版本的2D Pose人体关键点检测Demo，部署框架采用TNN，支持多线程CPU和GPU加速推理，在普通手机上可以实时处理。2D Pose人体关键点检测Android源码，核心算法均采用C++实现，上层通过JNI接口调用.</p>
<blockquote>
<p>如果你想在这个Android Demo部署你自己训练的分类模型，你可将训练好的Pytorch模型转换ONNX ，再转换成TNN模型，然后把TNN模型代替你模型即可。</p>
</blockquote>
<div>
<div>
<pre><code class="language-java">package com.cv.tnn.model;

import android.graphics.Bitmap;

public class Detector {

    static {
        System.loadLibrary("tnn_wrapper");
    }


    /***
     * 初始化关键点检测模型
     * @param proto： TNN *.tnnproto文件文件名（含后缀名）
     * @param model： TNN *.tnnmodel文件文件名（含后缀名）
     * @param root：模型文件的根目录，放在assets文件夹下
     * @param model_type：模型类型
     * @param num_thread：开启线程数
     * @param useGPU：关键点的置信度，小于值的坐标会置-1
     */
    public static native void init(String proto, String model, String root, int model_type, int num_thread, boolean useGPU);

    /***
     * 检测关键点
     * @param bitmap 图像（bitmap），ARGB_8888格式
     * @param threshold：关键点的置信度，小于值的坐标会置-1
     * @return
     */
    public static native FrameInfo[] detect(Bitmap bitmap, float threshold);
}

</code></pre>
</div>
</div>
<hr/>
<h2 id="6.%E9%A1%B9%E7%9B%AE%E6%BA%90%E7%A0%81%E4%B8%8B%E8%BD%BD">6.项目源码下载</h2>
<p> 整套项目源码《<a href="https://mp.weixin.qq.com/s/iyx937RO3zXXfCaB7nSZrg" title="人体姿态估计(人体关键点检测)2D Pose训练代码和Android源码">人体姿态估计(人体关键点检测)2D Pose训练代码和Android源码</a>》，主要内容包含有：</p>
<p><img alt="" height="205" src="image\ee23a826795a4f7c9ec1d86cf73e93a8.png" width="614"/></p>
<blockquote>
<ol><li>2D Pose人体关键点检测的<strong>训练和测试</strong>代码（Pytorch版本）</li><li>支持COCO数据集训练</li><li>支持MPII数据集训练</li><li>支持轻量化模型mobilenet-v2</li><li>支持高精度模型HRNet</li><li> <p id="3.%E4%BA%BA%E4%BD%93%E5%85%B3%E9%94%AE%E7%82%B9Demo%EF%BC%88C%2B%2B%E7%89%88%E6%9C%AC%EF%BC%89">人体关键点检测C++版本：<a href="https://github.com/PanJinquan/Human-Pose-Estimation-Lite-cpp" title="https://github.com/PanJinquan/Human-Pose-Estimation-Lite-cpp">https://github.com/PanJinquan/Human-Pose-Estimation-Lite-cpp</a></p> </li><li>人体关键点检测Android Demo源码，已经集成了轻量化版本的<code>人体检测模型</code>和<code>人体姿态估计模型</code>，在普通手机可实时检测</li></ol>
</blockquote>
<p> 如果，你不需要Python训练代码，仅考虑C++和Android Demo，可参考《2D Pose人体关键点实时检测(Python/Android /C++ Demo)》：</p>
<p><a class="has-card" href="https://panjinquan.blog.csdn.net/article/details/115765863" title="2D Pose人体关键点实时检测(Python/Android /C++ Demo)_pan_jinquan的博客-CSDN博客_python人体检测代码"><span class="link-card-box"><span class="link-title">2D Pose人体关键点实时检测(Python/Android /C++ Demo)_pan_jinquan的博客-CSDN博客_python人体检测代码</span><span class="link-desc">​人体关键点检测（Human Keypoints Detection）又称为人体姿态估计2D Pose，是计算机视觉中一个相对基础的任务，是人体动作识别、行为分析、人机交互等的前置任务。一般情况下可以将人体关键点检测细分为单人/多人关键点检测、2D/3D关键点检测，同时有算法在完成关键点检测之后还会进行关键点的跟踪，也被称为人体姿态跟踪。本博客提供2D Pose的Python代码，以及C++版本的推理代码，还提供Android Demo APP，已经集成了轻量化版本的人体检测模型和人体姿态估计模型，在</span><span class="link-link"><img alt="" class="link-link-icon" src="https://g.csdnimg.cn/static/logo/favicon32.ico"/>https://panjinquan.blog.csdn.net/article/details/115765863</span></span></a></p>
</div>
</div>