<div class="article_content clearfix" id="article_content">
<link href="style.css" rel="stylesheet"/>
<div class="markdown_views prism-atom-one-light" id="content_views">
<svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
<path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
</svg>
<p></p>
<div class="toc">
<h3>文章目录</h3>
<ul><li><a href="#1%09_1">1 原理简述</a></li><li><a href="#2%09PyTorch__21">2 PyTorch 实现</a></li></ul>
</div>
<p></p>
<h1><a id="1%09_1"></a>1 原理简述</h1>
<p>  Self-Attention Layer 一次检查同一句子中的所有单词的注意力，这使得它成为一个简单的矩阵计算，并且能够在计算单元上并行计算。 此外，Self-Attention Layer 可以使用下面提到的 Multi-Head 架构来拓宽视野，也就是多头注意力机制。Self-Attention Layer 基本结构如下：<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/fc65b9f0024549318aad9019931c293a.png"/></p>
<p>对于每个输入 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        x
       
      
      
       \boldsymbol{x}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4444em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span></span></span>，首先经过 <strong>Embedding</strong> 层对每个输入进行编码得到 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           a
          
          
           1
          
         
         
          ,
         
         
          
           a
          
          
           2
          
         
         
          ,
         
         
          
           a
          
          
           3
          
         
         
          ,
         
         
          
           a
          
          
           4
          
         
        
       
      
      
       \boldsymbol{a_1,a_2,a_3,a_4}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6389em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathbf mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathbf mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathbf mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathbf mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span>，后将输入特征经过三个全连接层分别得到 <strong>Query，Key，Value</strong>：</p>
<ul><li><span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          
           
            q
           
           
            i
           
          
          
           (
          
          
           Q
          
          
           u
          
          
           e
          
          
           r
          
          
           y
          
          
           )
          
          
           =
          
          
           
            W
           
           
            q
           
          
          
           
            a
           
           
            i
           
          
         
        
       
       
        \boldsymbol{q^i(Query) = W^q a^i}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0983em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.037em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span><span class="mopen mathbf">(</span><span class="mord boldsymbol" style="margin-right: 0.037em;">Query</span><span class="mclose mathbf">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel mathbf">=</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">q</span></span></span></span></span></span></span></span><span class="mord"><span class="mord boldsymbol">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span></span>；</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          
           
            k
           
           
            i
           
          
          
           (
          
          
           K
          
          
           e
          
          
           y
          
          
           )
          
          
           =
          
          
           
            W
           
           
            k
           
          
          
           
            a
           
           
            i
           
          
         
        
       
       
        \boldsymbol{k^i(Key) = W^k a^i}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0991em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.0185em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span><span class="mopen mathbf">(</span><span class="mord boldsymbol" style="margin-right: 0.037em;">Key</span><span class="mclose mathbf">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel mathbf">=</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8491em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.0185em;">k</span></span></span></span></span></span></span></span><span class="mord"><span class="mord boldsymbol">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span></span>；</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          
           
            v
           
           
            i
           
          
          
           (
          
          
           V
          
          
           a
          
          
           l
          
          
           u
          
          
           e
          
          
           )
          
          
           =
          
          
           
            W
           
           
            v
           
          
          
           
            a
           
           
            i
           
          
         
        
       
       
        \boldsymbol{v^i(Value) = W^v a^i}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0983em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.037em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span><span class="mopen mathbf">(</span><span class="mord boldsymbol">Value</span><span class="mclose mathbf">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel mathbf">=</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">v</span></span></span></span></span></span></span></span><span class="mord"><span class="mord boldsymbol">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span></span>。</li></ul>
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           W
          
          
           q
          
         
         
          ,
         
         
          
           W
          
          
           k
          
         
         
          ,
         
         
          
           W
          
          
           v
          
         
        
       
      
      
       \boldsymbol{W^q, W^k,W^v}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0435em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">q</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8491em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.0185em;">k</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">v</span></span></span></span></span></span></span></span></span></span></span></span></span></span> 由网络训练而来。注意力矩阵是由 Query 和 Key 计算得到，方式由许多种，如点积、缩放点积等。Value 可以看作是信息提取器，将根据单词的注意力提取一个唯一的值，也即某个特征有多少成分被提取出来。下面计算一种注意力矩阵的方式：缩放点积。<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/6385301128964680b30be08f2ac35638.gif#pic_center"/><br/> 注意力矩阵 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        A
       
      
      
       \boldsymbol{A}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6861em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">A</span></span></span></span></span></span></span> 定义为 Query (giver) 和 Key (receiver) 的内积除以其维度的平方根。 每个单词通过提供 Query 来匹配作为注意力的目标单词的 Key，从而对所有单词产生注意力。为防止注意力分数随维度增大而增大，让注意力矩阵除以向量的维度的开方。 然后对得到的注意力矩阵 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        A
       
      
      
       \boldsymbol{A}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6861em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">A</span></span></span></span></span></span></span> 进行 <strong>Softmax</strong> 归一化得到 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          A
         
         
          ^
         
        
       
      
      
       \boldsymbol{\hat{A}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9495em;"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord boldsymbol">A</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.2875em;"><span class="mord mathbf">^</span></span></span></span></span></span></span></span></span></span></span></span></span>，最后将 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          A
         
         
          ^
         
        
       
      
      
       \boldsymbol{\hat{A}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9495em;"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord boldsymbol">A</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.2875em;"><span class="mord mathbf">^</span></span></span></span></span></span></span></span></span></span></span></span></span> 乘以 Value 矩阵并相加得到最终的特征 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        b
       
      
      
       \boldsymbol{b}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">b</span></span></span></span></span></span></span>。<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/9c375b3ad70240f1b2014231cbdd5e14.gif#pic_center"/></p>
<p>矩阵化如下：<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/8b635a6a21ef402e8702d8f191da661a.png"/></p>
<p>在上述的 self-attention 中，我们最终只得到一个注意力矩阵，也就是说这个注意力矩阵所关注的信息只偏句子之间的一种关系，但是在时序序列中，往往特征之间不止一种关系，所以我们要提取多个注意力矩阵，这样可以捕获更多的信息，这种注意力机制也就是 <strong>多头注意力机制(Multi-Heads)</strong>。在实现过程中，我们只需要将原始的 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           q
          
          
           i
          
         
         
          ,
         
         
          
           k
          
          
           i
          
         
         
          ,
         
         
          
           v
          
          
           i
          
         
        
       
      
      
       \boldsymbol{q^i,k^i,v^i}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0427em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.037em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.0185em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.037em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span></span> 分裂为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        n
       
      
      
       \boldsymbol{n}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4444em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">n</span></span></span></span></span></span></span> 个就得到 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        n
       
      
      
       \boldsymbol{n}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4444em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">n</span></span></span></span></span></span></span> 头自注意力机制了。<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/6ba45518a73649e9818594897369ff57.gif#pic_center"/></p>
<h1><a id="2%09PyTorch__21"></a>2 PyTorch 实现</h1>
<p>定义 num_attention_heads 为注意力机制的头数，input_size 为输入特征维度，hidden_size 为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           q
          
          
           i
          
         
         
          ,
         
         
          
           k
          
          
           i
          
         
         
          ,
         
         
          
           v
          
          
           i
          
         
        
       
      
      
       \boldsymbol{q^i,k^i,v^i}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0427em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.037em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.0185em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.037em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8483em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span></span> 的总维度，这样每个头的维度也可以求出，定义为 attention_head_size：</p>
<pre><code class="prism language-python">self<span class="token punctuation">.</span>num_attention_heads <span class="token operator">=</span> num_attention_heads
self<span class="token punctuation">.</span>attention_head_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>hidden_size <span class="token operator">/</span> num_attention_heads<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>all_head_size <span class="token operator">=</span> hidden_size
</code></pre>
<p>定义 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           W
          
          
           q
          
         
         
          ,
         
         
          
           W
          
          
           k
          
         
         
          ,
         
         
          
           W
          
          
           v
          
         
        
       
      
      
       \boldsymbol{W^q, W^k,W^v}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0435em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">q</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8491em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.0185em;">k</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">v</span></span></span></span></span></span></span></span></span></span></span></span></span></span>，通过全连接网络生成：</p>
<pre><code class="prism language-python">self<span class="token punctuation">.</span>key_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>query_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>value_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
</code></pre>
<p>使用输入特征乘 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           W
          
          
           q
          
         
         
          ,
         
         
          
           W
          
          
           k
          
         
         
          ,
         
         
          
           W
          
          
           v
          
         
        
       
      
      
       \boldsymbol{W^q, W^k,W^v}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0435em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">q</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8491em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.0185em;">k</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">v</span></span></span></span></span></span></span></span></span></span></span></span></span></span> 得到 <strong>Query，Key，Value</strong> 矩阵，维度为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        (
       
       
        b
       
       
        a
       
       
        t
       
       
        c
       
       
        h
       
       
        _
       
       
        s
       
       
        i
       
       
        z
       
       
        e
       
       
        ,
       
       
        s
       
       
        e
       
       
        q
       
       
        _
       
       
        l
       
       
        e
       
       
        n
       
       
        ,
       
       
        h
       
       
        i
       
       
        d
       
       
        d
       
       
        e
       
       
        n
       
       
        _
       
       
        s
       
       
        i
       
       
        z
       
       
        e
       
       
        )
       
      
      
       (batch\_size,seq\_len, hidden\_size)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.06em; vertical-align: -0.31em;"></span><span class="mopen">(</span><span class="mord mathnormal">ba</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">se</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">hi</span><span class="mord mathnormal">dd</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mclose">)</span></span></span></span></span>：</p>
<pre><code class="prism language-python">key <span class="token operator">=</span> self<span class="token punctuation">.</span>key_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
query <span class="token operator">=</span> self<span class="token punctuation">.</span>query_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
value <span class="token operator">=</span> self<span class="token punctuation">.</span>value_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre>
<p>求多头注意力机制的 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           W
          
          
           q
          
         
         
          ,
         
         
          
           W
          
          
           k
          
         
         
          ,
         
         
          
           W
          
          
           v
          
         
        
       
      
      
       \boldsymbol{W^q, W^k,W^v}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0435em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">q</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8491em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.0185em;">k</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">v</span></span></span></span></span></span></span></span></span></span></span></span></span></span>，头数为 num_attention_heads，并要调换维度，即将 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        s
       
       
        e
       
       
        q
       
       
        _
       
       
        l
       
       
        e
       
       
        n
       
      
      
       seq\_len
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0044em; vertical-align: -0.31em;"></span><span class="mord mathnormal">se</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span></span></span></span></span> 维度与 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        n
       
       
        u
       
       
        m
       
       
        _
       
       
        a
       
       
        t
       
       
        t
       
       
        e
       
       
        n
       
       
        t
       
       
        i
       
       
        o
       
       
        n
       
       
        _
       
       
        h
       
       
        e
       
       
        a
       
       
        d
       
       
        s
       
      
      
       num\_attention\_heads
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0044em; vertical-align: -0.31em;"></span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">a</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">s</span></span></span></span></span> 维度对换，最终 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           W
          
          
           q
          
         
         
          ,
         
         
          
           W
          
          
           k
          
         
         
          ,
         
         
          
           W
          
          
           v
          
         
        
       
      
      
       \boldsymbol{W^q, W^k,W^v}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0435em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">q</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8491em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.0185em;">k</span></span></span></span></span></span></span></span><span class="mpunct mathbf">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.1597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right: 0.037em;">v</span></span></span></span></span></span></span></span></span></span></span></span></span></span> 维度为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        (
       
       
        b
       
       
        a
       
       
        t
       
       
        c
       
       
        h
       
       
        _
       
       
        s
       
       
        i
       
       
        z
       
       
        e
       
       
        ,
       
       
        n
       
       
        u
       
       
        m
       
       
        _
       
       
        a
       
       
        t
       
       
        t
       
       
        e
       
       
        n
       
       
        t
       
       
        i
       
       
        o
       
       
        n
       
       
        _
       
       
        h
       
       
        e
       
       
        a
       
       
        d
       
       
        s
       
       
        ,
       
       
        s
       
       
        e
       
       
        q
       
       
        _
       
       
        l
       
       
        e
       
       
        n
       
       
        ,
       
       
        a
       
       
        t
       
       
        t
       
       
        e
       
       
        n
       
       
        t
       
       
        i
       
       
        o
       
       
        n
       
       
        _
       
       
        h
       
       
        e
       
       
        a
       
       
        d
       
       
        _
       
       
        s
       
       
        i
       
       
        z
       
       
        e
       
       
        )
       
      
      
       (batch\_size,num\_attention\_heads,seq\_len,attention\_head\_size)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.06em; vertical-align: -0.31em;"></span><span class="mopen">(</span><span class="mord mathnormal">ba</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">a</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">se</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mclose">)</span></span></span></span></span>：</p>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">trans_to_multiple_heads</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    new_size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span> <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_attention_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attention_head_size<span class="token punctuation">)</span>
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>new_size<span class="token punctuation">)</span>
    <span class="token keyword">return</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
key_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>trans_to_multiple_heads<span class="token punctuation">(</span>key<span class="token punctuation">)</span>
query_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>trans_to_multiple_heads<span class="token punctuation">(</span>query<span class="token punctuation">)</span>
value_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>trans_to_multiple_heads<span class="token punctuation">(</span>value<span class="token punctuation">)</span>
</code></pre>
<p>将 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        Q
       
      
      
       \boldsymbol{Q}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8805em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">Q</span></span></span></span></span></span></span> 和 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        K
       
      
      
       \boldsymbol{K}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6861em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.0698em;">K</span></span></span></span></span></span></span> 矩阵做点积运算，并进行缩放，得到注意力矩阵的维度为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        (
       
       
        b
       
       
        a
       
       
        t
       
       
        c
       
       
        h
       
       
        _
       
       
        s
       
       
        i
       
       
        z
       
       
        e
       
       
        ,
       
       
        n
       
       
        u
       
       
        m
       
       
        _
       
       
        a
       
       
        t
       
       
        t
       
       
        e
       
       
        n
       
       
        t
       
       
        i
       
       
        o
       
       
        n
       
       
        _
       
       
        h
       
       
        e
       
       
        a
       
       
        d
       
       
        s
       
       
        ,
       
       
        s
       
       
        e
       
       
        q
       
       
        _
       
       
        l
       
       
        e
       
       
        n
       
       
        ,
       
       
        s
       
       
        e
       
       
        q
       
       
        _
       
       
        l
       
       
        e
       
       
        n
       
       
        )
       
      
      
       (batch\_size,num\_attention\_heads,seq\_len,seq\_len)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.06em; vertical-align: -0.31em;"></span><span class="mopen">(</span><span class="mord mathnormal">ba</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">a</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">se</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">se</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></span>：</p>
<pre><code class="prism language-python">attention_scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query_heads<span class="token punctuation">,</span> key_heads<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
attention_scores <span class="token operator">=</span> attention_scores <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention_head_size<span class="token punctuation">)</span>
</code></pre>
<p>对注意力矩阵进行归一化，归一化的维度为 3，矩阵的维度不发生变化：</p>
<pre><code class="prism language-python">attention_probs <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention_scores<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre>
<p>将注意力矩阵乘以矩阵 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        V
       
      
      
       \boldsymbol{V}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6861em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.2555em;">V</span></span></span></span></span></span></span>，得到输出特征，维度为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        (
       
       
        b
       
       
        a
       
       
        t
       
       
        c
       
       
        h
       
       
        _
       
       
        s
       
       
        i
       
       
        z
       
       
        e
       
       
        ,
       
       
        n
       
       
        u
       
       
        m
       
       
        _
       
       
        a
       
       
        t
       
       
        t
       
       
        e
       
       
        n
       
       
        t
       
       
        i
       
       
        o
       
       
        n
       
       
        _
       
       
        h
       
       
        e
       
       
        a
       
       
        d
       
       
        s
       
       
        ,
       
       
        s
       
       
        e
       
       
        q
       
       
        _
       
       
        l
       
       
        e
       
       
        n
       
       
        ,
       
       
        a
       
       
        t
       
       
        t
       
       
        e
       
       
        n
       
       
        t
       
       
        i
       
       
        o
       
       
        n
       
       
        _
       
       
        h
       
       
        e
       
       
        a
       
       
        d
       
       
        _
       
       
        s
       
       
        i
       
       
        z
       
       
        e
       
       
        )
       
      
      
       (batch\_size,num\_attention\_heads,seq\_len,attention\_head\_size)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.06em; vertical-align: -0.31em;"></span><span class="mopen">(</span><span class="mord mathnormal">ba</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">a</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">se</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord" style="margin-right: 0.0278em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mclose">)</span></span></span></span></span>：</p>
<pre><code class="prism language-python">context <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_probs<span class="token punctuation">,</span> value_heads<span class="token punctuation">)</span>
</code></pre>
<p>将各头的注意力矩阵进行拼接，contiguous() 是将 tensor 的内存变成连续的，否则进行 view 操作时会报错，至于原因可参考：<a href="https://blog.csdn.net/kdongyi/article/details/108180250"><strong>https://blog.csdn.net/kdongyi/article/details/108180250</strong></a>：</p>
<pre><code class="prism language-python">context <span class="token operator">=</span> context<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
new_size <span class="token operator">=</span> context<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span> <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>all_head_size <span class="token punctuation">,</span> <span class="token punctuation">)</span>
context <span class="token operator">=</span> context<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>new_size<span class="token punctuation">)</span>
</code></pre>
<p>全部代码：</p>
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> math
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">selfAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span> <span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_attention_heads<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>selfAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> hidden_size <span class="token operator">%</span> num_attention_heads <span class="token operator">!=</span> <span class="token number">0</span> <span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string">"the hidden size %d is not a multiple of the number of attention heads"</span>
                <span class="token string">"%d"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> num_attention_heads<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>num_attention_heads <span class="token operator">=</span> num_attention_heads
        self<span class="token punctuation">.</span>attention_head_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>hidden_size <span class="token operator">/</span> num_attention_heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>all_head_size <span class="token operator">=</span> hidden_size

        self<span class="token punctuation">.</span>key_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>query_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>value_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">trans_to_multiple_heads</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        new_size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span> <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_attention_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attention_head_size<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>new_size<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        key <span class="token operator">=</span> self<span class="token punctuation">.</span>key_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        query <span class="token operator">=</span> self<span class="token punctuation">.</span>query_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        value <span class="token operator">=</span> self<span class="token punctuation">.</span>value_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        key_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>trans_to_multiple_heads<span class="token punctuation">(</span>key<span class="token punctuation">)</span>
        query_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>trans_to_multiple_heads<span class="token punctuation">(</span>query<span class="token punctuation">)</span>
        value_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>trans_to_multiple_heads<span class="token punctuation">(</span>value<span class="token punctuation">)</span>

        attention_scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query_heads<span class="token punctuation">,</span> key_heads<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        attention_scores <span class="token operator">=</span> attention_scores <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention_head_size<span class="token punctuation">)</span>

        attention_probs <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention_scores<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

        context <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_probs<span class="token punctuation">,</span> value_heads<span class="token punctuation">)</span>
        context <span class="token operator">=</span> context<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
        new_size <span class="token operator">=</span> context<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span> <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>all_head_size <span class="token punctuation">,</span> <span class="token punctuation">)</span>
        context <span class="token operator">=</span> context<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>new_size<span class="token punctuation">)</span>
        <span class="token keyword">return</span> context
</code></pre>
<p>测试：</p>
<pre><code class="prism language-python">features <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
attention <span class="token operator">=</span> selfAttention<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>
result <span class="token operator">=</span> attention<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre>
<p>结果：</p>
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
<p>参考：<br/> <a href="https://blog.csdn.net/beilizhang/article/details/115282604"><em><strong>https://blog.csdn.net/beilizhang/article/details/115282604</strong></em></a></p>
</div>
<link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css" rel="stylesheet"/>
<link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css" rel="stylesheet"/>
</div>