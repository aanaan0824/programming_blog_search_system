<div class="article_content clearfix" id="article_content">
<link href="style.css" rel="stylesheet"/>
<div class="markdown_views prism-atom-one-dark" id="content_views">
<svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
<path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
</svg>
<h3><a id="MapReduce_2"></a>调用MapReduce对文件中各单词出现次数统计</h3>
<ul><li><a href="#MapReuce_10">调用MapReuce对文件中各单词出现次数统计</a></li><li><a href="#_15">一、安装环境</a></li><li><a href="#_22">二、需求分析</a></li><li>
<ul><li><a href="#1hadoop_23">1.创建hadoop账户</a></li><li><a href="#2Hadoop_35">2.设置Hadoop密码</a></li><li><a href="#3Hadoop_49">3.为Hadoop用户增加管理员权限</a></li><li><a href="#4apt_60">4.更新apt</a></li><li><a href="#5vim_70">5.安装vim</a></li><li><a href="#6SSHSSH_77">6.安装SSH、配置SSH无密码登陆</a></li></ul> </li><li><a href="#java_101">三、安装java环境</a></li><li>
<ul><li><a href="#1JDK_102">1.安装JDK</a></li><li><a href="#2JDK_116">2.验证JDK安装情况</a></li><li><a href="#3_Hadoop313_123">3.安装 Hadoop3.1.3</a></li></ul> </li><li><a href="#hadoop_151">四、安装hadoop</a></li><li><a href="#Hadoop_169">五、Hadoop单机配置(非分布式)</a></li><li><a href="#Hadoop_191">六、Hadoop伪分布式配置</a></li><li>
<ul><li><a href="#1_192">1.修改配置文件</a></li><li><a href="#2_NameNode_241">2.格式化 NameNode</a></li><li><a href="#3NameNodeDataNode_272">3.开启NameNode和DataNode守护进程</a></li><li><a href="#4_283">4.校验安装</a></li><li><a href="#5vim_290">5.安装vim</a></li></ul> </li><li><a href="#MapReduceWordCount_300">七、调用MapReduce执行WordCount对单词进行计数</a></li><li>
<ul><li><a href="#1_304">1.准备好单词文本</a></li><li><a href="#2_HadoopEclipsePlugin_334">2.配置 Hadoop-Eclipse-Plugin</a></li><li><a href="#3MapReduce_349">3.创建MapReduce项目</a></li><li><a href="#4WordCount_361">4.WordCount统计</a></li></ul> </li><li><a href="#_477">八.参考材料</a></li></ul>
<hr/>
<p>iii</p>
<h1><a id="MapReuce_36"></a>调用MapReuce对文件中各单词出现次数统计</h1>
<h1><a id="_39"></a>一、安装环境</h1>
<p>在Window上安装VMware虚拟机来安装Linux系统。虚拟机（Virtual Machine）指通过软件模拟的具有完整硬件系统功能的、运行在一个完全隔离环境中的完整计算机系统。虚拟系统通过生成现有操作系统的全新虚拟镜像，它具有真实windows系统完全一样的功能，进入虚拟系统后，所有操作都是在这个全新的独立的虚拟系统里面进行，可以独立安装运行软件，保存数据，拥有自己的独立桌面，不会对真正的系统产生任何影响 ，而且具有能够在现有系统与虚拟镜像之间灵活切换的一类操作系统。</p>
<h1><a id="_44"></a>二、需求分析</h1>
<h2><a id="1hadoop_47"></a>1.创建hadoop账户</h2>
<p>1.在虚拟机桌面找到终端或者按 ctrl+alt+t 打开终端窗口，输入如下命令创建新用户 :<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225194423275.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/></p>
<pre><code>sudo useradd -m hadoop -s /bin/bash
</code></pre>
<p>这条命令创建了可以登陆的 hadoop 用户，并使用 /bin/bash 作为 shell。</p>
<h2><a id="2Hadoop_58"></a>2.设置Hadoop密码</h2>
<p>这里我建议设置的密码简单点</p>
<pre><code>sudo passwd hadoop
</code></pre>
<h2><a id="3Hadoop_66"></a>3.为Hadoop用户增加管理员权限</h2>
<pre><code>sudo adduser hadoop sudo
</code></pre>
<p>然后返回登陆界面，进行切换用户选择刚创建的 hadoop 用户进行登陆。</p>
<h2><a id="4apt_74"></a>4.更新apt</h2>
<p>用 hadoop 用户登录后，我们先更新一下 apt，后续我们使用 apt 安装软件，如果没更新可能有一些软件安装不了。按 ctrl+alt+t 打开终端窗口，执行如下命令：</p>
<pre><code>sudo apt-get update
</code></pre>
<p>若出现如下 “Hash校验和不符” 的提示，可通过更改软件源来解决。若没有该问题，则不需要更改。从软件源下载某些软件的过程中，可能由于网络方面的原因出现没法下载的情况，那么建议更改软件源。在学习Hadoop过程中，即使出现“Hash校验和不符”的提示，也不会影响Hadoop的安装。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201225214111324.png)</p>
<h2><a id="5vim_84"></a>5.安装vim</h2>
<p>后续需要更改一些配置文件，我比较喜欢用的是 vim（vi增强版，基本用法相同），建议安装一下（如果你实在还不会用 vi/vim 的，请将后面用到 vim 的地方改为 gedit，这样可以使用文本编辑器进行修改，并且每次文件更改完成后请关闭整个 gedit 程序，否则会占用终端）：</p>
<pre><code>sudo apt-get install vim
</code></pre>
<p>安装软件时若需要确认，在提示处输入 y 即可</p>
<h2><a id="6SSHSSH_94"></a>6.安装SSH、配置SSH无密码登陆</h2>
<p>集群、单节点模式都需要用到 SSH 登陆（类似于远程登陆，你可以登录某台 Linux 主机，并且在上面运行命令），Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server：</p>
<pre><code>sudo apt-get install vim
</code></pre>
<p>安装后，可以使用如下命令登陆本机：</p>
<pre><code>ssh localhost
</code></pre>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225214520208.png"/><br/> 但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。<br/> 首先退出刚才的 ssh，就回到了我们原先的终端窗口，然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中：</p>
<pre><code>exit           # 退出刚才的 ssh localhost
cd ~/.ssh/     # 若没有该目录，请先执行一次ssh localhost
ssh-keygen -t rsa  # 会有提示，都按回车就可以
cat ./id_rsa.pub &gt;&gt; ./authorized_keys  # 加入授权
</code></pre>
<h1><a id="java_117"></a>三、安装java环境</h1>
<h2><a id="1JDK_120"></a>1.安装JDK</h2>
<p>Hadoop3.1.3需要JDK版本在1.8及以上。需要按照下面步骤来自己手动安装JDK1.8。<br/> 我们已经把JDK1.8的安装包jdk-8u162-linux-x64.tar.gz放在了百度云盘，可以点击这里到百度云盘下载（提取码：lnwl）。<br/> 接下来在Linux命令行界面中，执行如下Shell命令（注意：当前登录用户名是hadoop）：</p>
<pre><code>cd /usr/lib
sudo mkdir jvm   #创建/usr/lib/jvm目录用来存放JDK文件
cd ~             #进入hadoop用户的主目录
cd Downloads     #注意区分大小写字母，刚才已经通过FTP软件把JDK安装包jdk-8u162-linux-x64.tar.gz上传到该目录下
sudo tar -zxvf ./jdk-8u162-linux-x64.tar.gz -C /usr/lib/jvm  #把JDK文件解压到/usr/lib/jvm目录下
</code></pre>
<p>此时会有如下提示(SSH首次登陆提示)，输入 yes 。然后按提示输入密码 hadoop，这样就登陆到本机了。</p>
<h2><a id="2JDK_136"></a>2.验证JDK安装情况</h2>
<p>JDK文件解压缩以后，可以执行如下命令到/usr/lib/jvm目录查看一下：</p>
<pre><code>cd /usr/lib/jvm
ls
</code></pre>
<h2><a id="3_Hadoop313_145"></a>3.安装 Hadoop3.1.3</h2>
<p>后续需要更改一些配置文件，我比较喜欢用的是 vim（vi增强版，基本用法相同），建议安装一下（如果你实在还不会用 vi/vim 的，请将后面用到 vim 的地方改为 gedit，这样可以使用文本编辑器进行修改，并且每次文件更改完成后请关闭整个 gedit 程序，否则会占用终端）：</p>
<pre><code>sudo apt-get install vim
</code></pre>
<p>通过vim编辑器。打开环境变量配置.bashrc文件，在文件开头添加如下几行内容：<br/> （vim编辑器中，按“i”进去编辑模式，按“:wq”保存并返回终端）</p>
<pre><code>export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
</code></pre>
<p>保存.bashrc文件并退出vim编辑器。然后，继续执行如下命令让.bashrc文件的配置立即生效：</p>
<pre><code>source ~/.bashrc
</code></pre>
<p>验证安装情况</p>
<pre><code>java -version
</code></pre>
<p>若返回如下信息，则代表JAVA环境配置成功<br/> （图为java1.7版本）<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225215752190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/></p>
<h1><a id="hadoop_176"></a>四、安装hadoop</h1>
<p>Hadoop安装文件，可以到Hadoop官网下载hadoop-3.1.3.tar.gz。<br/> 也可以直接点击这里从百度云盘下载软件（提取码：lnwl），进入百度网盘后，进入“软件”目录，找到hadoop-3.1.3.tar.gz文件，下载到本地。<br/> 我们选择将 Hadoop 安装至 /usr/local/ 中：</p>
<pre><code>sudo tar -zxf ~/下载/hadoop-3.1.3.tar.gz -C /usr/local     # 解压到/usr/local中
cd /usr/local/
sudo mv ./hadoop-3.1.3/ ./hadoop                          # 将文件夹名改为hadoop
sudo chown -R hadoop ./hadoop                             # 修改文件权限


cd /usr/local/hadoop
./bin/hadoop version
</code></pre>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225220024186.png"/></p>
<h1><a id="Hadoop_195"></a>五、Hadoop单机配置(非分布式)</h1>
<p>Hadoop 默认模式为非分布式模式（本地模式），无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试。</p>
<p>现在我们可以执行例子来感受下 Hadoop 的运行。Hadoop 附带了丰富的例子（运行 ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar 可以看到所有例子），包括 wordcount、terasort、join、grep 等。</p>
<p>在此我们选择运行 grep 例子，我们将 input 文件夹中的所有文件作为输入，筛选当中符合正则表达式 dfs[a-z.]+ 的单词并统计出现的次数，最后输出结果到 output 文件夹中。</p>
<pre><code>cd /usr/local/hadoop
mkdir ./input
cp ./etc/hadoop/*.xml ./input                 # 将配置文件作为输入文件
./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar grep ./input ./output 'dfs[a-z.]+'
cat ./output/*                                # 查看运行结果
</code></pre>
<p>执行成功后如下所示，输出了作业的相关信息，输出的结果是符合正则的单词 dfsadmin 出现了1次<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225220135876.png"/><br/> 注意，Hadoop 默认不会覆盖结果文件，因此再次运行上面实例会提示出错，需要先将 ./output 删除。</p>
<pre><code>rm -r ./output
</code></pre>
<h1><a id="Hadoop_218"></a>六、Hadoop伪分布式配置</h1>
<h2><a id="1_221"></a>1.修改配置文件</h2>
<p>Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。</p>
<p>Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。</p>
<pre><code>cd /usr/local/hadoop/etc/hadoop/
</code></pre>
<p>需要在创建对应的文件夹进行存放后再修改配置文件，否则后续操作无法启动Hadoop。</p>
<pre><code>sudo mkdir /usr/local/hadoop/tmp
sudo mkdir /usr/local/hadoop/tmp/dfs/name
sudo mkdir /usr/local/hadoop/tmp/dfs/data
</code></pre>
<p>修改配置文件 core-site.xml (通过 gedit 编辑会比较方便: gedit ./etc/hadoop/core-site.xml)，将当中的</p>
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;
        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>同样的，修改配置文件 hdfs-site.xml：</p>
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<h2><a id="2_NameNode_271"></a>2.格式化 NameNode</h2>
<p>配置上述完成后，执行 NameNode 的格式化:</p>
<pre><code>cd /usr/local/hadoop
./bin/hdfs namenode -format
</code></pre>
<p>成功的话，会看到 “successfully formatted” 的提示，具体返回信息类似如下：</p>
<pre><code>2020-01-08 15:31:31,560 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************

STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = hadoop/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:  version = 3.1.3
*************************************************************/

......
2020-01-08 15:31:35,677 INFO common.Storage: Storage directory /usr/local/hadoop/tmp/dfs/name **has been successfully formatted**.
2020-01-08 15:31:35,700 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop/tmp/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
2020-01-08 15:31:35,770 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop/tmp/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 393 bytes saved in 0 seconds .
2020-01-08 15:31:35,810 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0
2020-01-08 15:31:35,816 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.
2020-01-08 15:31:35,816 INFO namenode.NameNode: SHUTDOWN_MSG:  
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at hadoop/127.0.1.1
*************************************************************/
</code></pre>
<h2><a id="3NameNodeDataNode_303"></a>3.开启NameNode和DataNode守护进程</h2>
<p>接着开启 NameNode 和 DataNode 守护进程：</p>
<pre><code>cd /usr/local/hadoop
./sbin/start-dfs.sh  #start-dfs.sh是个完整的可执行文件，中间没有空格
</code></pre>
<p>若出现如下SSH提示，输入yes即可。<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225221314397.png"/>启动时可能会出现如下 WARN 提示：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable WARN 提示可以忽略，并不会影响正常使用。</p>
<h2><a id="4_315"></a>4.校验安装</h2>
<p>后续需要更改一些配置文件，我比较喜欢用的是 vim（vi增强版，基本用法相同），建议安装一下（如果你实在还不会用 vi/vim 的，请将后面用到 vim 的地方改为 gedit，这样可以使用文本编辑器进行修改，并且每次文件更改完成后请关闭整个 gedit 程序，否则会占用终端）：</p>
<pre><code>sudo apt-get install vim
</code></pre>
<h2><a id="5vim_323"></a>5.安装vim</h2>
<p>启动完成后，可以通过命令 jps 来判断是否成功启动，若成功启动则会列出如下进程: “NameNode”、”DataNode” 和 “SecondaryNameNode”（如果 SecondaryNameNode 没有启动，请运行 sbin/stop-dfs.sh 关闭进程，然后再次尝试启动尝试）。如果没有 NameNode 或 DataNode ，那就是配置不成功，请仔细检查之前步骤，或通过查看启动日志排查原因。</p>
<pre><code>sudo apt-get install vim
</code></pre>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225221401882.png"/>成功启动后，可以访问 Web 界面 http://localhost:9000 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225221444233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/></p>
<h1><a id="MapReduceWordCount_334"></a>七、调用MapReduce执行WordCount对单词进行计数</h1>
<h2><a id="1_337"></a>1.准备好单词文本</h2>
<p>准备一个不少于10000万单词的文本文件，内容不限，可从各大英语文献网下载，将这个文件放置于hadoop文件夹中，以便实验。<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225221543827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>上传文本到HDFS中（请确保Hadoop为开启状态）</p>
<pre><code>./bin/hdfs dfs -put /usr/local/hadoop/demo.txt  input
</code></pre>
<p>调用ls命令查看文件上传情况</p>
<pre><code>./bin/hdfs dfs –ls input
</code></pre>
<p>检验上传情况</p>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225221700726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>在Ubuntu左侧找到自带的软件中心找到Eclipse并安装<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222117397.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>Eclipse 安装至 /usr/lib 目录中：</p>
<pre><code>sudo tar -zxf ~/下载/eclipse-java-mars-1-linux-gtk*.tar.gz -C /usr/lib
#eclipse-java-mars-1-linux-gtk*.tar.gz为文件名
</code></pre>
<p>安装完Eclipse，我们还需要安装 hadoop-eclipse-plugin，用于在 Eclipse 上编译和运行 MapReduce 程序，可下载 Github 上的hadoop2x-eclipse-plugin （备用下载地址：http://pan.baidu.com/s/1i4ikIoP）。</p>
<p>下载后，将 release 中的 hadoop-eclipse-kepler-plugin-2.6.0.jar （还提供了 2.2.0 和 2.4.1 版本）复制到 Eclipse 安装目录的 plugins 文件夹中，运行 eclipse -clean 重启 Eclipse 即可（添加插件后只需要运行一次该命令，以后按照正常方式启动就行了）。</p>
<pre><code>unzip -qo ~/下载/hadoop2x-eclipse-plugin-master.zip -d ~/下载    # 解压到 ~/下载 中
sudo cp ~/下载/hadoop2x-eclipse-plugin-master/release/hadoop-eclipse-plugin-2.6.0.jar /usr/lib/eclipse/plugins/                 # 复制到 eclipse 安装目录的 plugins 目录下
/usr/lib/eclipse/eclipse -clean                               # 添加插件后需要用这种方式使插件生效
</code></pre>
<h2><a id="2_HadoopEclipsePlugin_368"></a>2.配置 Hadoop-Eclipse-Plugin</h2>
<p>当执行完最后一条命令后，系统会自动打开Eclipse，打开后找到的Project Explorer的DFS Locations<br/> 接下来进行下一步配置。<br/> 选择 Window 菜单下的 Preference：<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222047502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>在窗体的左侧找到 Hadoop Map/Reduce 选项，填入Hadoop 的安装地址/usr/local/hadoop<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222251363.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>切换 Map/Reduce 开发视图，选择 Window 菜单下选择 Open Perspective -&gt; Other，选择 Map/Reduce 选项即可进行切换。<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/202012252223367.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/><br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222402467.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>建立与 Hadoop 集群的连接，点击 Eclipse软件右下角的 Map/Reduce Locations 面板，在面板中单击右键，选择 New Hadoop Location。<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222434334.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>在弹出来的 General 选项面板中，General 的设置要与 Hadoop 的配置一致。由于本文使用的Hadoop伪分布式配置，设置 fs.defaultFS 为 hdfs://localhost:9000，所以此处DFS Master 的 Port 要改为 9000。Map/Reduce(V2) Master 的 Port 用默认的即可，Location Name 随意填写。<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222526599.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>配置好后，在左侧的Project Explorer中找到实验文件。<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222601902.png"/></p>
<h2><a id="3MapReduce_377"></a>3.创建MapReduce项目</h2>
<p>点击File菜单，选择New——Project<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222632993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>选择Map/Reduce Project,点击Next<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222721272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>填写项目名称，此处用本实验Words作为项目名。填写完后点击Finish即可。<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222826399.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>接下来在左侧的Project Explorer中找到刚刚建好的WordCount文件夹，右击src选择New-Class创建一个类。<br/> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222853549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>在弹出的class窗口中填入相应信息。 Package 处填写 org.apache.hadoop.examples；在 Name 处填写Words<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225222931297.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/></p>
<h2><a id="4WordCount_386"></a>4.WordCount统计</h2>
<p>Class创建完成后，将下面代码复制进刚创建好的WordCount.java文件中</p>
<pre><code>package org.apache.hadoop.examples;
 
import java.io.IOException;
import java.util.Iterator;
import java.util.StringTokenizer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
 
public class WordCount {
    public WordCount() {
    }
 
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        //String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs();
        String[] otherArgs=new String[]{"input","output"};
        if(otherArgs.length &lt; 2) {
            System.err.println("Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;");
            System.exit(2);
        }
 
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(WordCount.TokenizerMapper.class);
        job.setCombinerClass(WordCount.IntSumReducer.class);
        job.setReducerClass(WordCount.IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
 
        for(int i = 0; i &lt; otherArgs.length - 1; ++i) {
            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));
        }
 
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]));
        System.exit(job.waitForCompletion(true)?0:1);
    }
 
    public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
        private IntWritable result = new IntWritable();
 
        public IntSumReducer() {
        }
 
        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException {
            int sum = 0;
 
            IntWritable val;
            for(Iterator&lt;IntWritable&gt; i$ = values.iterator(); i$.hasNext(); sum += val.get()) {
                val = i$.next();
            }
 
            this.result.set(sum);
            context.write(key, this.result);
        }
    }
 
    public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; {
        private static final IntWritable one = new IntWritable(1);
        private Text word = new Text();
 
        public TokenizerMapper() {
        }
 
        public void map(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
 
            while(itr.hasMoreTokens()) {
                this.word.set(itr.nextToken());
                context.write(this.word, one);
            }
 
        }
    }
}
</code></pre>
<p>在运行 MapReduce 程序前，需要将 /usr/local/hadoop/etc/hadoop 中将有修改过的配置文件（如伪分布式需要 core-site.xml 和 hdfs-site.xml），以及 log4j.properties 复制到 WordCount 项目下的 src 文件夹（~/workspace/WordCount/src）中，在终端中输入下列几行内容:</p>
<pre><code>cp /usr/local/hadoop/etc/hadoop/core-site.xml ~/workspace/WordCount/src
cp /usr/local/hadoop/etc/hadoop/hdfs-site.xml ~/workspace/WordCount/src
cp /usr/local/hadoop/etc/hadoop/log4j.properties ~/workspace/WordCount/src
</code></pre>
<p>复制完成后，务必右键点击 WordCount 选择 refresh 进行刷新（不会自动刷新，需要手动刷新），可以看到文件结构如下所示：</p>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225223124872.png"/>完成上面的工作后，在上方找到启动按钮，点击Run As——Run on Hadoop启动MapReduce程序<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225223147431.png"/>不过由于没有指定参数，运行时会提示 “Usage: wordcount “，需要通过Eclipse设定一下运行参数。</p>
<p>右键点击刚创建的 WordCount.java，选择 Run As -&gt; Run Configurations，在此处可以设置运行时的相关参数（如果 Java Application 下面没有 WordCount，那么需要先双击 Java Application）。切换到 “Arguments” 栏，在 Program arguments 处填写 “input output” 就可以了。</p>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201225223245470.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>当程序执行完毕后，在左侧output——part-r-00000这个文件中看到输出结果<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/2020122522325749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80ODkzMDAzMw==,size_16,color_FFFFFF,t_70"/>也可以通过输入下面命令进行查看</p>
<pre><code>cd /usr/local/haddop
./bin/hdfs dfs -cat output/part-r-00000
</code></pre>
<p>输入下面命令，可以把HDFS中文件下载到本地文件系统中的“/home/hadoop/下载/”这个目录下</p>
<pre><code>./bin/hdfs dfs -get output/part-r-00000  /home/hadoop/下载  
</code></pre>
<h1><a id="_500"></a>八.参考材料</h1>
<p><a href="http://dblab.xmu.edu.cn/blog/285/">http://dblab.xmu.edu.cn/blog/285/</a><br/> <a href="http://dblab.xmu.edu.cn/blog/290-2/">http://dblab.xmu.edu.cn/blog/290-2/</a><br/> <a href="http://dblab.xmu.edu.cn/blog/hadoop-build-project-using-eclipse/">http://dblab.xmu.edu.cn/blog/hadoop-build-project-using-eclipse/</a></p>
</div>
<link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css" rel="stylesheet"/>
<link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css" rel="stylesheet"/>
</div>