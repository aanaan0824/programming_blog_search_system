<div class="article_content clearfix" id="article_content">
<link href="style.css" rel="stylesheet"/>
<div class="htmledit_views" id="content_views">
<p id="main-toc"><strong>目录</strong></p>
<p id="-toc" style="margin-left:0px;"></p>
<p id="%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-toc" style="margin-left:0px;"><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">数据预处理</a></p>
<p id="%E5%8E%BB%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D-toc" style="margin-left:0px;"><a href="#%E5%8E%BB%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D">去除停用词</a></p>
<p id="%E6%9E%84%E5%BB%BALDA%E6%A8%A1%E5%9E%8B-toc" style="margin-left:0px;"><a href="#%E6%9E%84%E5%BB%BALDA%E6%A8%A1%E5%9E%8B">构建LDA模型</a></p>
<p id="%E5%8F%AF%E8%A7%86%E5%8C%96%E2%80%94%E2%80%94pyLDAvis-toc" style="margin-left:0px;"><a href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E2%80%94%E2%80%94pyLDAvis">可视化——pyLDAvis</a></p>
<p id="%C2%A0%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%95%B0%E7%A1%AE%E8%AE%A4-toc" style="margin-left:0px;"><a href="#%C2%A0%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%95%B0%E7%A1%AE%E8%AE%A4"> 主题个数确认</a></p>
<p id="%E5%9B%B0%E6%83%91%E5%BA%A6%E8%AE%A1%E7%AE%97-toc" style="margin-left:40px;"><a href="#%E5%9B%B0%E6%83%91%E5%BA%A6%E8%AE%A1%E7%AE%97">困惑度计算</a></p>
<p id="%E4%B8%80%E8%87%B4%E6%80%A7%E5%BE%97%E5%88%86-toc" style="margin-left:40px;"><a href="#%E4%B8%80%E8%87%B4%E6%80%A7%E5%BE%97%E5%88%86">一致性得分</a></p>
<hr id="hr-toc"/>
<p></p>
<h1 id="%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">数据预处理</h1>
<p>该步骤可自行处理，用excel也好，用python也罢，只要将待分析文本处理为csv或txt存储格式即可。注意：一条文本占一行</p>
<p>例如感想.txt：</p>
<p>我喜欢吃汉堡</p>
<p>小明喜欢吃螺蛳粉</p>
<p>螺蛳粉外卖好贵</p>
<p>以上句子来源于吃完一个汉堡还想再点碗螺蛳粉，但外卖好贵从而选择放弃的我</p>
<h1 id="%E5%8E%BB%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D">去除停用词</h1>
<pre><code class="language-python">import re
import jieba as jb
def stopwordslist(filepath):
    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]
    return stopwords

# 对句子进行分词
def seg_sentence(sentence):
    sentence = re.sub(u'[0-9\.]+', u'', sentence)
    #jb.add_word('词汇')		# 这里是加入自定义的词来补充jieba词典
    sentence_seged = jb.cut(sentence.strip())
    stopwords = stopwordslist('自己搜来的停用词表.txt')  # 这里加载停用词的路径
    outstr = ''
    for word in sentence_seged:
        if word not in stopwords and word.__len__()&gt;1:
            if word != '\t':
                outstr += word
                outstr += " "
    return outstr


inputs = open('感想.txt', 'r', encoding='utf-8')

outputs = open('感想分词.txt', 'w',encoding='utf-8')
for line in inputs:
    line_seg = seg_sentence(line)  # 这里的返回值是字符串
    outputs.write(line_seg + '\n')
outputs.close()
inputs.close()

</code></pre>
<p>该步骤生成感想分词.txt:</p>
<p>我 喜欢 吃 汉堡</p>
<p>小明 喜欢 吃 螺蛳粉</p>
<p>螺蛳粉 外卖 好贵</p>
<p>句子 来源于 吃完 一个 汉堡  再点碗 螺蛳粉 外卖 好贵  选择 放弃 </p>
<h1 id="%E6%9E%84%E5%BB%BALDA%E6%A8%A1%E5%9E%8B">构建LDA模型</h1>
<p>假设主题个数设为4个（num_topics的参数）</p>
<pre><code>import codecs
from gensim import corpora
from gensim.models import LdaModel
from gensim.corpora import Dictionary


train = []

fp = codecs.open('感想分词.txt','r',encoding='utf8')
for line in fp:
    if line != '':
        line = line.split()
        train.append([w for w in line])

dictionary = corpora.Dictionary(train)

corpus = [dictionary.doc2bow(text) for text in train]

lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=4, passes=100)
# num_topics：主题数目
# passes：训练伦次
# num_words：每个主题下输出的term的数目

for topic in lda.print_topics(num_words = 20):
    termNumber = topic[0]
    print(topic[0], ':', sep='')
    listOfTerms = topic[1].split('+')
    for term in listOfTerms:
        listItems = term.split('*')
        print('  ', listItems[1], '(', listItems[0], ')', sep='')</code></pre>
<h1 id="%E5%8F%AF%E8%A7%86%E5%8C%96%E2%80%94%E2%80%94pyLDAvis">可视化——pyLDAvis</h1>
<pre><code>import pyLDAvis.gensim_models

'''插入之前的代码片段'''
import codecs
from gensim import corpora
from gensim.models import LdaModel
from gensim.corpora import Dictionary


train = []

fp = codecs.open('感想分词.txt','r',encoding='utf8')
for line in fp:
    if line != '':
        line = line.split()
        train.append([w for w in line])

dictionary = corpora.Dictionary(train)

corpus = [dictionary.doc2bow(text) for text in train]

lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=4, passes=100)
# num_topics：主题数目
# passes：训练伦次
# num_words：每个主题下输出的term的数目

for topic in lda.print_topics(num_words = 20):
    termNumber = topic[0]
    print(topic[0], ':', sep='')
    listOfTerms = topic[1].split('+')
    for term in listOfTerms:
        listItems = term.split('*')
        print('  ', listItems[1], '(', listItems[0], ')', sep='')
        
d=pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)

'''
lda: 计算好的话题模型

corpus: 文档词频矩阵

dictionary: 词语空间
'''

#pyLDAvis.show(d)		#展示在浏览器
# pyLDAvis.displace(d) #展示在notebook的output cell中
pyLDAvis.save_html(d, 'lda_pass4.html')</code></pre>
<p>这样就会生成看起来很炫酷的图啦（只是示例）：</p>
<p><img alt="" height="444" src="image\9f4bf68319eb438986b780f836bd5b44.png" width="865"/></p>
<h1 id="%C2%A0%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%95%B0%E7%A1%AE%E8%AE%A4"> 主题个数确认</h1>
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#4d4d4d;">计算不同参数下结果的</span></span><span style="background-color:#FFFFFF;"><span style="color:#4d4d4d;"> Perlexity(</span></span><span style="background-color:#FFFFFF;"><span style="color:#4d4d4d;">困惑度</span></span><span style="background-color:#FFFFFF;"><span style="color:#4d4d4d;">)</span></span><span style="background-color:#FFFFFF;"><span style="color:#4d4d4d;">和</span></span><span style="background-color:#FFFFFF;"><span style="color:#4d4d4d;"> Coherence score(</span></span><span style="background-color:#FFFFFF;"><span style="color:#4d4d4d;">一致性评分</span></span><span style="background-color:#FFFFFF;"><span style="color:#4d4d4d;">)</span></span><span style="background-color:#FFFFFF;"><span style="color:#4d4d4d;">，选择困惑度最低且一致性评分最高的参数值作为最终参数设定。</span></span></p>
<h2 id="%E5%9B%B0%E6%83%91%E5%BA%A6%E8%AE%A1%E7%AE%97" style="margin-left:0px;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#4d4d4d;">困惑度计算</span></span></h2>
<pre><code>import gensim
from gensim import corpora, models
import matplotlib.pyplot as plt
import matplotlib
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
 
 
 
 # 准备数据
PATH = "感想分词.txt"  #已经进行了分词的文档（如何分词前面的文章有介绍）
 
 
file_object2=open(PATH,encoding = 'utf-8',errors = 'ignore').read().split('\n')  
data_set=[] #建立存储分词的列表
for i in range(len(file_object2)):
    result=[]
    seg_list = file_object2[i].split()  #读取没一行文本
    for w in seg_list :#读取每一行分词
        result.append(w)
    data_set.append(result)
print(data_set)  #输出所有分词列表
 
 
dictionary = corpora.Dictionary(data_set)  # 构建 document-term matrix
corpus = [dictionary.doc2bow(text) for text in data_set]
Lda = gensim.models.ldamodel.LdaModel  # 创建LDA对象
 
#计算困惑度
def perplexity(num_topics):
    ldamodel = Lda(corpus, num_topics=num_topics, id2word = dictionary, passes=50)  #passes为迭代次数，次数越多越精准
    print(ldamodel.print_topics(num_topics=num_topics, num_words=20))  #num_words为每个主题下的词语数量
    print(ldamodel.log_perplexity(corpus))
    return ldamodel.log_perplexity(corpus)
 
# 绘制困惑度折线图
x = range(1,20)  #主题范围数量
y = [perplexity(i) for i in x]
plt.plot(x, y)
plt.xlabel('主题数目')
plt.ylabel('困惑度大小')
plt.rcParams['font.sans-serif']=['SimHei']
matplotlib.rcParams['axes.unicode_minus']=False
plt.title('主题-困惑度变化情况')
plt.show()</code></pre>
<h2 id="%E2%80%8B"><img alt="" height="275" src="image\da4b3f093aa04b10973abd948d935722.png" width="399"/></h2>
<p> </p>
<h2 id="%E4%B8%80%E8%87%B4%E6%80%A7%E5%BE%97%E5%88%86">一致性得分</h2>
<pre><code>import gensim
from gensim import corpora, models
import matplotlib.pyplot as plt
import matplotlib
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
 
 
 
 # 准备数据
PATH = "感想分词.txt"  #已经进行了分词的文档（如何分词前面的文章有介绍）
 
 
file_object2=open(PATH,encoding = 'utf-8',errors = 'ignore').read().split('\n')  
data_set=[] #建立存储分词的列表
for i in range(len(file_object2)):
    result=[]
    seg_list = file_object2[i].split()  #读取没一行文本
    for w in seg_list :#读取每一行分词
        result.append(w)
    data_set.append(result)
print(data_set)  #输出所有分词列表
 
 
dictionary = corpora.Dictionary(data_set)  # 构建 document-term matrix
corpus = [dictionary.doc2bow(text) for text in data_set]
Lda = gensim.models.ldamodel.LdaModel  # 创建LDA对象
 

def coherence(num_topics):
    ldamodel = Lda(corpus, num_topics=num_topics, id2word = dictionary, passes=50)  #passes为迭代次数，次数越多越精准
    coherence_model_lda = CoherenceModel(model=ldamodel, texts=data_set, dictionary=dictionary, coherence='c_v')
    coherence_lda = coherence_model_lda.get_coherence()
    print('\nCoherence Score: ', coherence_lda)
    return coherence_lda
 
# 绘制困惑度折线图
x = range(1,20)  #主题范围数量
y = [coherence(i) for i in x]
plt.plot(x,y)
plt.xlabel('主题数目')
plt.ylabel('coherence大小')
plt.rcParams['font.sans-serif']=['SimHei']
matplotlib.rcParams['axes.unicode_minus']=False
plt.title('主题-coherence变化情况')
plt.show()</code></pre>
<p><img alt="" height="275" src="image\fb48c3dd70d544d1bc284f80883aa3a2.png" width="389"/></p>
<h1> 结语</h1>
<p>整个流程就大致是这样啦！有问题欢迎一起交流！！</p>
<p class="img-center"><img alt="" src="image\dfd5038263cc797ca5057804c1030cce.png"/></p>
<p> </p>
</div>
</div>