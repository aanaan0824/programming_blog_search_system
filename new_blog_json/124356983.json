{"blogid": "124356983", "writerAge": "码龄3年", "writerBlogNum": "26", "writerCollect": "620", "writerComment": "101", "writerFan": "290", "writerGrade": "3级", "writerIntegral": "528", "writerName": "indigo  love", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_124356983.jpg", "writerRankTotal": "46721", "writerRankWeekly": "359807", "writerThumb": "191", "writerVisitNum": "92890", "blog_read_count": "6947", "blog_time": "已于 2022-05-02 22:50:03 修改", "blog_title": "深度强化学习-DDPG算法原理与代码", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<p id=\"main-toc\"><strong>深度强化学习-DDPG算法原理与代码</strong></p>\n<p id=\"%E5%BC%95%E8%A8%80-toc\" style=\"margin-left:0px;\"><a href=\"#%E5%BC%95%E8%A8%80\">引言</a></p>\n<p id=\"1%20DDPG%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B-toc\" style=\"margin-left:0px;\"><a href=\"#1%20DDPG%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B\">1 DDPG算法简介</a></p>\n<p id=\"2%20DDPG%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86-toc\" style=\"margin-left:0px;\"><a href=\"#2%20DDPG%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86\">2 DDPG算法原理</a></p>\n<p id=\"2.1%20%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE-toc\" style=\"margin-left:40px;\"><a href=\"#2.1%20%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE\">2.1 经验回放</a></p>\n<p id=\"2.2%20%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C-toc\" style=\"margin-left:40px;\"><a href=\"#2.2%20%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C\">2.2 目标网络</a></p>\n<p id=\"2.2.1%20%E7%AE%97%E6%B3%95%E6%9B%B4%E6%96%B0%E8%BF%87%E7%A8%8B-toc\" style=\"margin-left:80px;\"><a href=\"#2.2.1%20%E7%AE%97%E6%B3%95%E6%9B%B4%E6%96%B0%E8%BF%87%E7%A8%8B\">2.2.1 算法更新过程</a></p>\n<p id=\"2.2.2%20%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9B%B4%E6%96%B0-toc\" style=\"margin-left:80px;\"><a href=\"#2.2.2%20%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9B%B4%E6%96%B0\">2.2.2 目标网络的更新</a></p>\n<p id=\"2.2.3%20%E5%BC%95%E5%85%A5%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C%E7%9A%84%E7%9B%AE%E7%9A%84-toc\" style=\"margin-left:80px;\"><a href=\"#2.2.3%20%E5%BC%95%E5%85%A5%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C%E7%9A%84%E7%9B%AE%E7%9A%84\">2.2.3 引入目标网络的目的</a></p>\n<p id=\"2.3%20%E5%99%AA%E5%A3%B0%E6%8E%A2%E7%B4%A2-toc\" style=\"margin-left:40px;\"><a href=\"#2.3%20%E5%99%AA%E5%A3%B0%E6%8E%A2%E7%B4%A2\">2.3 噪声探索</a></p>\n<p id=\"3%20DDPG%E7%AE%97%E6%B3%95%E4%BC%AA%E4%BB%A3%E7%A0%81-toc\" style=\"margin-left:0px;\"><a href=\"#3%20DDPG%E7%AE%97%E6%B3%95%E4%BC%AA%E4%BB%A3%E7%A0%81\">3 DDPG算法伪代码</a></p>\n<p id=\"%C2%A04%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc\" style=\"margin-left:0px;\"><a href=\"#%C2%A04%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0\"> 4 代码实现</a></p>\n<p id=\"5%20%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-toc\" style=\"margin-left:0px;\"><a href=\"#5%20%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C\">5 实验结果</a></p>\n<p id=\"6%20%E7%BB%93%E8%AE%BA-toc\" style=\"margin-left:0px;\"><a href=\"#6%20%E7%BB%93%E8%AE%BA\">6 结论</a></p>\n<hr/>\n<h1 id=\"%E5%BC%95%E8%A8%80\">引言</h1>\n<p>Deep Deterministic Policy Gradient (DDPG)算法是DeepMind团队提出的一种专门用于解决连续控制问题的在线式(on-line)深度强化学习算法，它其实本质上借鉴了Deep Q-Network (DQN)算法里面的一些思想。本文就带领大家了解一下这个算法，论文和代码的链接见下方。</p>\n<p>论文：<a class=\"link-info\" href=\"https://arxiv.org/pdf/1509.02971.pdf\" title=\"https://arxiv.org/pdf/1509.02971.pdf\">https://arxiv.org/pdf/1509.02971.pdf</a></p>\n<p>代码：<a class=\"link-info\" href=\"https://github.com/indigoLovee/DDPG\" title=\"https://github.com/indigoLovee/DDPG\">https://github.com/indigoLovee/DDPG</a></p>\n<p>喜欢的话请点个star哦！</p>\n<h1 id=\"1%20DDPG%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B\">1 DDPG算法简介</h1>\n<p>在DDPG算法之前，我们在求解连续动作空间问题时，主要有两种方式：一是对连续动作做离散化处理，然后再利用强化学习算法（例如<a class=\"link-info\" href=\"https://blog.csdn.net/weixin_46133643/article/details/121845874?spm=1001.2014.3001.5501\" title=\"DQN\">DQN</a>）进行求解。二是使用Policy Gradient (PG)算法 (例如<a class=\"link-info\" href=\"https://blog.csdn.net/weixin_46133643/article/details/122439616?spm=1001.2014.3001.5501\" title=\"Reinforce\">Reinforce</a>) 直接求解。但是对于方式一，离散化处理在一定程度上脱离了工程实际；对于方式二，PG算法在求解连续控制问题时效果往往不尽人意。为此，DDPG算法横空出世，在许多连续控制问题上取得了非常不错的效果。</p>\n<p>DDPG算法是Actor-Critic (AC) 框架下的一种在线式深度强化学习算法，因此算法内部包括Actor网络和Critic网络，每个网络分别遵从各自的更新法则进行更新，从而使得累计期望回报最大化。</p>\n<h1 id=\"2%20DDPG%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86\">2 DDPG算法原理</h1>\n<p>DDPG算法将确定性策略梯度算法和DQN算法中的相关技术结合在一起，之前我们在讲DQN算法时，详细说明了其中的两个重要的技术：经验回放和目标网络。具体而言，DDPG算法主要包括以下三个关键技术：</p>\n<p>（1）经验回放：智能体将得到的经验数据<img alt=\"(s,a,r,s^{'},done)\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%28s%2Ca%2Cr%2Cs%5E%7B%27%7D%2Cdone%29\"/>放入Replay Buffer中，更新网络参数时按照批量采样。</p>\n<p>（2）目标网络：在Actor网络和Critic网络外再使用一套用于估计目标的Target Actor网络和Target Critic网络。在更新目标网络时，为了避免参数更新过快，采用软更新方式。</p>\n<p>（3）噪声探索：确定性策略输出的动作为确定性动作，缺乏对环境的探索。在训练阶段，给Actor网络输出的动作加入噪声，从而让智能体具备一定的探索能力。</p>\n<h2 id=\"2.1%20%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE\">2.1 经验回放</h2>\n<blockquote>\n<p>经验回放就是一种让经验概率分布变得稳定的技术，可以提高训练的稳定性。经验回放主要有“存储”和“回放”两大关键步骤：</p>\n<p><strong>存储：</strong>将经验以<img alt=\"(s_{t},a_{t},r_{t+1},s_{t+1},done)\" src=\"https://latex.codecogs.com/gif.latex?%28s_%7Bt%7D%2Ca_%7Bt%7D%2Cr_%7Bt&amp;plus;1%7D%2Cs_%7Bt&amp;plus;1%7D%2Cdone%29\"/>形式存储在经验池中。</p>\n<p><strong>回放：</strong>按照某种规则从经验池中采样一条或多条经验数据。</p>\n</blockquote>\n<p><strong>从<span style=\"color:#fe2c24;\">存储</span>的角度来看，经验回放可以分为集中式回放和分布式回放：</strong><br/><strong>集中式回放：</strong>智能体在一个环境中运行，把经验统一存储在经验池中。</p>\n<p><strong>分布式回放：</strong>多个智能体同时在多个环境中运行，并将经验统一存储在经验池中。由于多个智能体同时生成经验，所以能够使用更多资源的同时更快地收集经验。</p>\n<p><strong>从<span style=\"color:#fe2c24;\">采样</span>的角度来看，经验回放可以分为均匀回放和优先回放：</strong><br/><strong>均匀回放：</strong>等概率从经验池中采样经验。<br/><strong>优先回放：</strong>为经验池中每条经验指定一个优先级，在采样经验时更倾向于选择优先级更高的经验。一般的做法是，如果某条经验（例如经验<img alt=\"i\" src=\"https://latex.codecogs.com/gif.latex?i\"/>）的优先级为<img alt=\"p_{i}\" src=\"https://latex.codecogs.com/gif.latex?p_%7Bi%7D\"/>，那么选取该经验的概率为：</p>\n<blockquote>\n<p style=\"text-align:center;\"><img alt=\"p_{i}=\\frac{p_{i}}{\\sum p_{k}}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?p_%7Bi%7D%3D%5Cfrac%7Bp_%7Bi%7D%7D%7B%5Csum%20p_%7Bk%7D%7D\"/></p>\n</blockquote>\n<p>优先回放可以具体参照这篇论文 :<a class=\"link-info\" href=\"https://arxiv.org/pdf/1511.05952.pdf\" title=\"优先经验回放\">优先经验回放</a></p>\n<blockquote>\n<p><strong>经验回放的优点：</strong></p>\n<p>1.在训练Q网络时，可以打破数据之间的相关性，使得数据满足独立同分布，从而减小参数更新的方差，提高收敛速度。</p>\n<p>2.能够重复使用经验，数据利用率高，对于数据获取困难的情况尤其有用。</p>\n<p><strong>经验回放的缺点：</strong></p>\n<p>无法应用于回合更新和多步学习算法。但是将经验回放应用于Q学习，就规避了这个缺点。</p>\n</blockquote>\n<p> 代码中采用集中式均匀回放，具体如下：</p>\n<pre><code class=\"language-python\">import numpy as np\n\n\nclass ReplayBuffer:\n    def __init__(self, max_size, state_dim, action_dim, batch_size):\n        self.mem_size = max_size\n        self.batch_size = batch_size\n        self.mem_cnt = 0\n\n        self.state_memory = np.zeros((self.mem_size, state_dim))\n        self.action_memory = np.zeros((self.mem_size, action_dim))\n        self.reward_memory = np.zeros((self.mem_size, ))\n        self.next_state_memory = np.zeros((self.mem_size, state_dim))\n        self.terminal_memory = np.zeros((self.mem_size, ), dtype=np.bool)\n\n    def store_transition(self, state, action, reward, state_, done):\n        mem_idx = self.mem_cnt % self.mem_size\n\n        self.state_memory[mem_idx] = state\n        self.action_memory[mem_idx] = action\n        self.reward_memory[mem_idx] = reward\n        self.next_state_memory[mem_idx] = state_\n        self.terminal_memory[mem_idx] = done\n\n        self.mem_cnt += 1\n\n    def sample_buffer(self):\n        mem_len = min(self.mem_size, self.mem_cnt)\n        batch = np.random.choice(mem_len, self.batch_size, replace=False)\n\n        states = self.state_memory[batch]\n        actions = self.action_memory[batch]\n        rewards = self.reward_memory[batch]\n        states_ = self.next_state_memory[batch]\n        terminals = self.terminal_memory[batch]\n\n        return states, actions, rewards, states_, terminals\n\n    def ready(self):\n        return self.mem_cnt &gt;= self.batch_size</code></pre>\n<h2 id=\"2.2%20%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C\">2.2 目标网络</h2>\n<p>由于DDPG算法是基于AC框架，因此算法中必然含有Actor和Critic网络。另外每个网络都有其对应的目标网络，所以DDPG算法中包括四个网络，分别是Actor网络<img alt=\"\\mu(\\cdot \\mid \\theta ^{\\mu })\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Cmu%28%5Ccdot%20%5Cmid%20%5Ctheta%20%5E%7B%5Cmu%20%7D%29\"/>，Critic网络<img alt=\"Q(\\cdot \\mid \\theta ^{Q})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?Q%28%5Ccdot%20%5Cmid%20%5Ctheta%20%5E%7BQ%7D%29\"/>，Target Actor网络<img alt=\"\\mu^{'}(\\cdot \\mid \\theta ^{\\mu^{'} })\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Cmu%5E%7B%27%7D%28%5Ccdot%20%5Cmid%20%5Ctheta%20%5E%7B%5Cmu%5E%7B%27%7D%20%7D%29\"/>和Target Critic网络 <img alt=\"Q^{'}(\\cdot \\mid \\theta ^{Q^{'}})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?Q%5E%7B%27%7D%28%5Ccdot%20%5Cmid%20%5Ctheta%20%5E%7BQ%5E%7B%27%7D%7D%29\"/>。本节主要介绍一下DDPG算法的更新过程，目标网络的更新方式以及引入目标网络的目的</p>\n<h3 id=\"2.2.1%20%E7%AE%97%E6%B3%95%E6%9B%B4%E6%96%B0%E8%BF%87%E7%A8%8B\">2.2.1 算法更新过程</h3>\n<p><strong>算法更新主要更新的是Actor和Critic网络的参数，其中Actor网络通过最大化累积期望回报来更新<img alt=\"\\theta ^{\\mu }\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Ctheta%20%5E%7B%5Cmu%20%7D\"/>，Critic网络通过最小化评估值与目标值之间的误差来更新<img alt=\"\\theta ^{Q}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Ctheta%20%5E%7BQ%7D\"/>。</strong>在训练阶段，我们从Replay Buffer中采样一个批次的数据，假设采样到的一条数据为<img alt=\"(s,a,r,s^{'},done)\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%28s%2Ca%2Cr%2Cs%5E%7B%27%7D%2Cdone%29\"/>，Actor和Critic网络更新过程如下。</p>\n<p><span style=\"color:#fe2c24;\"><strong>Critic网络更新过程：</strong></span>利用Target Actor网络计算出状态<img alt=\"s^{'}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?s%5E%7B%27%7D\"/>下的动作：</p>\n<blockquote>\n<p style=\"text-align:center;\"><img alt=\"a^{'}=\\mu^{'} (s^{'}\\mid \\theta ^{\\mu ^{'}})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?a%5E%7B%27%7D%3D%5Cmu%5E%7B%27%7D%20%28s%5E%7B%27%7D%5Cmid%20%5Ctheta%20%5E%7B%5Cmu%20%5E%7B%27%7D%7D%29\"/></p>\n</blockquote>\n<p> <strong>这里需要注意：计算出动作后不需要加入噪声。</strong>然后利用Target Critic网络计算出状态动作对<img alt=\"(s,a)\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%28s%2Ca%29\"/>的目标值：</p>\n<blockquote>\n<p style=\"text-align:center;\"><img alt=\"y=r+\\gamma(1- done) Q^{'}(s^{'},a^{'}\\mid \\theta ^{Q^{'}})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?y%3Dr&amp;plus;%5Cgamma%281-%20done%29%20Q%5E%7B%27%7D%28s%5E%7B%27%7D%2Ca%5E%7B%27%7D%5Cmid%20%5Ctheta%20%5E%7BQ%5E%7B%27%7D%7D%29\"/></p>\n</blockquote>\n<p>接着利用 Critic网络计算出状态动作对<img alt=\"(s,a)\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%28s%2Ca%29\"/>的评估值：</p>\n<blockquote>\n<p style=\"text-align:center;\"><img alt=\"q = Q(s,a\\mid \\theta ^{Q})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?q%20%3D%20Q%28s%2Ca%5Cmid%20%5Ctheta%20%5E%7BQ%7D%29\"/></p>\n</blockquote>\n<p> 最后利用<strong>梯度下降算法最小化评估值和期望值之间的差值<img alt=\"L_{c}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?L_%7Bc%7D\"/></strong>，从而对Critic网络中的参数进行更新：</p>\n<blockquote>\n<p style=\"text-align:center;\"><img alt=\"L_{c}=(y-q)^{2}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?L_%7Bc%7D%3D%28y-q%29%5E%7B2%7D\"/></p>\n</blockquote>\n<p> 上述过程其实和DQN算法非常类似。</p>\n<p><span style=\"color:#fe2c24;\"><strong>Actor网络更新过程：</strong></span>利用Actor网络计算出状态<img alt=\"s\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?s\"/>下的动作：</p>\n<blockquote>\n<p style=\"text-align:center;\"><img alt=\"a_{new}=\\mu (s\\mid \\theta ^{\\mu })\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?a_%7Bnew%7D%3D%5Cmu%20%28s%5Cmid%20%5Ctheta%20%5E%7B%5Cmu%20%7D%29\"/></p>\n</blockquote>\n<p><strong>这里需要注意：计算出动作后不需要加入噪声。</strong>然后利用Critic网络计算出状态动作对<img alt=\"(s,a_{new})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%28s%2Ca_%7Bnew%7D%29\"/>的评估值（即累积期望回报）：</p>\n<blockquote>\n<p style=\"text-align:center;\"><img alt=\"q_{new}=Q(s,a_{new}\\mid \\theta ^{Q})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?q_%7Bnew%7D%3DQ%28s%2Ca_%7Bnew%7D%5Cmid%20%5Ctheta%20%5E%7BQ%7D%29\"/></p>\n</blockquote>\n<p>最后利用<strong>梯度上升算法最大化累积期望回报<img alt=\"q_{new}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?q_%7Bnew%7D\"/>（代码实现是采用梯度下降算法优化<img alt=\"-q_{new}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?-q_%7Bnew%7D\"/>，其实本质上都是一样的）</strong>，从而对Actor网络中的参数进行更新。</p>\n<p>至此我们就完成了对Actor和Critic网络的更新。</p>\n<h3 id=\"2.2.2%20%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9B%B4%E6%96%B0\">2.2.2 目标网络的更新</h3>\n<p>对于目标网络的更新，<strong>DDPG算法中采用软更新方式，也可以称为指数平均移动 (Exponential Moving Average, EMA)。</strong>即引入一个学习率（或者成为动量）<img alt=\"\\tau\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Ctau\"/>，将旧的目标网络参数和新的对应网络参数做加权平均，然后赋值给目标网络：</p>\n<p><span style=\"color:#fe2c24;\"><strong>Target Actor网络更新过程：</strong></span></p>\n<blockquote>\n<p style=\"text-align:center;\"><img alt=\"\\theta ^{\\mu ^{'}}=\\tau \\theta ^{\\mu }+(1-\\tau )\\theta ^{\\mu ^{'}}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Ctheta%20%5E%7B%5Cmu%20%5E%7B%27%7D%7D%3D%5Ctau%20%5Ctheta%20%5E%7B%5Cmu%20%7D&amp;plus;%281-%5Ctau%20%29%5Ctheta%20%5E%7B%5Cmu%20%5E%7B%27%7D%7D\"/></p>\n</blockquote>\n<p><span style=\"color:#fe2c24;\"><strong>Target Critic网络更新过程：</strong></span></p>\n<blockquote>\n<p style=\"text-align:center;\"><img alt=\"\\theta ^{Q ^{'}}=\\tau \\theta ^{Q }+(1-\\tau )\\theta ^{Q ^{'}}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Ctheta%20%5E%7BQ%20%5E%7B%27%7D%7D%3D%5Ctau%20%5Ctheta%20%5E%7BQ%20%7D&amp;plus;%281-%5Ctau%20%29%5Ctheta%20%5E%7BQ%20%5E%7B%27%7D%7D\"/></p>\n</blockquote>\n<p> 学习率（动量）<img alt=\"\\tau \\in (0,1)\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Ctau%20%5Cin%20%280%2C1%29\"/>，通常取值0.005。</p>\n<h3 id=\"2.2.3%20%E5%BC%95%E5%85%A5%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C%E7%9A%84%E7%9B%AE%E7%9A%84\"><strong>2.2.3 引入目标网络的目的</strong></h3>\n<p>我们在前面提到过，引入目标网络的目的其实和DQN算法的想法是一样的。<strong>由于目标网络中的参数是通过软更新的方式来缓慢更新的，因此它的输出会更加稳定，利用目标网络来计算目标值自然也会更加稳定，从而进一步保证Critic网络的学习过程更加平稳。</strong>试想，如果直接使用Critic网络来计算目标值</p>\n<p style=\"text-align:center;\"><img alt=\"y=r+\\gamma(1- done) Q(s^{'},a^{'}\\mid \\theta ^{Q})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?y%3Dr&amp;plus;%5Cgamma%281-%20done%29%20Q%28s%5E%7B%27%7D%2Ca%5E%7B%27%7D%5Cmid%20%5Ctheta%20%5E%7BQ%7D%29\"/></p>\n<p>那么由于Critic网络在不断更新，网络波动剧烈，自然目标值<img alt=\"y\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?y\"/>的变化也很剧烈。在学习过程中，让Critic网络的评估值追逐一个变化剧烈的目标，很容易出现网络震荡，从而导致整个学习过程坍塌。</p>\n<p>上述是一种目的，其实还有另外一个目的。<strong>当使用Critic网络来计算目标值时（如上式所示），它其实本质上是一种自举 (Bootstrapping) 的过程。</strong>然后让<img alt=\"Q(s,a\\mid \\theta ^{Q})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?Q%28s%2Ca%5Cmid%20%5Ctheta%20%5E%7BQ%7D%29\"/>不断逼近<img alt=\"y\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?y\"/>，很容易导致网络过估计。因为当<img alt=\"Q(s^{'},a^{'}\\mid \\theta ^{Q})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?Q%28s%5E%7B%27%7D%2Ca%5E%7B%27%7D%5Cmid%20%5Ctheta%20%5E%7BQ%7D%29\"/>出现过估计时，会将其回传至<img alt=\"Q(s,a\\mid \\theta ^{Q})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?Q%28s%2Ca%5Cmid%20%5Ctheta%20%5E%7BQ%7D%29\"/>，导致该项也出现了过估计，从而<strong>形成了一种正反馈，最终导致整个网络出现过估计。</strong></p>\n<blockquote>\n<p style=\"text-align:center;\"><strong>自举 (Bootstrapping)</strong></p>\n<p>表示在当前值函数的计算过程中，会利用到后续的状态值函数或动作值函数，即利用到后续的状态或者状态动作对。</p>\n</blockquote>\n<p>那么过估计会出现什么问题呢？<strong>如果过估计是均匀的，对于最终的决策不会造成影响；但是如果不均匀，对于最终的决策会产生很大影响。</strong>我们举个栗子吧，大家很容易就能明白。</p>\n<p style=\"text-align:center;\"><img alt=\"\" height=\"144\" src=\"..\\..\\static\\image\\ed9cb277afa84f8b91ae818e7cc3dc92.png\" width=\"489\"/></p>\n<p> 上图中我们假设有三个动作，每个动作的实际动作价值依次是200，100和230，显然动作3的动作价值是最高的，智能体会选择动作3。如果网络出现过估计，并且是均匀的，假设过估计的量是100，那么网络评估出来的动作价值就依次是300，200和330，显然动作3的动作价值依然是最高的，智能体依旧会选择动作3。因此，均匀过估计对于最终的决策并不会产生影响。</p>\n<p style=\"text-align:center;\"><img alt=\"\" height=\"107\" src=\"..\\..\\static\\image\\624254e6d08f481c80a4ba8667edfe92.png\" width=\"513\"/></p>\n<p> 同样我们假设有三个动作，每个动作的实际动作价值依次是200，100和230，显然动作3的动作价值是最高的，智能体会选择动作3。如果网络出现不均匀过估计，评估出来的动作价值依次是280，300和240，此时显然动作2的动作价值是最高的，智能体会选择动作2。但是实际上动作2的真实动作价值是最低的，即该动作是最差的。因此，不均匀过估计对于最终的决策会产生很大的影响。</p>\n<p>然而<strong>实际上网络的过估计是非均匀的，因此需要避免这个问题，本质上就是要解决Bootstrapping问题。</strong>采用目标网络后就能解决这个问题</p>\n<p style=\"text-align:center;\"><img alt=\"y=r+\\gamma(1- done) Q^{'}(s^{'},a^{'}\\mid \\theta ^{Q^{'}})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?y%3Dr&amp;plus;%5Cgamma%281-%20done%29%20Q%5E%7B%27%7D%28s%5E%7B%27%7D%2Ca%5E%7B%27%7D%5Cmid%20%5Ctheta%20%5E%7BQ%5E%7B%27%7D%7D%29\"/></p>\n<p>此时我们再让<img alt=\"Q(s,a\\mid \\theta ^{Q})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?Q%28s%2Ca%5Cmid%20%5Ctheta%20%5E%7BQ%7D%29\"/>逼近目标值<img alt=\"y\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?y\"/>时，就已经不再是自举了（大家可以对照自举的含义仔细观察一下）。</p>\n<h2 id=\"2.3%20%E5%99%AA%E5%A3%B0%E6%8E%A2%E7%B4%A2\">2.3 噪声探索</h2>\n<p> <strong>探索对于智能体来说是至关重要的，而确定性策略“天生”就缺乏探索能力，因此我们需要人为地给输出的动作上加入噪声，从而让智能体具备探索能力。</strong>在DDPG算法中，作者采用Ornstein Uhlenbeck过程作为动作噪声。Ornstein Uhlenbeck过程是用下列随机微分方程定义的 (以一维的情况为例)：</p>\n<p style=\"text-align:center;\"><img alt=\"dN_{t}=\\theta (\\mu -N_{t})dt+\\sigma dB_{t}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?dN_%7Bt%7D%3D%5Ctheta%20%28%5Cmu%20-N_%7Bt%7D%29dt&amp;plus;%5Csigma%20dB_%7Bt%7D\"/></p>\n<p>其中<img alt=\"\\theta\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Ctheta\"/>，<img alt=\"\\mu\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Cmu\"/>，<img alt=\"\\sigma\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Csigma\"/>是参数（<img alt=\"\\theta &gt;0,\\sigma &gt;0\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Ctheta%20%3E0%2C%5Csigma%20%3E0\"/>），<img alt=\"B_{t}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?B_%7Bt%7D\"/>是标准Brownain运动。当初始扰动是在原点的单点分布（即限定<img alt=\"N_{0}=0\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?N_%7B0%7D%3D0\"/>），并且<img alt=\"\\mu =0\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Cmu%20%3D0\"/>时，上述方程的解为</p>\n<p style=\"text-align:center;\"><img alt=\"N_{t}=\\sigma \\int_{0}^{t}e^{\\theta (\\tau -t)}dB_{t}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?N_%7Bt%7D%3D%5Csigma%20%5Cint_%7B0%7D%5E%7Bt%7De%5E%7B%5Ctheta%20%28%5Ctau%20-t%29%7DdB_%7Bt%7D\"/></p>\n<p> (证明：将<img alt=\"dN_{t}=\\theta (\\mu -N_{t})dt+\\sigma dB_{t}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?dN_%7Bt%7D%3D%5Ctheta%20%28%5Cmu%20-N_%7Bt%7D%29dt&amp;plus;%5Csigma%20dB_%7Bt%7D\"/>代入<img alt=\"d(N_{t}e^{\\theta t})=\\theta N_{t}e^{\\theta t}dt+e^{\\theta t}dN_{t}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?d%28N_%7Bt%7De%5E%7B%5Ctheta%20t%7D%29%3D%5Ctheta%20N_%7Bt%7De%5E%7B%5Ctheta%20t%7Ddt&amp;plus;e%5E%7B%5Ctheta%20t%7DdN_%7Bt%7D\"/>，化简可得<img alt=\"d(N_{t}e^{\\theta t})=\\mu \\theta e^{\\theta t}dt+\\sigma e^{\\theta t}dB_{t}\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?d%28N_%7Bt%7De%5E%7B%5Ctheta%20t%7D%29%3D%5Cmu%20%5Ctheta%20e%5E%7B%5Ctheta%20t%7Ddt&amp;plus;%5Csigma%20e%5E%7B%5Ctheta%20t%7DdB_%7Bt%7D\"/>。将此式从0积到<img alt=\"t\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?t\"/>，得<img alt=\"N_{t}e^{\\theta t}-N_{0}=\\mu (e^{\\theta t}-1)+\\sigma \\int_{0}^{t}e^{\\theta t}dB_{\\tau }\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?N_%7Bt%7De%5E%7B%5Ctheta%20t%7D-N_%7B0%7D%3D%5Cmu%20%28e%5E%7B%5Ctheta%20t%7D-1%29&amp;plus;%5Csigma%20%5Cint_%7B0%7D%5E%7Bt%7De%5E%7B%5Ctheta%20t%7DdB_%7B%5Ctau%20%7D\"/>。当<img alt=\"N_{0}=0\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?N_%7B0%7D%3D0\"/>且<img alt=\"\\mu =0\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Cmu%20%3D0\"/>时化简可得结果。)</p>\n<p>这个解得均值为0，方差为<img alt=\"\\frac{\\sigma ^{2}}{2\\theta }(1-e^{-2\\theta })\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Csigma%20%5E%7B2%7D%7D%7B2%5Ctheta%20%7D%281-e%5E%7B-2%5Ctheta%20%7D%29\"/>，协方差为</p>\n<p style=\"text-align:center;\"><img alt=\"Cov(N_{t},N_{s})=\\frac{\\sigma ^{2}}{2\\theta }(e^{-\\theta \\left | t-s \\right |}-e^{-\\theta \\left | t+s \\right |})\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?Cov%28N_%7Bt%7D%2CN_%7Bs%7D%29%3D%5Cfrac%7B%5Csigma%20%5E%7B2%7D%7D%7B2%5Ctheta%20%7D%28e%5E%7B-%5Ctheta%20%5Cleft%20%7C%20t-s%20%5Cright%20%7C%7D-e%5E%7B-%5Ctheta%20%5Cleft%20%7C%20t&amp;plus;s%20%5Cright%20%7C%7D%29\"/></p>\n<p>(证明：由于均值为0，所以<img alt=\"Cov(N_{t},N_{s})=E[N_{t}N_{s}]=\\sigma ^{2}e^{-\\theta \\left | t+s \\right |}E\\left [ \\int_{0}^{t}e^{\\theta \\tau }dB_{t}\\int_{0}^{s}e^{\\theta \\tau }dB_{t} \\right ]\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?Cov%28N_%7Bt%7D%2CN_%7Bs%7D%29%3DE%5BN_%7Bt%7DN_%7Bs%7D%5D%3D%5Csigma%20%5E%7B2%7De%5E%7B-%5Ctheta%20%5Cleft%20%7C%20t&amp;plus;s%20%5Cright%20%7C%7DE%5Cleft%20%5B%20%5Cint_%7B0%7D%5E%7Bt%7De%5E%7B%5Ctheta%20%5Ctau%20%7DdB_%7Bt%7D%5Cint_%7B0%7D%5E%7Bs%7De%5E%7B%5Ctheta%20%5Ctau%20%7DdB_%7Bt%7D%20%5Cright%20%5D\"/>。另外，Ito Isometry告诉我们<img alt=\"E\\left [ \\int_{0}^{t}e^{\\theta \\tau }dB_{t}\\int_{0}^{s}e^{\\theta \\tau }dB_{t} \\right ]=E\\left [ \\int_{0}^{min(t,s)} e^{2\\theta \\tau }d\\tau \\right ]\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?E%5Cleft%20%5B%20%5Cint_%7B0%7D%5E%7Bt%7De%5E%7B%5Ctheta%20%5Ctau%20%7DdB_%7Bt%7D%5Cint_%7B0%7D%5E%7Bs%7De%5E%7B%5Ctheta%20%5Ctau%20%7DdB_%7Bt%7D%20%5Cright%20%5D%3DE%5Cleft%20%5B%20%5Cint_%7B0%7D%5E%7Bmin%28t%2Cs%29%7D%20e%5E%7B2%5Ctheta%20%5Ctau%20%7Dd%5Ctau%20%5Cright%20%5D\"/>，所以<img alt=\"Cov(N_{t},N_{s})=\\sigma ^{2}e^{-\\theta \\left | t+s \\right |}\\int_{0}^{min(t,s)}e^{2\\theta \\tau }d\\tau\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?Cov%28N_%7Bt%7D%2CN_%7Bs%7D%29%3D%5Csigma%20%5E%7B2%7De%5E%7B-%5Ctheta%20%5Cleft%20%7C%20t&amp;plus;s%20%5Cright%20%7C%7D%5Cint_%7B0%7D%5E%7Bmin%28t%2Cs%29%7De%5E%7B2%5Ctheta%20%5Ctau%20%7Dd%5Ctau\"/>，进一步化简可得结果。)</p>\n<p>对于<img alt=\"t\\neq s\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?t%5Cneq%20s\"/>总有<img alt=\"\\left | t-s \\right |&lt;t+s\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?%5Cleft%20%7C%20t-s%20%5Cright%20%7C%3Ct&amp;plus;s\"/>，所以 <img alt=\"Cov(N_{t},N_{s})&gt;0\" class=\"mathcode\" src=\"https://latex.codecogs.com/gif.latex?Cov%28N_%7Bt%7D%2CN_%7Bs%7D%29%3E0\"/>。据此可知，使用Ornstein Uhlenbeck过程让相邻扰动正相关，进而让动作向相近的方向偏移。</p>\n<p>OU噪声的代码实现：</p>\n<pre><code class=\"language-python\">class OUActionNoise:\n    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n        self.theta = theta\n        self.mu = mu\n        self.sigma = sigma\n        self.dt = dt\n        self.x0 = x0\n        self.reset()\n\n    def __call__(self):\n        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n        self.x_prev = x\n\n        return x\n\n    def reset(self):\n        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)</code></pre>\n<p>看完OU噪声后，可能很多小伙伴是懵的，这个也太复杂了。不过我会告诉大家，<strong>其实OU噪声是没必要的，因为我们完全可以采用服从正态分布的噪声来取代它，实验结果也证实了这一点。</strong>因此Twin Delayed Deep Deterministic policy gradient (TD3)算法舍弃了OU噪声，而是采用服从正态分布的噪声，实现起来更加简单。</p>\n<p><strong>另外还需要提醒大家一点：噪声只会加在训练阶段Actor网络输出的动作上，推理阶段不要加上噪声，以及在更新网络参数时也不要加上噪声，前面已经提醒过了。</strong>因为我们只需要在训练阶段让智能体具备探索能力，推理时是不需要的。</p>\n<h1 id=\"3%20DDPG%E7%AE%97%E6%B3%95%E4%BC%AA%E4%BB%A3%E7%A0%81\">3 DDPG算法伪代码</h1>\n<p style=\"text-align:center;\"><img alt=\"\" src=\"..\\..\\static\\image\\ec654947dde345b6bfc2bfb93003f6f5.png\"/></p>\n<h1 id=\"%C2%A04%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0\"> 4 代码实现</h1>\n<p>Actor和Critic网络的代码实现（networks.py）:</p>\n<pre><code class=\"language-python\">import torch as T\nimport torch.nn as nn\nimport torch.optim as optim\n\ndevice = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n\n\ndef weight_init(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_normal_(m.weight)\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif isinstance(m, nn.BatchNorm1d):\n        nn.init.constant_(m.weight, 1.0)\n        nn.init.constant_(m.bias, 0.0)\n\n\nclass ActorNetwork(nn.Module):\n    def __init__(self, alpha, state_dim, action_dim, fc1_dim, fc2_dim):\n        super(ActorNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, fc1_dim)\n        self.ln1 = nn.LayerNorm(fc1_dim)\n        self.fc2 = nn.Linear(fc1_dim, fc2_dim)\n        self.ln2 = nn.LayerNorm(fc2_dim)\n        self.action = nn.Linear(fc2_dim, action_dim)\n\n        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n        self.apply(weight_init)\n        self.to(device)\n\n    def forward(self, state):\n        x = T.relu(self.ln1(self.fc1(state)))\n        x = T.relu(self.ln2(self.fc2(x)))\n        action = T.tanh(self.action(x))\n\n        return action\n\n    def save_checkpoint(self, checkpoint_file):\n        T.save(self.state_dict(), checkpoint_file)\n\n    def load_checkpoint(self, checkpoint_file):\n        self.load_state_dict(T.load(checkpoint_file))\n\n\nclass CriticNetwork(nn.Module):\n    def __init__(self, beta, state_dim, action_dim, fc1_dim, fc2_dim):\n        super(CriticNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, fc1_dim)\n        self.ln1 = nn.LayerNorm(fc1_dim)\n        self.fc2 = nn.Linear(fc1_dim, fc2_dim)\n        self.ln2 = nn.LayerNorm(fc2_dim)\n        self.fc3 = nn.Linear(action_dim, fc2_dim)\n        self.q = nn.Linear(fc2_dim, 1)\n\n        self.optimizer = optim.Adam(self.parameters(), lr=beta, weight_decay=0.001)\n        self.apply(weight_init)\n        self.to(device)\n\n    def forward(self, state, action):\n        x_s = T.relu(self.ln1(self.fc1(state)))\n        x_s = self.ln2(self.fc2(x_s))\n        x_a = self.fc3(action)\n        x = T.relu(x_s + x_a)\n        q = self.q(x)\n\n        return q\n\n    def save_checkpoint(self, checkpoint_file):\n        T.save(self.state_dict(), checkpoint_file)\n\n    def load_checkpoint(self, checkpoint_file):\n        self.load_state_dict(T.load(checkpoint_file))\n\n</code></pre>\n<p>注：<strong>Actor和Critic网络中前面两个Linear层后面都跟上了Layer Normalization (LN)层。</strong>因为我在实验时发现了一个非常有趣的现象，如果不加LN层，或者加入Batch Normalization (BN)层，整个训练过程很容易坍塌或者训练效果很差，具体原因我也不是特别清楚。感兴趣的小伙伴可以把代码git下来跑一遍，如果知道原因的话不妨一起交流。</p>\n<p>DDPG算法的代码实现（DDPG.py）:</p>\n<pre><code class=\"language-python\">import torch as T\nimport torch.nn.functional as F\nimport numpy as np\nfrom networks import ActorNetwork, CriticNetwork\nfrom buffer import ReplayBuffer\n\ndevice = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n\n\nclass DDPG:\n    def __init__(self, alpha, beta, state_dim, action_dim, actor_fc1_dim,\n                 actor_fc2_dim, critic_fc1_dim, critic_fc2_dim, ckpt_dir,\n                 gamma=0.99, tau=0.005, action_noise=0.1, max_size=1000000,\n                 batch_size=256):\n        self.gamma = gamma\n        self.tau = tau\n        self.action_noise = action_noise\n        self.checkpoint_dir = ckpt_dir\n\n        self.actor = ActorNetwork(alpha=alpha, state_dim=state_dim, action_dim=action_dim,\n                                  fc1_dim=actor_fc1_dim, fc2_dim=actor_fc2_dim)\n        self.target_actor = ActorNetwork(alpha=alpha, state_dim=state_dim, action_dim=action_dim,\n                                         fc1_dim=actor_fc1_dim, fc2_dim=actor_fc2_dim)\n        self.critic = CriticNetwork(beta=beta, state_dim=state_dim, action_dim=action_dim,\n                                    fc1_dim=critic_fc1_dim, fc2_dim=critic_fc2_dim)\n        self.target_critic = CriticNetwork(beta=beta, state_dim=state_dim, action_dim=action_dim,\n                                           fc1_dim=critic_fc1_dim, fc2_dim=critic_fc2_dim)\n\n        self.memory = ReplayBuffer(max_size=max_size, state_dim=state_dim, action_dim=action_dim,\n                                   batch_size=batch_size)\n\n        self.update_network_parameters(tau=1.0)\n\n    def update_network_parameters(self, tau=None):\n        if tau is None:\n            tau = self.tau\n\n        for actor_params, target_actor_params in zip(self.actor.parameters(),\n                                                     self.target_actor.parameters()):\n            target_actor_params.data.copy_(tau * actor_params + (1 - tau) * target_actor_params)\n\n        for critic_params, target_critic_params in zip(self.critic.parameters(),\n                                                       self.target_critic.parameters()):\n            target_critic_params.data.copy_(tau * critic_params + (1 - tau) * target_critic_params)\n\n    def remember(self, state, action, reward, state_, done):\n        self.memory.store_transition(state, action, reward, state_, done)\n\n    def choose_action(self, observation, train=True):\n        self.actor.eval()\n        state = T.tensor([observation], dtype=T.float).to(device)\n        action = self.actor.forward(state).squeeze()\n\n        if train:\n            noise = T.tensor(np.random.normal(loc=0.0, scale=self.action_noise),\n                             dtype=T.float).to(device)\n            action = T.clamp(action+noise, -1, 1)\n        self.actor.train()\n\n        return action.detach().cpu().numpy()\n\n    def learn(self):\n        if not self.memory.ready():\n            return\n\n        states, actions, reward, states_, terminals = self.memory.sample_buffer()\n        states_tensor = T.tensor(states, dtype=T.float).to(device)\n        actions_tensor = T.tensor(actions, dtype=T.float).to(device)\n        rewards_tensor = T.tensor(reward, dtype=T.float).to(device)\n        next_states_tensor = T.tensor(states_, dtype=T.float).to(device)\n        terminals_tensor = T.tensor(terminals).to(device)\n\n        with T.no_grad():\n            next_actions_tensor = self.target_actor.forward(next_states_tensor)\n            q_ = self.target_critic.forward(next_states_tensor, next_actions_tensor).view(-1)\n            q_[terminals_tensor] = 0.0\n            target = rewards_tensor + self.gamma * q_\n        q = self.critic.forward(states_tensor, actions_tensor).view(-1)\n\n        critic_loss = F.mse_loss(q, target.detach())\n        self.critic.optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic.optimizer.step()\n\n        new_actions_tensor = self.actor.forward(states_tensor)\n        actor_loss = -T.mean(self.critic(states_tensor, new_actions_tensor))\n        self.actor.optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor.optimizer.step()\n\n        self.update_network_parameters()\n\n    def save_models(self, episode):\n        self.actor.save_checkpoint(self.checkpoint_dir + 'Actor/DDPG_actor_{}.pth'.format(episode))\n        print('Saving actor network successfully!')\n        self.target_actor.save_checkpoint(self.checkpoint_dir +\n                                          'Target_actor/DDPG_target_actor_{}.pth'.format(episode))\n        print('Saving target_actor network successfully!')\n        self.critic.save_checkpoint(self.checkpoint_dir + 'Critic/DDPG_critic_{}'.format(episode))\n        print('Saving critic network successfully!')\n        self.target_critic.save_checkpoint(self.checkpoint_dir +\n                                           'Target_critic/DDPG_target_critic_{}'.format(episode))\n        print('Saving target critic network successfully!')\n\n    def load_models(self, episode):\n        self.actor.load_checkpoint(self.checkpoint_dir + 'Actor/DDPG_actor_{}.pth'.format(episode))\n        print('Loading actor network successfully!')\n        self.target_actor.load_checkpoint(self.checkpoint_dir +\n                                          'Target_actor/DDPG_target_actor_{}.pth'.format(episode))\n        print('Loading target_actor network successfully!')\n        self.critic.load_checkpoint(self.checkpoint_dir + 'Critic/DDPG_critic_{}'.format(episode))\n        print('Loading critic network successfully!')\n        self.target_critic.load_checkpoint(self.checkpoint_dir +\n                                           'Target_critic/DDPG_target_critic_{}'.format(episode))\n        print('Loading target critic network successfully!')\n</code></pre>\n<p>算法仿真环境是gym库中的LunarLanderContinuous-v2环境，因此需要先配置好gym库。进入Aanconda中对应的Python环境中，执行下面的指令</p>\n<pre><code class=\"language-python\">pip install gym</code></pre>\n<p>但是，这样安装的gym库只包括少量的内置环境，如算法环境、简单文字游戏环境和经典控制环境，无法使用LunarLanderContinuous-v2。因此还要安装一些其他依赖项，具体可以参照这篇blog: <a class=\"link-info\" href=\"https://blog.csdn.net/weixin_46133643/article/details/121842866?spm=1001.2014.3001.5501\" title=\"AttributeError: module ‘gym.envs.box2d‘ has no attribute ‘LunarLander‘ 解决办法\">AttributeError: module ‘gym.envs.box2d‘ has no attribute ‘LunarLander‘ 解决办法</a>。如果已经配置好环境，那请忽略这一段。</p>\n<p>训练脚本（train.py）:</p>\n<pre><code class=\"language-python\">import gym\nimport numpy as np\nimport argparse\nfrom DDPG import DDPG\nfrom utils import create_directory, plot_learning_curve, scale_action\n\nparser = argparse.ArgumentParser(\"DDPG parameters\")\nparser.add_argument('--max_episodes', type=int, default=1000)\nparser.add_argument('--checkpoint_dir', type=str, default='./checkpoints/DDPG/')\nparser.add_argument('--figure_file', type=str, default='./output_images/reward.png')\n\nargs = parser.parse_args()\n\n\ndef main():\n    env = gym.make('LunarLanderContinuous-v2')\n    agent = DDPG(alpha=0.0003, beta=0.0003, state_dim=env.observation_space.shape[0],\n                 action_dim=env.action_space.shape[0], actor_fc1_dim=400, actor_fc2_dim=300,\n                 critic_fc1_dim=400, critic_fc2_dim=300, ckpt_dir=args.checkpoint_dir,\n                 batch_size=256)\n    create_directory(args.checkpoint_dir,\n                     sub_paths=['Actor', 'Target_actor', 'Critic', 'Target_critic'])\n\n    reward_history = []\n    avg_reward_history = []\n    for episode in range(args.max_episodes):\n        done = False\n        total_reward = 0\n        observation = env.reset()\n        while not done:\n            action = agent.choose_action(observation, train=True)\n            action_ = scale_action(action.copy(), env.action_space.high, env.action_space.low)\n            observation_, reward, done, info = env.step(action_)\n            agent.remember(observation, action, reward, observation_, done)\n            agent.learn()\n            total_reward += reward\n            observation = observation_\n\n        reward_history.append(total_reward)\n        avg_reward = np.mean(reward_history[-100:])\n        avg_reward_history.append(avg_reward)\n        print('Ep: {} Reward: {:.1f} AvgReward: {:.1f}'.format(episode+1, total_reward, avg_reward))\n\n        if (episode + 1) % 200 == 0:\n            agent.save_models(episode+1)\n\n    episodes = [i+1 for i in range(args.max_episodes)]\n    plot_learning_curve(episodes, avg_reward_history, title='AvgReward',\n                        ylabel='reward', figure_file=args.figure_file)\n\n\nif __name__ == '__main__':\n    main()</code></pre>\n<p>训练脚本中有三个参数，max_episodes表示训练幕数，checkpoint_dir表示训练权重保存路径，figure_file表示训练结果的保存路径（其实是一张累积奖励曲线图），按照默认设置即可。</p>\n<p>训练时还会用到画图函数和创建文件夹函数，它们都被放在utils.py脚本中：</p>\n<pre><code class=\"language-python\">import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass OUActionNoise:\n    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n        self.theta = theta\n        self.mu = mu\n        self.sigma = sigma\n        self.dt = dt\n        self.x0 = x0\n        self.reset()\n\n    def __call__(self):\n        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n        self.x_prev = x\n\n        return x\n\n    def reset(self):\n        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n\n\ndef create_directory(path: str, sub_paths: list):\n    for sub_path in sub_paths:\n        if not os.path.exists(path + sub_path):\n            os.makedirs(path + sub_path, exist_ok=True)\n            print('Create path: {} successfully'.format(path+sub_path))\n        else:\n            print('Path: {} is already existence'.format(path+sub_path))\n\n\ndef plot_learning_curve(episodes, records, title, ylabel, figure_file):\n    plt.figure()\n    plt.plot(episodes, records, color='r', linestyle='-')\n    plt.title(title)\n    plt.xlabel('episode')\n    plt.ylabel(ylabel)\n\n    plt.show()\n    plt.savefig(figure_file)\n\n\ndef scale_action(action, high, low):\n    action = np.clip(action, -1, 1)\n    weight = (high - low) / 2\n    bias = (high + low) / 2\n    action_ = action * weight + bias\n\n    return action_\n</code></pre>\n<p>另外我们还提供了测试代码，主要用于测试训练效果以及观察环境的动态渲染 (test.py)：</p>\n<pre><code class=\"language-python\">import gym\nimport imageio\nimport argparse\nfrom DDPG import DDPG\nfrom utils import scale_action\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--filename', type=str, default='./output_images/LunarLander.gif')\nparser.add_argument('--checkpoint_dir', type=str, default='./checkpoints/DDPG/')\nparser.add_argument('--save_video', type=bool, default=True)\nparser.add_argument('--fps', type=int, default=30)\nparser.add_argument('--render', type=bool, default=True)\n\nargs = parser.parse_args()\n\n\ndef main():\n    env = gym.make('LunarLanderContinuous-v2')\n    agent = DDPG(alpha=0.0003, beta=0.0003, state_dim=env.observation_space.shape[0],\n                 action_dim=env.action_space.shape[0], actor_fc1_dim=400, actor_fc2_dim=300,\n                 critic_fc1_dim=400, critic_fc2_dim=300, ckpt_dir=args.checkpoint_dir,\n                 batch_size=256)\n    agent.load_models(1000)\n    video = imageio.get_writer(args.filename, fps=args.fps)\n\n    done = False\n    observation = env.reset()\n    while not done:\n        if args.render:\n            env.render()\n        action = agent.choose_action(observation, train=True)\n        action_ = scale_action(action.copy(), env.action_space.high, env.action_space.low)\n        observation_, reward, done, info = env.step(action_)\n        observation = observation_\n        if args.save_video:\n            video.append_data(env.render(mode='rgb_array'))\n\n\nif __name__ == '__main__':\n    main()</code></pre>\n<p>测试脚本中包括五个参数，filename表示环境动态图的保存路径，checkpoint_dir表示加载的权重路径，save_video表示是否要保存动态图，fps表示动态图的帧率，rander表示是否开启环境渲染。大家只需要调整save_video和rander这两个参数，其余保持默认即可。</p>\n<h1 id=\"5%20%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C\">5 实验结果</h1>\n<p style=\"text-align:center;\"><img alt=\"\" src=\"..\\..\\static\\image\\3616f9ca3f8046c4807ef7c327feaaf2.png\"/></p>\n<p>通过平均奖励曲线可以看出，大概迭代到700步左右时算法趋于收敛。 </p>\n<p style=\"text-align:center;\"><img alt=\"\" src=\"https://img-blog.csdnimg.cn/8a540e832ecd409cb9df9b608166232f.gif\"/></p>\n<p>这是测试效果图，智能体能够很好地完成降落任务！ </p>\n<h1 id=\"6%20%E7%BB%93%E8%AE%BA\">6 结论</h1>\n<p>本文主要讲解了DDPG算法的原理以及代码实现。尽管它是一个非常优秀的算法，但是仍然存在一些问题需要改进，例如过估计。后面我们会讲解一下TD3算法，它其实就是在DDPG算法的基础做了一些改进工作，克服了DDPG算法中的一些问题，从而让算法的性能得到显著提升。</p>\n<p><strong>以上如果有出现错误的地方，欢迎各位怒斥！</strong></p>\n</div>\n</div>", "first_tag": "Python", "cpp": 0, "csharp": 0, "python": 1, "javascript": 0, "java": 0, "sql": 0, "php": 0, "time": "2022-05-02 22:50:03", "summary": "深度强化学习算法原理与代码引言算法简介算法原理经验回放目标网络算法更新过程目标网络的更新引入目标网络的目的噪声探索算法伪代码代码实现实验结果结论引言算法是团队提出的一种专门用于解决连续控制问题的在线式"}