{"blogid": "123495329", "writerAge": "码龄4年", "writerBlogNum": "116", "writerCollect": "743", "writerComment": "150", "writerFan": "5202", "writerGrade": "4级", "writerIntegral": "1595", "writerName": "小馨馨的小翟", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_123495329.jpg", "writerRankTotal": "10854", "writerRankWeekly": "1762", "writerThumb": "199", "writerVisitNum": "120171", "blog_read_count": "6153", "blog_time": "已于 2022-04-27 15:59:40 修改", "blog_title": "手撕Alexnet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<p><strong> Alexnet网络详解代码：</strong><a href=\"https://blog.csdn.net/qq_43215597/article/details/123495329?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165104333916782388030041%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165104333916782388030041&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-123495329.nonecase&amp;utm_term=alexnet&amp;spm=1018.2226.3001.4450\" title=\"手撕Alexnet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客_alexnet神经网络代码\">手撕Alexnet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客_alexnet神经网络代码</a></p>\n<p><strong>VGG网络详解代码：</strong> <a href=\"https://blog.csdn.net/qq_43215597/article/details/123578789?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165104342016781685397692%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165104342016781685397692&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-4-123578789.nonecase&amp;utm_term=VGG&amp;spm=1018.2226.3001.4450\" title=\"手撕VGG卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客\">手撕VGG卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客</a></p>\n<p><strong>Resnet网络详解代码：</strong> <a href=\"https://blog.csdn.net/qq_43215597/article/details/123757602?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165104344616780366521059%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165104344616780366521059&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-123757602.nonecase&amp;utm_term=rESNET&amp;spm=1018.2226.3001.4450\" title=\"手撕Resnet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客\">手撕Resnet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客</a></p>\n<p><strong>Googlenet网络详解代码：</strong><a href=\"https://blog.csdn.net/qq_43215597/article/details/124061070?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165104343616782246433545%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165104343616782246433545&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-124061070.nonecase&amp;utm_term=GOOGLENET&amp;spm=1018.2226.3001.4450\" title=\"手撕Googlenet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客_cnn测试集准确率低\">手撕Googlenet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客_cnn测试集准确率低</a></p>\n<p><strong>集成学习模型融合网络详解代码： </strong></p>\n<p><a href=\"https://blog.csdn.net/qq_43215597/article/details/123907732?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165104348416782388059672%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165104348416782388059672&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-3-123907732.nonecase&amp;utm_term=ALEXNET&amp;spm=1018.2226.3001.4450\" title=\"集成学习-模型融合（Lenet，Alexnet，Vgg）三个模型进行融合-附源代码-宇宙的尽头一定是融合模型而不是单个模型。_小馨馨的小翟的博客-CSDN博客_torch模型融合\">集成学习-模型融合（Lenet，Alexnet，Vgg）三个模型进行融合-附源代码-宇宙的尽头一定是融合模型而不是单个模型。_小馨馨的小翟的博客-CSDN博客_torch模型融合</a></p>\n<p><strong>深度学习常用数据增强，数据扩充代码数据缩放代码： </strong></p>\n<p><a href=\"https://blog.csdn.net/qq_43215597/article/details/123905879?spm=1001.2014.3001.5501\" title=\"深度学习数据增强方法-内含（亮度增强，对比度增强，旋转图图像，翻转图像，仿射变化扩充图像，错切变化扩充图像，HSV数据增强）七种方式进行增强-每种扩充一张实现7倍扩）+ 图像缩放代码-批量_小馨馨的小翟的博客-CSDN博客_训练数据增强\">深度学习数据增强方法-内含（亮度增强，对比度增强，旋转图图像，翻转图像，仿射变化扩充图像，错切变化扩充图像，HSV数据增强）七种方式进行增强-每种扩充一张实现7倍扩）+ 图像缩放代码-批量_小馨馨的小翟的博客-CSDN博客_训练数据增强</a></p>\n<p></p>\n<p><strong>        </strong></p>\n<p><strong>Alexnet卷积神经网络是由谷歌Hinton率领的团队在2010年的Imagnet图像识别大赛获得冠军的一个卷积神经网络，该网络放到现在可能相对比较简单，但是也不失为用于入门深度学习的卷积神经网络。</strong></p>\n<p><strong>       对于网上很多的Alexnet的代码写的不够详细，比如没有详细的写出同时画出训练集的loss accuracy 和测试集的loss和accuracy的折线图，因此这些详细的使用pytorch框架复现了一下Alexnet代码，并且对于我们需要的loss和accuracy的折线图用matplotlib进行了绘制。</strong></p>\n<p><strong>Alexnet卷积神经网络的结构图如下：</strong></p>\n<p style=\"text-align:center;\"><img alt=\"\" src=\"..\\..\\static\\image\\378e3a39e1b9c412bc33c1bca9ab435b.png\"/></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p><strong>导入库</strong></p>\n<pre><code>import torch\nimport torchvision\nimport torchvision.models\n\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms\n\nfrom PIL import ImageFile\n</code></pre>\n<p><strong>图像预处理-缩放裁剪等等</strong></p>\n<pre><code>data_transform = {\n    \"train\": transforms.Compose([transforms.RandomResizedCrop(120),\n                                 transforms.RandomHorizontalFlip(),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n    \"val\": transforms.Compose([transforms.Resize((120, 120)), #这种预处理的地方尽量别修改，修改意味着需要修改网络结构的参数，如果新手的话请勿修改\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}</code></pre>\n<p><strong>导入自己的数据，自己的数据放在跟代码相同的文件夹下新建一个data文件夹，data文件夹里的新建一个train文件夹用于放置训练集的图片。同理新建一个val文件夹用于放置测试集的图片。</strong></p>\n<pre><code>    train_data = torchvision.datasets.ImageFolder(root = \"./data/train\" ,   transform = data_transform[\"train\"])\n\n\n\n    traindata = DataLoader(dataset= train_data , batch_size= 32 , shuffle= True , num_workers=0 )\n\n    # test_data = torchvision.datasets.CIFAR10(root = \"./data\" , train = False ,download = False,\n    #                                           transform = trans)\n    test_data = torchvision.datasets.ImageFolder(root = \"./data/val\" , transform = data_transform[\"val\"])\n\n    train_size = len(train_data)#求出训练集的长度\n    test_size = len(test_data)  #求出测试集的长度\n    print(train_size)  #输出训练集的长度\n    print(test_size)   #输出测试集的长度\n    testdata = DataLoader(dataset = test_data , batch_size= 32 , shuffle= True , num_workers=0 )#windows系统下，num_workers设置为0，linux系统下可以设置多进程</code></pre>\n<p><strong>设置调用GPU，如果有GPU就调用GPU，如果没有GPU则调用CPU训练模型</strong></p>\n<pre><code>    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"using {} device.\".format(device))</code></pre>\n<p><strong>Alexnet卷积神经网络</strong></p>\n<pre><code>    class alexnet(nn.Module):\n        def __init__(self):\n            super(alexnet , self).__init__()\n            self.model = nn.Sequential(\n\n                nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input[3, 120, 120]  output[48, 55, 55]\n\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),  # output[48, 27, 27]\n                nn.Conv2d(48, 128, kernel_size=5, padding=2),  # output[128, 27, 27]\n\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 13, 13]\n                nn.Conv2d(128, 192, kernel_size=3, padding=1),  # output[192, 13, 13]\n\n                nn.ReLU(inplace=True),\n                nn.Conv2d(192, 192, kernel_size=3, padding=1),  # output[192, 13, 13]\n\n                nn.ReLU(inplace=True),\n                nn.Conv2d(192, 128, kernel_size=3, padding=1),  # output[128, 13, 13]\n\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 6, 6]\n                nn.Flatten(),\n                nn.Dropout(p=0.5),\n                nn.Linear(512, 2048),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=0.5),\n                nn.Linear(2048, 1024),\n                nn.ReLU(inplace=True),\n                nn.Linear(1024, 7),  #自己的数据集是几种，这个7就设置为几\n\n            )\n        def forward(self , x):\n            x = self.model(x)\n            return x\n</code></pre>\n<p><strong>启动模型，将模型放入GPU</strong></p>\n<pre><code>    alexnet1 = alexnet()\n    print(alexnet1)\n    alexnet1.to(device)  #将模型放入GPU\n    test1 = torch.ones(64, 3, 120, 120)  #输入数据测试一下模型能不能跑\n\n    test1 = alexnet1(test1.to(device))\n    print(test1.shape)</code></pre>\n<p><strong>设置训练需要的参数，epoch，学习率learning 优化器。损失函数。</strong></p>\n<pre><code>    epoch  = 10#这里是训练的轮数\n    learning = 0.0001 #学习率\n    optimizer = torch.optim.Adam(alexnet1.parameters(), lr = learning)#优化器\n    loss = nn.CrossEntropyLoss()#损失函数</code></pre>\n<p><strong>设置四个空数组，用来存放训练集的loss和accuracy    测试集的loss和 accuracy</strong></p>\n<pre><code>    train_loss_all = []\n    train_accur_all = []\n    test_loss_all = []\n    test_accur_all = []</code></pre>\n<p><strong>开始训练：</strong></p>\n<pre><code>    for i in range(epoch):\n        train_loss = 0\n        train_num = 0.0\n        train_accuracy = 0.0\n        alexnet1.train()\n        train_bar = tqdm(traindata)\n        for step , data in enumerate(train_bar):\n            img , target = data\n            optimizer.zero_grad()\n            outputs = alexnet1(img.to(device))\n\n            loss1  = loss(outputs , target.to(device))\n            outputs = torch.argmax(outputs, 1)\n            loss1.backward()\n            optimizer.step()\n            train_loss += abs(loss1.item())*img.size(0)\n            accuracy = torch.sum(outputs == target.to(device))\n            train_accuracy = train_accuracy + accuracy\n            train_num += img.size(0)\n\n        print(\"epoch：{} ， train-Loss：{} , train-accuracy：{}\".format(i+1 , train_loss/train_num , train_accuracy/train_num))\n        train_loss_all.append(train_loss/train_num)\n        train_accur_all.append(train_accuracy.double().item()/train_num)</code></pre>\n<p><strong>开始测试：</strong></p>\n<pre><code>        test_loss = 0\n        test_accuracy = 0.0\n        test_num = 0\n        alexnet1.eval()\n        with torch.no_grad():\n            test_bar = tqdm(testdata)\n            for data in test_bar:\n                img , target = data\n\n                outputs = alexnet1(img.to(device))\n\n                loss2 = loss(outputs, target.to(device))\n                outputs = torch.argmax(outputs, 1)\n                test_loss = test_loss + abs(loss2.item())*img.size(0)\n                accuracy = torch.sum(outputs == target.to(device))\n                test_accuracy = test_accuracy + accuracy\n                test_num += img.size(0)\n\n        print(\"test-Loss：{} , test-accuracy：{}\".format(test_loss / test_num, test_accuracy / test_num))\n        test_loss_all.append(test_loss/test_num)\n        test_accur_all.append(test_accuracy.double().item()/test_num)\n</code></pre>\n<p><strong>开始测试：</strong></p>\n<pre><code>        test_loss = 0\n        test_accuracy = 0.0\n        test_num = 0\n        alexnet1.eval()\n        with torch.no_grad():\n            test_bar = tqdm(testdata)\n            for data in test_bar:\n                img , target = data\n\n                outputs = alexnet1(img.to(device))\n\n                loss2 = loss(outputs, target.to(device))\n                outputs = torch.argmax(outputs, 1)\n                test_loss = test_loss + abs(loss2.item())*img.size(0)\n                accuracy = torch.sum(outputs == target.to(device))\n                test_accuracy = test_accuracy + accuracy\n                test_num += img.size(0)\n\n        print(\"test-Loss：{} , test-accuracy：{}\".format(test_loss / test_num, test_accuracy / test_num))\n        test_loss_all.append(test_loss/test_num)\n        test_accur_all.append(test_accuracy.double().item()/test_num)</code></pre>\n<p><strong>绘制训练集loss和accuracy图 和测试集的loss和accuracy图</strong></p>\n<pre><code>    plt.figure(figsize=(12,4))\n    plt.subplot(1 , 2 , 1)\n    plt.plot(range(epoch) , train_loss_all,\n             \"ro-\",label = \"Train loss\")\n    plt.plot(range(epoch), test_loss_all,\n             \"bs-\",label = \"test loss\")\n    plt.legend()\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"Loss\")\n    plt.subplot(1, 2, 2)\n    plt.plot(range(epoch) , train_accur_all,\n             \"ro-\",label = \"Train accur\")\n    plt.plot(range(epoch) , test_accur_all,\n             \"bs-\",label = \"test accur\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"acc\")\n    plt.legend()\n    plt.show()\n\n    torch.save(alexnet1.state_dict(), \"alexnet.pth\")\n\n    print(\"模型已保存\")\n</code></pre>\n<p><strong>全部train训练代码：</strong></p>\n<pre><code>import torch\nimport torchvision\nimport torchvision.models\n\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms\n\nfrom PIL import ImageFile\n\ndata_transform = {\n    \"train\": transforms.Compose([transforms.RandomResizedCrop(120),\n                                 transforms.RandomHorizontalFlip(),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n    \"val\": transforms.Compose([transforms.Resize((120, 120)),  # cannot 224, must (224, 224)\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n# train_data = torchvision.datasets.CIFAR10(root = \"./data\" , train = True ,download = True,\n#                                           transform = trans)\n\n\n\n\ndef main():\n    train_data = torchvision.datasets.ImageFolder(root = \"./data/train\" ,   transform = data_transform[\"train\"])\n\n\n\n    traindata = DataLoader(dataset= train_data , batch_size= 128 , shuffle= True , num_workers=0 )\n\n    # test_data = torchvision.datasets.CIFAR10(root = \"./data\" , train = False ,download = False,\n    #                                           transform = trans)\n    test_data = torchvision.datasets.ImageFolder(root = \"./data/val\" , transform = data_transform[\"val\"])\n\n    train_size = len(train_data)\n    test_size = len(test_data)\n    print(train_size)\n    print(test_size)\n    testdata = DataLoader(dataset = test_data , batch_size= 128 , shuffle= True , num_workers=0 )\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"using {} device.\".format(device))\n\n\n    class alexnet(nn.Module):\n        def __init__(self):\n            super(alexnet , self).__init__()\n            self.model = nn.Sequential(\n\n                nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input[3, 120, 120]  output[48, 55, 55]\n\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),  # output[48, 27, 27]\n                nn.Conv2d(48, 128, kernel_size=5, padding=2),  # output[128, 27, 27]\n\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 13, 13]\n                nn.Conv2d(128, 192, kernel_size=3, padding=1),  # output[192, 13, 13]\n\n                nn.ReLU(inplace=True),\n                nn.Conv2d(192, 192, kernel_size=3, padding=1),  # output[192, 13, 13]\n\n                nn.ReLU(inplace=True),\n                nn.Conv2d(192, 128, kernel_size=3, padding=1),  # output[128, 13, 13]\n\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 6, 6]\n                nn.Flatten(),\n                nn.Dropout(p=0.5),\n                nn.Linear(512, 2048),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=0.5),\n                nn.Linear(2048, 1024),\n                nn.ReLU(inplace=True),\n                nn.Linear(1024, 7),  #自己的数据是几种，这里的7就改成几\n\n            )\n        def forward(self , x):\n            x = self.model(x)\n            return x\n\n\n\n    alexnet1 = alexnet()\n    print(alexnet1)\n    alexnet1.to(device)\n    test1 = torch.ones(64, 3, 120, 120)\n\n    test1 = alexnet1(test1.to(device))\n    print(test1.shape)\n\n\n\n    epoch  = 10\n    learning = 0.0001\n    optimizer = torch.optim.Adam(alexnet1.parameters(), lr = learning)\n    loss = nn.CrossEntropyLoss()\n\n    train_loss_all = []\n    train_accur_all = []\n    test_loss_all = []\n    test_accur_all = []\n    for i in range(epoch):\n        train_loss = 0\n        train_num = 0.0\n        train_accuracy = 0.0\n        alexnet1.train()\n        train_bar = tqdm(traindata)\n        for step , data in enumerate(train_bar):\n            img , target = data\n            optimizer.zero_grad()\n            outputs = alexnet1(img.to(device))\n\n            loss1  = loss(outputs , target.to(device))\n            outputs = torch.argmax(outputs, 1)\n            loss1.backward()\n            optimizer.step()\n            train_loss += abs(loss1.item())*img.size(0)\n            accuracy = torch.sum(outputs == target.to(device))\n            train_accuracy = train_accuracy + accuracy\n            train_num += img.size(0)\n\n        print(\"epoch：{} ， train-Loss：{} , train-accuracy：{}\".format(i+1 , train_loss/train_num , train_accuracy/train_num))\n        train_loss_all.append(train_loss/train_num)\n        train_accur_all.append(train_accuracy.double().item()/train_num)\n        test_loss = 0\n        test_accuracy = 0.0\n        test_num = 0\n        alexnet1.eval()\n        with torch.no_grad():\n            test_bar = tqdm(testdata)\n            for data in test_bar:\n                img , target = data\n\n                outputs = alexnet1(img.to(device))\n\n                loss2 = loss(outputs, target.to(device))\n                outputs = torch.argmax(outputs, 1)\n                test_loss = test_loss + abs(loss2.item())*img.size(0)\n                accuracy = torch.sum(outputs == target.to(device))\n                test_accuracy = test_accuracy + accuracy\n                test_num += img.size(0)\n\n        print(\"test-Loss：{} , test-accuracy：{}\".format(test_loss / test_num, test_accuracy / test_num))\n        test_loss_all.append(test_loss/test_num)\n        test_accur_all.append(test_accuracy.double().item()/test_num)\n    plt.figure(figsize=(12,4))\n    plt.subplot(1 , 2 , 1)\n    plt.plot(range(epoch) , train_loss_all,\n             \"ro-\",label = \"Train loss\")\n    plt.plot(range(epoch), test_loss_all,\n             \"bs-\",label = \"test loss\")\n    plt.legend()\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"Loss\")\n    plt.subplot(1, 2, 2)\n    plt.plot(range(epoch) , train_accur_all,\n             \"ro-\",label = \"Train accur\")\n    plt.plot(range(epoch) , test_accur_all,\n             \"bs-\",label = \"test accur\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"acc\")\n    plt.legend()\n    plt.show()\n\n    torch.save(alexnet1.state_dict(), \"alexnet.pth\")\n\n    print(\"模型已保存\")\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n<p><strong>全部predict代码：</strong></p>\n<pre><code>import torch\nfrom PIL import Image\nfrom torch import nn\nfrom torchvision.transforms import transforms\n\nimage_path = \"1.JPG\"#将要预测的图片放到跟predict同一个文件夹下\ntrans = transforms.Compose([transforms.Resize((120 , 120)),\n                           transforms.ToTensor()])\nimage = Image.open(image_path)\n\nimage = image.convert(\"RGB\")\nimage = trans(image)\n\nimage = torch.unsqueeze(image, dim=0)\n\nclasses = [\"1\" , \"2\" , \"3\" , \"4\" , \"5\" , \"6\" , \"7\"]#自己是几种，这里就改成自己种类的字符数组\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"using {} device.\".format(device))\nclass alexnet(nn.Module):\n    def __init__(self):\n        super(alexnet, self).__init__()\n        self.model = nn.Sequential(\n\n            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input[3, 120, 120]  output[48, 55, 55]\n\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),  # output[48, 27, 27]\n            nn.Conv2d(48, 128, kernel_size=5, padding=2),  # output[128, 27, 27]\n\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 13, 13]\n            nn.Conv2d(128, 192, kernel_size=3, padding=1),  # output[192, 13, 13]\n\n            nn.ReLU(inplace=True),\n            nn.Conv2d(192, 192, kernel_size=3, padding=1),  # output[192, 13, 13]\n\n            nn.ReLU(inplace=True),\n            nn.Conv2d(192, 128, kernel_size=3, padding=1),  # output[128, 13, 13]\n\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 6, 6]\n            nn.Flatten(),\n            nn.Dropout(p=0.5),\n            nn.Linear(512, 2048),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(2048, 1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 7),#这里就用train把你网络结构代码复制过来即可\n\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\nalexnet1 = alexnet()\nalexnet1.load_state_dict(torch.load(\"alexnet.pth\", map_location=device))#train代码保存的模型要放在跟predict同一个文件夹下\n\noutputs = alexnet1(image)\n\nans = (outputs.argmax(1)).item()\nprint(classes[ans]) #输出的是那种即为预测结果</code></pre>\n<p><strong>Alexnet代码下载链接：链接：https://pan.baidu.com/s/1fc3dSBZN7WsxEjwMxelAwA <br/> 提取码：3dzb</strong></p>\n<p></p>\n<p><strong>有用的话麻烦点一下关注，博主后续会开源更多代码，非常感谢支持！</strong></p>\n</div>\n</div>", "first_tag": "Others", "cpp": 0, "csharp": 0, "python": 0, "javascript": 0, "java": 0, "sql": 0, "php": 0, "time": "2022-04-27 15:59:40", "summary": "网络详解代码：手撕卷积神经网络详细注释版可以直接替换自己数据集直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。小馨馨的小翟的博客博客神经网络代码"}