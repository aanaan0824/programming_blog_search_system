{"blogid": "126743858", "writerAge": "码龄32天", "writerBlogNum": "138", "writerCollect": "8", "writerComment": "3", "writerFan": "477", "writerGrade": "4级", "writerIntegral": "1581", "writerName": "ai智能网络", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_126743858.jpg", "writerRankTotal": "14581", "writerRankWeekly": "2839", "writerThumb": "7", "writerVisitNum": "11918", "blog_read_count": "12", "blog_time": "于 2022-09-07 13:31:17 发布", "blog_title": "神经网络梯度公式推导,神经网络求梯度", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<h2>梯度下降法是什么？</h2>\n<p>梯度下降法（英语：Gradientdescent）是一个一阶最优化算法，通常也称为最陡下降法。</p>\n<p>要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。</p>\n<p>如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。梯度下降一般归功于柯西，他在1847年首次提出它。Hadamard在1907年独立提出了类似的方法。</p>\n<p>HaskellCurry在1944年首先研究了它对非线性优化问题的收敛性，随着该方法在接下来的几十年中得到越来越多的研究和使用，通常也称为最速下降。</p>\n<p>梯度下降适用于任意维数的空间，甚至是无限维的空间。在后一种情况下，搜索空间通常是一个函数空间，并且计算要最小化的函数的Fréchet导数以确定下降方向。</p>\n<p>梯度下降适用于任意数量的维度（至少是有限数量）可以看作是柯西-施瓦茨不等式的结果。那篇文章证明了任意维度的两个向量的内（点）积的大小在它们共线时最大化。</p>\n<p>在梯度下降的情况下，当自变量调整的向量与偏导数的梯度向量成正比时。</p>\n<p>修改为了打破梯度下降的锯齿形模式，动量或重球方法使用动量项，类似于重球在被最小化的函数值的表面上滑动，或牛顿动力学中的质量运动在保守力场中通过粘性介质。</p>\n<p>具有动量的梯度下降记住每次迭代时的解更新，并将下一次更新确定为梯度和前一次更新的线性组合。对于无约束二次极小化，重球法的理论收敛速度界与最优共轭梯度法的理论收敛速度界渐近相同。</p>\n<p>该技术用于随机梯度下降，并作为用于训练人工神经网络的反向传播算法的扩展。</p>\n<p><strong>谷歌人工智能写作项目：神经网络伪原创</strong></p>\n<p><img alt=\"\" src=\"..\\..\\static\\image\\1343ebe46ee142b89e7d10fa2aee3b80.png\"/></p>\n<h2>梯度下降的求解过程</h2>\n<p>顾名思义，梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）<strong><a href=\"http://www.maoxiezuo.com/\" title=\"写作猫\">写作猫</a></strong>。其迭代公式为,其中代表梯度负方向，表示梯度方向上的搜索步长。</p>\n<p>梯度方向我们可以通过对函数求导得到，步长的确定比较麻烦，太大了的话可能会发散，太小收敛速度又太慢。</p>\n<p>一般确定步长的方法是由线性搜索算法来确定，即把下一个点的坐标ak+1看做是的函数，然后求满足f(ak+1)的最小值的即可。</p>\n<p>因为一般情况下，梯度向量为0的话说明是到了一个极值点，此时梯度的幅值也为0.而采用梯度下降算法进行最优化求解时，算法迭代的终止条件是梯度向量的幅值接近0即可，可以设置个非常小的常数阈值。</p>\n<h2>梯度下降法是什么?</h2>\n<p>梯度下降是通过迭代搜索一个函数极小值的优化算法。使用梯度下降，寻找一个函数的局部极小值的过程起始于一个随机点，并向该函数在当前点梯度（或近似梯度）的反方向移动。</p>\n<p>梯度下降算法是一种非常经典的求极小值的算法。</p>\n<p>比如逻辑回归可以用梯度下降进行优化，因为这两个算法的损失函数都是严格意义上的凸函数，即存在全局唯一极小值，较小的学习率和足够的迭代次数，一定可以达到最小值附近，满足精度要求是完全没有问题的。</p>\n<p>并且随着特征数目的增多，梯度下降的效率将远高于去解析标准方程的逆矩阵。常用的梯度下降法有3种不同的形式：（1）批量梯度下降法，简称BGD，使用所有样本，比较耗时。</p>\n<p>（2）随机梯度下降法，简称SGD，随机选择一个样本，简单高效。（3）小批量梯度下降法，简称MBGD，使用少量的样本，这是一个折中的办法。机梯度下降法优点：1、更容易跳出局部最优解。</p>\n<p>2、具有更快的运行速度。</p>\n<h2>什么是梯度下降算法</h2>\n<p>梯度下降是迭代法的一种，梯度下降法是一个最优化算法，通常也称为最速下降法。最速下降法是求解无约束优化问题最简单和最古老的方法之一，常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型。</p>\n<p>梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）。</p>\n<h2>机器学习中的降维算法和梯度下降法</h2>\n<p>机器学习中有很多算法都是十分经典的，比如说降维算法以及梯度下降法，这些方法都能够帮助大家解决很多问题，因此学习机器学习一定要掌握这些算法，而且这些算法都是比较受大家欢迎的。</p>\n<p>在这篇文章中我们就给大家重点介绍一下降维算法和梯度下降法。降维算法首先，来说一说降维算法，降维算法是一种无监督学习算法，其主要特征是将数据从高维降低到低维层次。</p>\n<p>在这里，维度其实表示的是数据的特征量的大小，当特征量大的话，那么就给计算机带来了很大的压力，所以我们可以通过降维计算，把维度高的特征量降到维度低的特征量，比如说从4维的数据压缩到2维。</p>\n<p>类似这样将数据从高维降低到低维有两个好处，第一就是利于表示，第二就是在计算上也能带来加速。当然，有很多降维过程中减少的维度属于肉眼可视的层次，同时压缩也不会带来信息的损失。</p>\n<p>但是如果肉眼不可视，或者没有冗余的特征，这怎么办呢？其实这样的方式降维算法也能工作，不过这样会带来一些信息的损失。不过，降维算法可以从数学上证明，从高维压缩到的低维中最大程度地保留了数据的信息。</p>\n<p>所以说，降维算法还是有很多好处的。那么降维算法的主要作用是什么呢？具体就是压缩数据与提升机器学习其他算法的效率。通过降维算法，可以将具有几千个特征的数据压缩至若干个特征。</p>\n<p>另外，降维算法的另一个好处是数据的可视化。这个优点一直别广泛应用。梯度下降法下面我们给大家介绍一下梯度下降法，所谓梯度下降法就是一个最优化算法，通常也称为最速下降法。</p>\n<p>最速下降法是求解无约束优化问题最简单和最古老的方法之一，虽然现在已经不具有实用性，但是许多有效算法都是以它为基础进行改进和修正而得到的。</p>\n<p>最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。</p>\n<p>好比将函数比作一座山，我们站在某个山坡上，往四周看，从哪个方向向下走一小步，能够下降的最快;当然解决问题的方法有很多，梯度下降只是其中一个，还有很多种方法。</p>\n<p>在这篇文章中我们给大家介绍了关于机器算法中的降维算法以及梯度下降法，这两种方法是机器学习中十分常用的算法，降维算法和梯度下降法都是十分实用的，大家在进行学习机器学习的时候一定要好好学习这两种算法，希望这篇文章能够帮助大家理解这两种算法。</p>\n<h2>根号6+根号2×多少等于1？</h2>\n<p>梯度下降是非常常用的优化算法。作为机器学习的基础知识，这是一个必须要掌握的算法。借助本文，让我们来一起详细了解一下这个算法。</p>\n<p>前言本文的代码可以到我的Github上获取：本文的算法示例通过Python语言实现，在实现中使用到了numpy和matplotlib。如果你不熟悉这两个工具，请自行在网上搜索教程。</p>\n<p>关于优化大多数学习算法都涉及某种形式的优化。优化指的是改变x以最小化或者最大化某个函数的任务。我们通常以最小化指代大多数最优化问题。最大化可经由最小化来实现。</p>\n<p>我们把要最小化或最大化的函数成为目标函数（objectivefunction）或准则（criterion）。</p>\n<p>我们通常使用一个上标*表示最小化或最大化函数的x值，记做这样：[x^*=arg;min;f(x)]优化本身是一个非常大的话题。如果有兴趣，可以通过《数值优化》和《运筹学》的书籍进行学习。</p>\n<p>模型与假设函数所有的模型都是错误的，但其中有些是有用的。</p>\n<p>–GeorgeEdwardPelhamBox模型是我们对要分析的数据的一种假设，它是为解决某个具体问题从数据中学习到的，因此它是机器学习最核心的概念。针对一个问题，通常有大量的模型可以选择。</p>\n<p>本文不会深入讨论这方面的内容，关于各种模型请参阅机器学习的相关书籍。本文仅以最简单的线性模型为基础来讨论梯度下降算法。</p>\n<p>这里我们先介绍一下在监督学习（supervisedlearning）中常见的三个符号：m，描述训练样本的数量x，描述输入变量或特征y，描述输出变量或者叫目标值请注意，一个样本可能有很多的特征，因此x和y通常是一个向量。</p>\n<p>不过在刚开始学习的时候，为了便于理解，你可以暂时理解为这就是一个具体的数值。训练集会包含很多的样本，我们用表示其中第i个样本。x是数据样本的特征，y是其目标值。</p>\n<p>例如，在预测房价的模型中，x是房子的各种信息，例如：面积，楼层，位置等等，y是房子的价格。在图像识别的任务中，x是图形的所有像素点数据，y是图像中包含的目标对象。</p>\n<p>我们是希望寻找一个函数，将x映射到y，这个函数要足够的好，以至于能够预测对应的y。由于历史原因，这个函数叫做假设函数（hypothesisfunction）。学习的过程如下图所示。</p>\n<p>即：首先根据已有的数据（称之为训练集）训练我们的算法模型，然后根据模型的假设函数来进行新数据的预测。线性模型（linearmodel）正如其名称那样：是希望通过一个直线的形式来描述模式。</p>\n<p>线性模型的假设函数如下所示：[h_{\\theta}(x)=\\theta_{0}+\\theta_{1}*x]这个公式对于大家来说应该都是非常简单的。如果把它绘制出来，其实就是一条直线。</p>\n<p>下图是一个具体的例子，即：的图形：在实际的机器学习工程中，你会拥有大量的数据。这些数据会来自于某个数据源。它们存储在csv文件中，或者以其他的形式打包。</p>\n<p>但是本文作为演示使用，我们通过一些简单的代码自动生成了需要的数据。为了便于计算，演示的数据量也很小。</p>\n<p>importnumpyasnpmax_x=10data_size=10theta_0=5theta_1=2defget_data:x=np.linspace(1,max_x,data_size)noise=np.random.normal(0,0.2,len(x))y=theta_0+theta_1*x+noisereturnx,y这段代码很简单，我们生成了x范围是[1,10]整数的10条数据。</p>\n<p>对应的y是以线性模型的形式计算得到，其函数是：。现实中的数据常常受到各种因素的干扰，所以对于y我们故意加上了一些高斯噪声。因此最终的y值为比原先会有轻微的偏离。</p>\n<p>最后我们的数据如下所示：x=[1,2,3,4,5,6,7,8,9,10]y=[6.66,9.11,11.08,12.67,15.12,16.76,18.75,21.35,22.77,24.56]我们可以把这10条数据绘制出来这样就有一个直观的了解了，如下图所示：虽然演示用的数据是我们通过公式计算得到的。</p>\n<p>但在实际的工程中，模型的参数是需要我们通过数据学习到的。所以下文我们假设我们不知道这里线性模式的两个参数是什么，而是通过算法的形式求得。最后再跟已知的参数进行对比以验证我们的算法是否正确。</p>\n<p>有了上面的数据，我们可以尝试画一条直线来描述我们的模型。例如，像下面这样画一条水平的直线：很显然，这条水平线离数据太远了，非常的不匹配。那我们可以再画一条斜线。</p>\n<p>我们初次画的斜线可能也不贴切，它可能像下面这样：最后我们通过不断尝试，找到了最终最合适的那条，如下所示：梯度下降算法的计算过程，就和这种本能式的试探是类似的，它就是不停的迭代，一步步的接近最终的结果。</p>\n<p>代价函数上面我们尝试了几次通过一条直线来拟合（fitting）已有的数据。二维平面上的一条直线可以通过两个参数唯一的确定，两个参数的确定也即模型的确定。那如何描述模型与数据的拟合程度呢？</p>\n<p>答案就是代价函数。代价函数（costfunction）描述了学习到的模型与实际结果的偏差程度。以上面的三幅图为例，最后一幅图中的红线相比第一条水平的绿线，其偏离程度（代价）应该是更小的。</p>\n<p>很显然，我们希望我们的假设函数与数据尽可能的贴近，也就是说：希望代价函数的结果尽可能的小。这就涉及到结果的优化，而梯度下降就是寻找最小值的方法之一。代价函数也叫损失函数。</p>\n<p>对于每一个样本，假设函数会依据计算出一个估算值，我们常常用来表示。即。</p>\n<p>很自然的，我们会想到，通过下面这个公式来描述我们的模型与实际值的偏差程度：[(h_\\theta(x^i)-y^i)^2=(\\widehat{y}^{i}-y^i)^2=(\\theta_{0}+\\theta_{1}*x^{i}-y^{i})^2]请注意，是实际数据的值，是我们的模型的估算值。</p>\n<p>前者对应了上图中的离散点的y坐标，后者对应了离散点在直线上投影点的y坐标。</p>\n<p>每一条数据都会存在一个偏差值，而代价函数就是对所有样本的偏差求平均值，其计算公式如下所示：[L(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^i)-y^i)^2=\\frac{1}{m}\\sum_{i=1}^{m}(\\theta_{0}+\\theta_{1}*x^{i}-y^{i})^2]当损失函数的结果越小，则意味着通过我们的假设函数估算出的结果与真实值越接近。</p>\n<p>这也就是为什么我们要最小化损失函数的原因。不同的模型可能会用不同的损失函数。例如，logistic回归的假设函数是这样的：。</p>\n<p>其代价函数是这样的：借助上面这个公式，我们可以写一个函数来实现代价函数：defcost_function(x,y,t0,t1):cost_sum=0foriinrange(len(x)):cost_item=np.power(t0+t1*x[i]-y[i],2)cost_sum+=cost_itemreturncost_sum/len(x)这个函数的代码应该不用多做解释，它就是根据上面的完成计算。</p>\n<p>我们可以尝试选取不同的和组合来计算代价函数的值，然后将结果绘制出来：importnumpyasnpimportmatplotlib.pyplotaspltfrommatplotlibimportcmfrommpl_toolkits.mplot3dimportAxes3Dtheta_0=5theta_1=2defdraw_cost(x,y):fig=plt.figure(figsize=(10,8))ax=(projection='3d')scatter_count=100radius=1t0_range=np.linspace(theta_0-radius,theta_0+radius,scatter_count)t1_range=np.linspace(theta_1-radius,theta_1+radius,scatter_count)cost=np.zeros((len(t0_range),len(t1_range)))forainrange(len(t0_range)):forbinrange(len(t1_range)):cost[a][b]=cost_function(x,y,t0_range[a],t1_range[b])t0,t1=np.meshgrid(t0_range,t1_range)ax.set_xlabel('theta_0')ax.set_ylabel('theta_1')ax.plot_surface(t0,t1,cost,)在这段代码中，我们对和各自指定了一个范围进行100次的采样，然后以不同的组合对来计算代价函数的值。</p>\n<p>如果我们将所有点的代价函数值绘制出来，其结果如下图所示：从这个图形中我们可以看出，当越接近[5,2]时其结果（偏差）越小。相反，离得越远，结果越大。</p>\n<p>直观解释从上面这幅图中我们可以看出，代价函数在不同的位置结果大小不同。从三维的角度来看，这就和地面的高低起伏一样。最高的地方就好像是山顶。</p>\n<p>而我们的目标就是：从任意一点作为起点，能够快速寻找到一条路径并以此到达图形最低点（代价值最小）的位置。而梯度下降的算法过程就和我们从山顶想要快速下山的做法是一样的。</p>\n<p>在生活中，我们很自然会想到沿着最陡峭的路往下行是下山速度最快的。如下面这幅图所示：针对这幅图，细心的读者可能很快就会有很多的疑问，例如：对于一个函数，怎么确定下行的方向？每一步该往前走多远？</p>\n<p>有没有可能停留在半山腰的平台上？这些问题也就是本文接下来要讨论的内容。算法描述梯度下降算法最开始的一点就是需要确定下降的方向，即：梯度。我们常常用来表示梯度。</p>\n<p>对于一个二维空间的曲线来说，梯度就是其切线的方向。如下图所示：而对于更高维空间的函数来说，梯度由所有变量的偏导数决定。</p>\n<p>其表达式如下所示：[\\nablaf({\\theta})=(\\frac{\\partialf({\\theta})}{\\partial\\theta_1},\\frac{\\partialf({\\theta})}{\\partial\\theta_2},...,\\frac{\\partialf({\\theta})}{\\partial\\theta_n})]在机器学习中，我们主要是用梯度下降算法来最小化代价函数，记做：[\\theta^*=argminL(\\theta)]其中，L是代价函数，是参数。</p>\n<p>梯度下降算法的主体逻辑很简单，就是沿着梯度的方向一直下降，直到参数收敛为止。</p>\n<p>记做：[\\theta^{k+1}_i=\\theta^{k}_i-\\lambda\\nablaf(\\theta^{k})]这里的下标i表示第i个参数。</p>\n<p>上标k指的是第k步的计算结果，而非k次方。在能够理解的基础上，下文的公式中将省略上标k。这里有几点需要说明：收敛是指函数的变化率很小。具体选择多少合适需要根据具体的项目来确定。</p>\n<p>在演示项目中我们可以选择0.01或者0.001这样的值。不同的值将影响算法的迭代次数，因为在梯度下降的最后，我们会越来越接近平坦的地方，这个时候函数的变化率也越来越小。</p>\n<p>如果选择一个很小的值，将可能导致算法迭代次数暴增。公式中的称作步长，也称作学习率（learningrate）。它决定了每一步往前走多远，关于这个值我们会在下文中详细讲解。</p>\n<p>你可以暂时人为它是一个类似0.01或0.001的固定值。在具体的项目，我们不会让算法无休止的运行下去，所以通常会设置一个迭代次数的最大上限。</p>\n<p>线性回归的梯度下降有了上面的知识，我们可以回到线性模型代价函数的梯度下降算法实现了。</p>\n<p>首先，根据代价函数我们可以得到梯度向量如下：[\\nablaf({\\theta})=(\\frac{\\partialL(\\theta)}{\\partial\\theta_{0}},\\frac{\\partialL(\\theta)}{\\partial\\theta_{1}})=(\\frac{2}{m}\\sum_{i=1}^{m}(\\theta_{0}+\\theta_{1}*x^{i}-y^{i}),\\frac{2}{m}\\sum_{i=1}^{m}(\\theta_{0}+\\theta_{1}*x^{i}-y^{i})x^{i})]接着，将每个偏导数带入迭代的公式中，得到：[\\theta_{0}:=\\theta_{0}-\\lambda\\frac{\\partialL(\\theta_{0})}{\\partial\\theta_{0}}=\\theta_{0}-\\frac{2\\lambda}{m}\\sum_{i=1}^{m}(\\theta_{0}+\\theta_{1}*x^{i}-y^{i})\\\\theta_{1}:=\\theta_{1}-\\lambda\\frac{\\partialL(\\theta_{1})}{\\partial\\theta_{1}}=\\theta_{1}-\\frac{2\\lambda}{m}\\sum_{i=1}^{m}(\\theta_{0}+\\theta_{1}*x^{i}-y^{i})x^{i}]由此就可以通过代码实现我们的梯度下降算法了，算法逻辑并不复杂：learning_rate=0.01defgradient_descent(x,y):t0=10t1=10delta=0.001fortimesinrange(1000):sum1=0sum2=0foriinrange(len(x)):sum1+=(t0+t1*x[i]-y[i])sum2+=(t0+t1*x[i]-y[i])*x[i]t0_=t0-2*learning_rate*sum1/len(x)t1_=t1-2*learning_rate*sum2/len(x)print('Times:{},gradient:[{},{}]'.format(times,t0_,t1_))if(abs(t0-t0_)</p>\n<p>这时的结果[5.193562479839619,1.9533620008416623]，这已经比较接近目标值[5,2]了。</p>\n<p>如果需要更高的精度，可以将delta的值调的更小，当然，此时会需要更多的迭代次数。高维扩展虽然我们举的例子是二维的，但是对于更高维的情况也是类似的。</p>\n<p>同样是根据迭代的公式进行运算即可：[\\theta_{i}=\\theta_{i}-\\lambda\\frac{\\partialL(\\theta)}{\\partial\\theta_i}=\\theta_{i}-\\frac{2\\lambda}{m}\\sum_{i=1}^{m}(h_\\theta(x^{k})-y^k)x_i^k]这里的下标i表示第i个参数，上标k表示第k个数据。</p>\n<p>梯度下降家族BGD在上面的内容中我们看到，算法的每一次迭代都需要把所有样本进行遍历处理。这种做法称为之BatchGradientDescent，简称BGD。</p>\n<p>作为演示示例只有10条数据，这是没有问题的。但在实际的项目中，数据集的数量可能是几百万几千万条，这时候每一步迭代的计算量就会非常的大了。于是就有了下面两个变种。</p>\n<p>SGDStochasticGradientDescent，简称SGD，这种算法是每次从样本集中仅仅选择一个样本来进行计算。很显然，这样做算法在每一步的计算量一下就少了很多。</p>\n<p>其算法公式如下：[\\theta_{i}=\\theta_{i}-\\lambda\\frac{\\partialL(\\theta)}{\\partial\\theta_i}=\\theta_{i}-\\lambda(h_\\theta(x^k)-y^k)x_i^k]当然，减少算法计算量也是有代价的，那就是：算法结果会强依赖于随机取到的数据情况，这可能会导致算法的最终结果不太令人满意。</p>\n<p>MBGD以上两种做法其实是两个极端，一个是每次用到了所有数据，另一个是每次只用一个数据。我们自然就会想到两者取其中的方法：每次选择一小部分数据进行迭代。</p>\n<p>这样既避免了数据集过大导致每次迭代计算量过大的问题，也避免了单个数据对算法的影响。这种算法称之为Mini-batchGradientDescent，简称MBGD。</p>\n<p>其算法公式如下：[\\theta_{i}=\\theta_{i}-\\lambda\\frac{\\partialL(\\theta)}{\\partial\\theta_i}=\\theta_{i}-\\frac{2\\lambda}{m}\\sum_{i=a}^{a+b}(h_\\theta(x^k)-y^k)x_i^k]当然，我们可以认为SGD是Mini-batch为1的特例。</p>\n<p>针对上面提到的算法变种，该如何选择呢？下面是AndrewNg给出的建议：如果样本数量较小（例如小于等于2000），选择BGD即可。</p>\n<p>如果样本数量很大，选择来进行MBGD，例如：64，128，256，512。</p>\n<p>下表是OptimizationforDeepLearning中对三种算法的对比方法准确性更新速度内存占用在线学习BGD好慢高否SGD好（withannealing）快低是MBGD好中等中等是算法优化式7是算法的基本形式，在这个基础上有很多人进行了更多的研究。</p>\n<p>接下来我们介绍几种梯度下降算法的优化方法。MomentumMomentum是动量的意思。这个算法的思想就是借助了动力学的模型：每次算法的迭代会使用到上一次的速度作为依据。</p>\n<p>算法的公式如下：[v^t=\\gammav^{t-1}+\\lambda\\nablaf(\\theta)\\\\theta=\\theta-v_t]对比式7可以看出，这个算法的主要区别就是引入了，并且，每个时刻的受前一个时刻的影响。</p>\n<p>从形式上看，动量算法引入了变量v充当速度角色——它代表参数在参数空间移动的方向和速率。速度被设为负梯度的指数衰减平均。名称动量来自物理类比，根据牛顿运动定律，负梯度是移动参数空间中粒子的力。</p>\n<p>动量在物理学上定义为质量乘以速度。在动量学习算法中，我们假设是单位质量，因此速度向量v也可以看作是粒子的动量。对于可以取值0，而是一个常量，设为0.9是一个比较好的选择。</p>\n<p>下图是momentum算法的效果对比：对原来的算法稍加修改就可以增加动量效果：defgradient_descent_with_momentum(x,y):t0=10t1=10delta=0.001v0=0v1=0gamma=0.9fortimesinrange(1000):sum1=0sum2=0foriinrange(len(x)):sum1+=(t0+t1*x[i]-y[i])sum2+=(t0+t1*x[i]-y[i])*x[i]v0=gamma*v0+2*learning_rate*sum1/len(x)v1=gamma*v1+2*learning_rate*sum2/len(x)t0_=t0-v0t1_=t1-v1print('Times:{},gradient:[{},{}]'.format(times,t0_,t1_))if(abs(t0-t0_)</p>\n<p>速度比原来660次快了很多。</p>\n<p>同样的，我们可以把算法计算的过程做成动态图：对比原始的算法过程可以看出，改进算法最大的区别是：在寻找目标值时会在最终结果上下跳动，但是越往后跳动的幅度越小，这也就是动量所产生的效果。</p>\n<p>LearningRate优化至此，你可能还是好奇该如何设定学习率的值。事实上，这个值的选取需要一定的经验或者反复尝试才能确定。</p>\n<p>《深度学习》一书中是这样描述的：“与其说是科学，这更像是一门艺术，我们应该谨慎地参考关于这个问题的大部分指导。”。关键在于，这个值的选取不能过大也不能过小。</p>\n<p>如果这个值过小，会导致每一次迭代的步长很小，其结果就是算法需要迭代非常多的次数。那么，如果这个值过大会怎么样呢？其结果就是：算法可能在结果的周围来回震荡，却落不到目标的点上。</p>\n<p>下面这幅图描述了这个现象：事实上，学习率的取值未必一定要是一个常数，关于这个值的设定有很多的研究。下面是比较常见的一些改进算法。</p>\n<p>AdaGradAdaGrad是AdaptiveGradient的简写，该算法会为每个参数设定不同的学习率。它使用历史梯度的平方和作为基础来进行计算。</p>\n<p>其算法公式如下：[\\theta_i=\\theta_i-\\frac{\\lambda}{\\sqrt{G_t+\\epsilon}}\\nablaf(\\theta)]对比式7，这里的改动就在于分号下面的根号。</p>\n<p>根号中有两个符号，第二个符号比较好理解，它就是为了避免除0而人为引入的一个很小的常数，例如可以设为：0.001。</p>\n<p>第一个符号的表达式展开如下：[G_t=\\sum_{i=1}^{t}\\nablaf(\\theta){i}\\nablaf(\\theta){i}^{T}]这个值其实是历史中每次梯度的平方的累加和。</p>\n<p>AdaGrad算法能够在训练中自动的对learningrate进行调整，对于出现频率较低参数采用较大的学习率；相反，对于出现频率较高的参数采用较小的学习率。因此，Adagrad非常适合处理稀疏数据。</p>\n<p>但该算法的缺点是它可能导致学习率非常小以至于算法收敛非常的慢。关于这个算法的直观解释可以看李宏毅教授的视频课程：MLLecture3-1:GradientDescent。</p>\n<p>RMSPropRMS是RootMeanSquare的简写。RMSProp是AI教父GeoffHinton提出的一种自适应学习率方法。</p>\n<p>AdaGrad会累加之前所有的梯度平方，而RMSProp仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题。</p>\n<p>该算法的公式如下：[E[\\nablaf(\\theta_{i})^2]^{t}=\\gammaE[\\nablaf(\\theta_{i})^2]^{t-1}+(1-\\gamma)(\\nablaf(\\theta_{i})^{t})^{2}\\\\theta_i=\\theta_i-\\frac{\\lambda}{\\sqrt{E[g^2]^{t+1}+\\epsilon}}\\nablaf(\\theta_{i})]类似的，是为了避免除0而引入。</p>\n<p>是衰退参数，通常设为0.9。这里的是t时刻梯度平方的平均值。AdamAdam是AdaptiveMomentEstimation的简写。</p>\n<p>它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。</p>\n<p>该算法公式如下：[m^{t}=\\beta_{1}m^{t-1}+(1-\\beta_{1})\\nablaf(\\theta)\\v^{t}=\\beta_{2}v^{t-1}+(1-\\beta_{2})\\nablaf(\\theta)^2\\\\widehat{m}^{t}=\\frac{m^{t}}{1-\\beta^{t}_1}\\\\widehat{v}^{t}=\\frac{v^{t}}{1-\\beta^{t}_2}\\\\theta=\\theta-\\frac{\\lambda}{\\sqrt{\\widehat{v}^{t}}+\\epsilon}\\widehat{m}^{t}]，分别是对梯度的一阶矩估计和二阶矩估计。</p>\n<p>，是对，的校正，这样可以近似为对期望的无偏估计。Adam算法的提出者建议默认值为0.9，默认值为0.999，默认值为。在实际应用中，Adam较为常用，它可以比较快地得到一个预估结果。</p>\n<p>优化小结这里我们列举了几种优化算法。它们很难说哪种最好，不同的算法适合于不同的场景。在实际的工程中，可能需要逐个尝试一下才能确定选择哪一个，这个过程也是目前现阶段AI项目要经历的工序之一。</p>\n<p>实际上，该方面的研究远不止于此，如果有兴趣，可以继续阅读《SebastianRuder:Anoverviewofgradientdescentoptimizationalgorithms》这篇论文或者OptimizationforDeepLearning这个Slides进行更多的研究。</p>\n<p>由于篇幅所限，这里不再继续展开了。算法限制梯度下降算法存在一定的限制。首先，它要求函数必须是可微分的，对于不可微的函数，无法使用这种方法。</p>\n<p>除此之外，在某些情况下，使用梯度下降算法在接近极值点的时候可能收敛速度很慢，或者产生Z字形的震荡。这一点需要通过调整学习率来回避。另外，梯度下降还会遇到下面两类问题。</p>\n<p>局部最小值局部最小值（LocalMinima）指的是，我们找到的最小值仅仅是一个区域内的最小值，而并非全局的。由于算法的起点是随意取的，以下面这个图形为例，我们很容易落到局部最小值的点里面。</p>\n<p>这就是好像你从上顶往下走，你第一次走到的平台未必是山脚，它有可能只是半山腰的一个平台的而已。算法的起点决定了算法收敛的速度以及是否会落到局部最小值上。</p>\n<p>坏消息是，目前似乎没有特别好的方法来确定选取那个点作为起点是比较好的，这就有一点看运气的成分了。多次尝试不同的随机点或许是一个比较好的方法，这也就是为什么做算法的优化这项工作是特别消耗时间的了。</p>\n<p>但好消息是：对于凸函数或者凹函数来说，不存在局部极值的问题。其局部极值一定是全局极值。最近的一些研究表明，某些局部极值并没有想象中的那么糟糕，它们已经非常的接近全局极值所带来的结果了。</p>\n<p>鞍点除了LocalMinima，在梯度下降的过程中，还有可能遇到另外一种情况，即：鞍点（SaddlePoint）。</p>\n<p>鞍点指的是我们找到点某个点确实是梯度为0，但它却不是函数的极值，它的周围既有比它小的值，也有比它大的值。这就好像马鞍一样。如下图所示：多类随机函数表现出以下性质：在低维空间中，局部极值很普遍。</p>\n<p>但在高维空间中，局部极值比较少见，而鞍点则很常见。不过对于鞍点，可以通过数学方法Hessian矩阵来确定。关于这点，这里就不再展开了，有兴趣的读者可以以这里提供的几个链接继续探索。</p>\n<p>参考资料与推荐读物Wikipeida:GradientdescentSebastianRuder:Anoverviewofgradientdescentoptimizationalgorithms吴恩达：机器学习吴恩达：深度学习PeterFlach：机器学习李宏毅-MLLecture3-1:GradientDescentPDF:李宏毅-GradientDescentIntrotooptimizationindeeplearning:GradientDescentIntrotooptimizationindeeplearning:Momentum,RMSPropandAdamStochasticGradientDescent–Mini-batchandmore刘建平Pinard-梯度下降（GradientDescent）小结多元函数的偏导数、方向导数、梯度以及微分之间的关系思考[MachineLearning]梯度下降法的三种形式BGD、SGD以及MBGD作者：阿Paul。</p>\n<h2>梯度下降算法是指什么 神经网络</h2>\n<p> </p>\n</div>\n</div>", "first_tag": "Python", "cpp": 0, "csharp": 0, "python": 1, "javascript": 0, "java": 0, "sql": 0, "php": 0, "time": "2022-09-07 13:31:17", "summary": "梯度下降法是什么？梯度下降法英语：是一个一阶最优化算法，通常也称为最陡下降法。要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度或者是近似梯度的反方向的规定步长距离点进行迭代搜索。如"}