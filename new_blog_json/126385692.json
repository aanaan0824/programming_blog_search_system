{"blogid": "126385692", "writerAge": "码龄2年", "writerBlogNum": "61", "writerCollect": "224", "writerComment": "66", "writerFan": "97", "writerGrade": "3级", "writerIntegral": "758", "writerName": "takedachia", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_126385692.jpg", "writerRankTotal": "24113", "writerRankWeekly": "15277", "writerThumb": "96", "writerVisitNum": "60085", "blog_read_count": "787", "blog_time": "已于 2022-08-23 11:54:41 修改", "blog_title": "【论文精读】Fast R-CNN", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-atom-one-dark\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<p>《Fast R-CNN》是同一个作者基于自己之前的R-CNN工作的改进。<br/> Fast R-CNN也是基于深度卷积神经网络用于计算机视觉任务（主要用于目标检测）的算法。<br/> 他在R-CNN的基础上进行了大幅创新，比如将目标的分类和定位的步骤进行了统一，实现了端到端的训练、预测流程。<br/> Fast R-CNN在使用VGG-16的主干网络时，训练速度比R-CNN快9倍，测试速度快了213倍，并提升了检测精度。<br/> 训练速度比SPPnet快了3倍，测试速度快了10倍，并提升了精度。</p>\n<p>本文是对论文的精读总结，我会把每一个点都研究讲透，不仅让大家了解Fast R-CNN，还会总结自己在读论文各个部分时自己的心得总结。<br/> Fast R-CNN的前作R-CNN论文精读可参考我的<a href=\"https://blog.csdn.net/takedachia/article/details/126052406\">这篇文章</a>。</p>\n<p></p>\n<div class=\"toc\">\n<h3>文章目录</h3>\n<ul><li><ul><li><a href=\"#_12\">背景</a></li><li><ul><li><a href=\"#RCNN_16\">R-CNN的缺点</a></li><li><a href=\"#SPPnet_31\">SPPnet解决的问题和缺点</a></li><li><a href=\"#Fast_RCNN_49\">Fast R-CNN的优点</a></li></ul>\n</li><li><a href=\"#Fast_RCNN_58\">Fast R-CNN模型结构</a></li><li><ul><li><a href=\"#RoI_pooling_layer_85\">RoI pooling layer</a></li></ul>\n</li><li><a href=\"#_117\">模型的训练细节</a></li><li><ul><li><a href=\"#CNN_120\">构建模型（基于预训练CNN网络进行修改）</a></li><li><a href=\"#SPPnet_129\">为什么SPPnet训练时难以更新卷积网络的权重？（关于训练时的采样方法）</a></li><li><a href=\"#_156\">多任务的损失函数</a></li><li><a href=\"#_RoI_pooling__211\">关于 RoI pooling 层的梯度反传</a></li><li><a href=\"#_251\">关于尺度不变性</a></li></ul>\n</li><li><a href=\"#_261\">模型的测试细节</a></li><li><ul><li><a href=\"#_SVD__266\">利用 截断性的SVD 加速推理</a></li></ul>\n</li><li><a href=\"#1_282\">实验结果1</a></li><li><ul><li><a href=\"#VOC_294\">在VOC数据集上的结果</a></li><li><a href=\"#_299\">训练和测试速度的提升</a></li><li><a href=\"#finefune__307\">fine-fune 卷积层的权重提升检测精度</a></li></ul>\n</li><li><a href=\"#2Fast_RCNN_318\">实验结果2——验证Fast R-CNN设计的有效性</a></li><li><ul><li><a href=\"#_322\">多任务训练真的有用吗？</a></li><li><a href=\"#_333\">关于尺度不变性</a></li><li><a href=\"#_343\">是否需要更多的训练数据</a></li><li><a href=\"#SVM_vs_softmax_351\">SVM分类器 vs softmax</a></li><li><a href=\"#_362\">候选框越多越好吗？</a></li></ul>\n</li></ul>\n</li></ul>\n</div>\n<p></p>\n<h2><a id=\"_12\"></a>背景</h2>\n<p>为什么提出了Fast R-CNN？<br/> 作者指出之前的目标检测算法 R-CNN 和 SPPnet 都存在各种缺点。</p>\n<h3><a id=\"RCNN_16\"></a>R-CNN的缺点</h3>\n<p>R-CNN的缺点：</p>\n<ol><li>训练是是多阶段的。首先需要训练一个CNN网络用于图像特征的提取。<br/> 接着需要训练一个线性SVM分类器，对提取出来的特征进行图片分类。<br/> 最后还需要训练一个回归模型，用于调整目标检测候选框的定位。</li><li>训练非常贵。因为是多阶段的目标检测，先需要保存每一张图片上的每一个候选框抽取的特征，然后再进行训练。<br/> 因此，需要将这些特征存到磁盘上。<br/> 不仅提取特征耗时，并且储存特征还耗费空间。</li><li>测试时，目标检测速度很慢。一个使用VGG-16主干模型的R-CNN，预测一张图片需要耗费47秒的时间（在GPU）上。</li></ol>\n<p>作者指出R-CNN慢的根源在于每一个候选框都使用同个CNN网络前向计算，它们之间存在大量重复运算，没有共享计算资源。<br/> 为了解决这个问题，后面有大佬（何凯明等）提出了SPPnet。</p>\n<h3><a id=\"SPPnet_31\"></a>SPPnet解决的问题和缺点</h3>\n<p>SPPnet概要：<br/> SPPnet将整张图片经过一次CNN网络前向计算，在最后进入全连接层之前使用了一个<strong>SPP层</strong>（空间金字塔池化层），而非普通的最大池化层。如下图。<br/> <img alt=\"SPPnet\" src=\"..\\..\\static\\image\\efaf895b6f9844c89166ed83494bb82b.png\"/></p>\n<p>SPP层中，会将卷积层输出的<strong>convolutional feature map</strong>（图中的feature maps of conv5，它可以任意尺寸）进行一种固定输出的池化，如上图。不管如输入的image的尺寸如何变化，在最后SPP层都会固定输出 1个4×4的特征图、1个2×2的特征图、1个1×1的特征图（具体使用到了动态池化核和步长的算法），类似金字塔堆叠的感觉，最后将这些固定的特征图展品成21×256固定长度的向量传入全连接层。<br/> 这样做的好处：<br/> ①这里先解决了输入图片固定的问题，让图片无需固定尺寸输入CNN。<br/> ②其次每一张图片对应的2000个候选框都可以在最后SPP层前的<strong>convolutional feature map</strong>中找到对应位置的小区域，将图片中候选框的区域映射到convolutional feature map的对应区域。用抠出来的这块小特征图（下图中的window）进行SPP池化，输出固定特征。（如下图）<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\e56f4f416afa41568f7522c358448afb.png\"/></p>\n<p>这样我们仅需一次前向CNN网络的传播计算，就可以将所有候选框的特征提取出来（不用跑2000次了）。<br/> 这就解决了上面R-CNN中共享计算的问题，这会加快检测的速度。<br/> 但是SPPnet算法将空间金字塔池化层最后提取到的向量依然使用一个线性SVM分类器和一个回归器进行分类和定位，和R-CNN一样依然是多阶段的。这样依然不能解决R-CNN中多阶段训练慢、耗费资源多的问题（前面的第1、2点）。<br/> 并且同时，作者提出了在SPPnet中，使用fine tuning对CNN网络微调训练时，SPP层前的网络参数难以被更新，这会影响目标检测的精度（这个会在后面详述）。</p>\n<h3><a id=\"Fast_RCNN_49\"></a>Fast R-CNN的优点</h3>\n<p>作者提出的Fast R-CNN解决了以上R-CNN和SPPnet的缺点，同时提升了训练和测试速度，并提升精度。<br/> Fast R-CNN的特点：</p>\n<ul><li>比R-CNN和SPPnet更高的检测精度。</li><li>使用了一个多任务的损失函数，是一个单阶段的训练过程</li><li>训练时可更新所有网络层的参数</li><li>不再需要将提取的候选框特征缓存到磁盘上</li></ul>\n<h2><a id=\"Fast_RCNN_58\"></a>Fast R-CNN模型结构</h2>\n<p>我们来看Fast R-CNN的模型结构（如下图）：<br/> 网络的输入为 一张图像+一组目标候选框（也是通过selective search生成）信息。</p>\n<p>①图像经过CNN网络后，在最后一个卷积层输出一组<strong>Conv feature map</strong>。<br/> 目标候选框信息，则通过RoI projection，即目标位置的投影，投影定位到Conv feature map对应位置，抠出这块小feature map。</p>\n<p>②我们将抠出的这块小feature map传入 <strong>RoI pooling layer</strong>。<br/> 经过这个RoI pooling layer，我们可提取出一个<strong>固定</strong>长度的特征向量。</p>\n<p>③将这个特征向量同时传入两个模块。<br/> 传入全连接层+softmax分类，用于预测<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        K\n       \n       \n        +\n       \n       \n        1\n       \n      \n      \n       K+1\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.7667em; vertical-align: -0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.0715em;\">K</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span>个类别的概率。<br/> 传入 bounding-box 回归器，用于预测每一个类别的定位。</p>\n<p><img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\1e80c196784145b7a9140c140ad1dcc6.png\"/></p>\n<hr/>\n<p>可以看到相比R-CNN（下图）而言，该模型有三个不同的地方：</p>\n<ul><li>传入的不再是目标候选框，而是一整张图片+候选框的定位信息，输出整张图片的feature map。再利用候选框的定位信息进行位置投影，找到候选框在feature map上的对应位置，抠出来传入下一层。这样解决了CNN网络重复计算的问题，不用再对一张图片2000个候选框进行2000次前向计算了，只需1次计算就可以。</li><li>模型没有对输入的图片进行大小限制，因此输出的 (conv) feature map有大有小，抠出来的小feature map也有大有小。论文使用了 <strong>RoI pooling layer</strong>，将不同尺寸的小feature map统一池化下采样成固定大小的特征向量（取代了普通CNN网络的最后一个Max Pooling层）。（RoI pooling layer的原理下面细讲）</li><li>对特征向量进行分类预测时，不再使用线性SVM分类器，而是直接使用softmax输出各个类别的概率。这样就不用再特地对每个类别训练线性SVM分类器了。这个设计也是和单阶段训练的设计相统一，后面会详细讲到。</li></ul>\n<p><img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\c46428623b944feca424f221ed08e726.png\"/></p>\n<p>(上图摘自<a href=\"https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e\">towardsdatascience</a>)</p>\n<h3><a id=\"RoI_pooling_layer_85\"></a>RoI pooling layer</h3>\n<p>RoI pooling层是一类特殊的自适应Max Pooling层，是一种根据当前feature map尺寸动态指定<strong>池化核</strong>和<strong>步长</strong>的Max Pooling，它其实是沿袭了SPPnet中SPP层（空间金字塔池化层）的设计。</p>\n<p>我举个两个例子详细地说明RoI pooling layer做了一件什么事：</p>\n<p><strong>①</strong>假设我们在一个conv feature map中，根据映射关系取到其中一个C×13×13的小feature map进入RoI pooling层，我们想指定输出C×4×4大小的特征向量。<br/> 对于我们的Max Pooling：<br/> <strong>池化核的size</strong>是feature map的size除以输出的size后向上取整，即size=ceil(13/4)=4<br/> <strong>步长stride</strong>则是feature map的size除以输出的size后向下取整，即size=floor(13/4)=3<br/> 这样得到的池化结果正好是一个4×4的输出。<br/> 详见下图。最后我们将输出展平，得到C×16的一个特征向量。</p>\n<p><img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\dfc381d5b39c4208b1253889f6633f19.png\"/></p>\n<p><strong>②</strong>本例是一个feature map非正方形的例子（图摘自<a href=\"https://benrishi-ai.com/fast-rcnn02/\">benrishi.ai</a>）<br/> 假设我们对图中这个6×4的feature map进行RoI pooling，设定输出2×2大小的特征图。<br/> 那么Max pooling的<strong>池化核</strong>和<strong>步长</strong>在水平和垂直方向应当各有自己的size和stride。<br/> 文中作者并没有详细说明非正方形的情况下如何进行pooling，根据我对作者论文RoI pooling这段描述的理解，矩形feature map的池化参数应为：</p>\n<p>kernel_size=( ceil(6/2), ceil(4/2) )=(3,2)<br/> stride=( floor(6/2), floor(4/2) )=(3,2)</p>\n<p>即池化核大小为(3,2)，步长在垂直方向为3，水平方向为2。<br/> （PS：也符合Pytorch中nn.MaxPool2d层中的参数规范）<br/> 这样池化得到的特征图也是2×2。我们还可以取下图中5×8这张小feature map自行验证一下。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\09891846be974d4daa36a7ac10d394b0.png\"/></p>\n<p>以上就是RoI pooling layer的详解，通过一种的自适应Max Pooling层，固定输出的size。作者自己也说了RoI pooling是借鉴自SPPnet的SPP层，是SPP层的一种特殊情况。</p>\n<h2><a id=\"_117\"></a>模型的训练细节</h2>\n<p>总体上模型使用Fine-tuning的方法，对预训练好的图像分类模型修改后进行微调训练。<br/> 同时，微调训练是单阶段的流程，即用一个损失函数同时优化softmax和bbox回归器，训练一步到位。（不像R-CNN和SPPnet需要训练3个模块）</p>\n<h3><a id=\"CNN_120\"></a>构建模型（基于预训练CNN网络进行修改）</h3>\n<p>作者使用了在ImgeNet图像分类数据集上预训练好的CNN模型（如AlexNet、VGG-16等），对其进行如下修改。</p>\n<ul><li>将网络最后一个Max Pooling改成RoI Pooling层。</li><li>将网络最后的全连接+softmax层改成了两个并行的层，即前面所述的：<br/> ①全连接层+softmax，用于预测<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n     \n      \n       \n        \n         K\n        \n        \n         +\n        \n        \n         1\n        \n       \n       \n        K+1\n       \n      \n     </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.7667em; vertical-align: -0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.0715em;\">K</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span>个类别的概率。<br/> ② bounding-box 回归器，用于预测每一个类别的定位。</li><li>将网络的输入改成 图片+图片中目标候选框定位信息。</li></ul>\n<h3><a id=\"SPPnet_129\"></a>为什么SPPnet训练时难以更新卷积网络的权重？（关于训练时的采样方法）</h3>\n<p>作者在文中提到了Fast R-CNN可通过反向传播训练整个网络的权重参数。<br/> 论文这里对上面提出的SPPne使用fine tuning对CNN网络微调训练时，网络的参数难以被更新进行了详细解释。<br/> <em>作者在文中使用了\"SPPnet is unable to update weights\"的说法，我觉得更合适的说法是难以更新。</em></p>\n<p>我搜了全网都找不到一个比较详细的解答，所以我自己画了一张图详细讲述一下是难以更新参数是怎么回事。<br/> 首先要明确，这个问题和Fast R-CNN的采样方法息息相关，这个问题直接指导了Fast R-CNN在训练时对一个mini-batch的采样方法。<br/> 我画了一张图，对比了Fast-R-CNN和SPPnet的训练差异：<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\91655eed20ee43c4aeeaf602a263de98.png\"/></p>\n<p>上半部分是SPPnet/R-CNN的训练流程。假设我们的batch_size设为4，根据平均采样的原则，一般是4张图片中各自的候选框进入前向传播，由于是4张不同的图片，所以我们需要经过<strong>4次正向传播</strong>。然后算出Loss，再进行反向传播。<br/> 下半部分是Fast R-CNN的训练流程。我们依然设batch_size为4，但我们使用了分层抽样的原则。因为每个图片都有2000个候选框，所以我们先抽取若干张图片，再在每张图片中抽取多个候选框。比如我们抽取2张图片（N=2），每张图片抽取2个候选框。这样的话对于Fast R-CNN每个图片只需要经过一次正向传播，对于候选框按位置进行映射抠图即可，在这个例子中我们只需要<strong>2次正向传播</strong>就能行。</p>\n<p>如果把batch_size放大到128，我们假设每个mini-batch只抽取2张图片，每张图片各自抽64个候选框，那么我们只需要2次正向传播就能扫完一个批次。<br/> 但是SPPnet/R-CNN则要128次正向传播，这样对算力的要求急剧增加。所以对于fine tuning来说，这是不效率的。</p>\n<p>这就是为什么SPPnet训练时难以更新卷积网络的权重的原因，并不是无法跟新，而是非常低效。</p>\n<hr/>\n<p>所以，文中作者采用的mini-batch<strong>采样策略</strong>为：<br/> 先随机采样2张图片，每张图片采样64个候选框，batch_size共计128。<br/> 这128个样本中，25%为正样本，其余为负样本（这个和R-CNN一样）。<br/> 其中，IoU大于0.5为正样本，IoU在0.1到0.5之间的为负样本。<br/> 此外，对IoU小于0.1的样本采用难例挖掘的策略（hard example mining）来训练。我的理解是，因为IoU小于0.1的样本数量非常多，所以值得挖掘，训练时对目标框IoU&lt;0.1的样本进行预测，如预测有误就加入hard example错题集进入下一轮的训练。<br/> 此外，作者还使用了0.5概率的水平翻转作为数据增强的手段。</p>\n<h3><a id=\"_156\"></a>多任务的损失函数</h3>\n<p>Fast R-CNN网络最后有两个输出，分别是softmax分类和bbox回归器，所以这是一个多目标优化问题。<br/> 对于这个两个目标的优化问题，作者的思路是将两个设计好的损失函数进行加和，把两个目标的优化问题简化成单目标优化问题。</p>\n<p>Fast R-CNN中，每个样本（候选框）都会输出：<br/> 一个离散概率分布 <span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        p\n       \n       \n        =\n       \n       \n        \n         (\n        \n        \n         \n          p\n         \n         \n          0\n         \n        \n        \n         ,\n        \n        \n         …\n        \n        \n         ,\n        \n        \n         \n          p\n         \n         \n          K\n         \n        \n        \n         )\n        \n       \n      \n      \n       p=\\left(p_{0}, \\ldots, p_{K}\\right)\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.625em; vertical-align: -0.1944em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top: 0em;\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">0</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3283em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0715em;\">K</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top: 0em;\">)</span></span></span></span></span></span>，包含了k+1个类别的预测概率<br/> 一组回归系数 <span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         t\n        \n        \n         k\n        \n       \n       \n        =\n       \n       \n        \n         (\n        \n        \n         \n          t\n         \n         \n          x\n         \n         \n          k\n         \n        \n        \n         ,\n        \n        \n         \n          t\n         \n         \n          y\n         \n         \n          k\n         \n        \n        \n         ,\n        \n        \n         \n          t\n         \n         \n          w\n         \n         \n          k\n         \n        \n        \n         ,\n        \n        \n         \n          t\n         \n         \n          h\n         \n         \n          k\n         \n        \n        \n         )\n        \n       \n      \n      \n       t^{k}=\\left(t_{\\mathrm{x}}^{k}, t_{\\mathrm{y}}^{k}, t_{\\mathrm{w}}^{k}, t_{\\mathrm{h}}^{k}\\right)\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.8491em;\"><span class=\"\" style=\"top: -3.063em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0315em;\">k</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 1.2331em; vertical-align: -0.3831em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top: 0em;\"><span class=\"delimsizing size1\">(</span></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.8491em;\"><span class=\"\" style=\"top: -2.453em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">x</span></span></span></span><span class=\"\" style=\"top: -3.063em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0315em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.247em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.8491em;\"><span class=\"\" style=\"top: -2.453em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\" style=\"margin-right: 0.0139em;\">y</span></span></span></span><span class=\"\" style=\"top: -3.063em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0315em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3831em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.8491em;\"><span class=\"\" style=\"top: -2.453em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\" style=\"margin-right: 0.0139em;\">w</span></span></span></span><span class=\"\" style=\"top: -3.063em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0315em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.247em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.8491em;\"><span class=\"\" style=\"top: -2.4169em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">h</span></span></span></span><span class=\"\" style=\"top: -3.063em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0315em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2831em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top: 0em;\"><span class=\"delimsizing size1\">)</span></span></span></span></span></span></span>，<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        k\n       \n      \n      \n       k\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.0315em;\">k</span></span></span></span></span>对应各个分类。</p>\n<p>（注：对于回归系数和Bounding box regression，详细可见我<a href=\"https://blog.csdn.net/takedachia/article/details/126146828\">这篇文章</a>。）</p>\n<p>每一轮的训练中，一个候选框都会有一个Ground-Truth相关的类别标签（记为类别号<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        u\n       \n      \n      \n       u\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.4306em;\"></span><span class=\"mord mathnormal\">u</span></span></span></span></span>，其中0为背景类）；还有一个基于Ground-Truth算出来的一组回归系数<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        v\n       \n      \n      \n       v\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">v</span></span></span></span></span>。<br/> 因为有了类别标签，我们的回归系数<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         t\n        \n        \n         k\n        \n       \n      \n      \n       t^{k}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.8491em;\"><span class=\"\" style=\"top: -3.063em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0315em;\">k</span></span></span></span></span></span></span></span></span></span></span></span></span>只需要关注<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        u\n       \n      \n      \n       u\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.4306em;\"></span><span class=\"mord mathnormal\">u</span></span></span></span></span>类别的，因此我们只需要 <span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         t\n        \n        \n         u\n        \n       \n      \n      \n       t^{u}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.6644em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.6644em;\"><span class=\"\" style=\"top: -3.063em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span></span></span></span></span></span></span></span></span></span>。<br/> 如下图，这样就可以对每个候选框分别计算softmax和回归器的Loss了。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\bea721f820634de79ca13f4b73e5aa11.png\"/></p>\n<p>我们的目的是设计出softmax和回归器各自的Loss后，加和后能成为一个单目标的优化问题，这样一个多任务损失函数能正常优化。</p>\n<p>作者设计的损失函数为：<br/> <span class=\"katex--display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\">\n     \n      \n       \n        \n         L\n        \n        \n         \n          (\n         \n         \n          p\n         \n         \n          ,\n         \n         \n          u\n         \n         \n          ,\n         \n         \n          \n           t\n          \n          \n           u\n          \n         \n         \n          ,\n         \n         \n          v\n         \n         \n          )\n         \n        \n        \n         =\n        \n        \n         \n          L\n         \n         \n          \n           c\n          \n          \n           l\n          \n          \n           s\n          \n         \n        \n        \n         (\n        \n        \n         p\n        \n        \n         ,\n        \n        \n         u\n        \n        \n         )\n        \n        \n         +\n        \n        \n         λ\n        \n        \n         [\n        \n        \n         u\n        \n        \n         ≥\n        \n        \n         1\n        \n        \n         ]\n        \n        \n         \n          L\n         \n         \n          \n           l\n          \n          \n           o\n          \n          \n           c\n          \n         \n        \n        \n         \n          (\n         \n         \n          \n           t\n          \n          \n           u\n          \n         \n         \n          ,\n         \n         \n          v\n         \n         \n          )\n         \n        \n       \n       \n        L\\left(p, u, t^{u}, v\\right)=L_{\\mathrm{cls}}(p, u)+\\lambda[u \\geq 1] L_{\\mathrm{loc}}\\left(t^{u}, v\\right)\n       \n      \n     </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top: 0em;\">(</span><span class=\"mord mathnormal\">p</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord mathnormal\">u</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.7144em;\"><span class=\"\" style=\"top: -3.113em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">v</span><span class=\"mclose delimcenter\" style=\"top: 0em;\">)</span></span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3361em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">cls</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">p</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord mathnormal\">u</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mopen\">[</span><span class=\"mord mathnormal\">u</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">≥</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">]</span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3361em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">loc</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top: 0em;\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.7144em;\"><span class=\"\" style=\"top: -3.113em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">v</span><span class=\"mclose delimcenter\" style=\"top: 0em;\">)</span></span></span></span></span></span></span></p>\n<p>损失函数第一项即分类损失<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         L\n        \n        \n         \n          c\n         \n         \n          l\n         \n         \n          s\n         \n        \n       \n      \n      \n       L_{\\mathrm{cls}}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.8333em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3361em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">cls</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>，为<strong>交叉熵损失函数</strong>。由于只有第<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        u\n       \n      \n      \n       u\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.4306em;\"></span><span class=\"mord mathnormal\">u</span></span></span></span></span>项的真实概率为1，所以交叉熵损失函数项直接可直接等价成：<br/> <span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         L\n        \n        \n         \n          c\n         \n         \n          l\n         \n         \n          s\n         \n        \n       \n       \n        (\n       \n       \n        p\n       \n       \n        ,\n       \n       \n        u\n       \n       \n        )\n       \n       \n        =\n       \n       \n        −\n       \n       \n        log\n       \n       \n        ⁡\n       \n       \n        \n         p\n        \n        \n         u\n        \n       \n      \n      \n       L_{\\mathrm{cls}}(p, u)=-\\log p_{u}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3361em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">cls</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">p</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord mathnormal\">u</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.8889em; vertical-align: -0.1944em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mop\">lo<span style=\"margin-right: 0.0139em;\">g</span></span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.1514em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span><br/> <span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         p\n        \n        \n         u\n        \n       \n      \n      \n       p_{u}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.625em; vertical-align: -0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.1514em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>为第<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        u\n       \n      \n      \n       u\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.4306em;\"></span><span class=\"mord mathnormal\">u</span></span></span></span></span>项的概率，即对第<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        u\n       \n      \n      \n       u\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.4306em;\"></span><span class=\"mord mathnormal\">u</span></span></span></span></span>项的预测概率取对数损失即可。</p>\n<hr/>\n<p>损失函数第二项是定位系数的回归器损失<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         L\n        \n        \n         \n          l\n         \n         \n          o\n         \n         \n          c\n         \n        \n       \n      \n      \n       L_{\\mathrm{loc}}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.8333em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3361em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">loc</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>，作者这里采用了 <span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         \n          smooth\n         \n         \n          ⁡\n         \n        \n        \n         \n          L\n         \n         \n          1\n         \n        \n       \n      \n      \n       \\operatorname{smooth}_{L_{1}}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.9445em; vertical-align: -0.2501em;\"></span><span class=\"mop\"><span class=\"mop\"><span class=\"mord mathrm\">smooth</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3283em;\"><span class=\"\" style=\"top: -2.55em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3173em;\"><span class=\"\" style=\"top: -2.357em; margin-left: 0em; margin-right: 0.0714em;\"><span class=\"pstrut\" style=\"height: 2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.143em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2501em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span> 损失函数。表示为：<br/> <span class=\"katex--display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\">\n     \n      \n       \n        \n         \n          L\n         \n         \n          \n           l\n          \n          \n           o\n          \n          \n           c\n          \n         \n        \n        \n         \n          (\n         \n         \n          \n           t\n          \n          \n           u\n          \n         \n         \n          ,\n         \n         \n          v\n         \n         \n          )\n         \n        \n        \n         =\n        \n        \n         \n          ∑\n         \n         \n          \n           i\n          \n          \n           ∈\n          \n          \n           {\n          \n          \n           x\n          \n          \n           ,\n          \n          \n           y\n          \n          \n           ,\n          \n          \n           w\n          \n          \n           ,\n          \n          \n           h\n          \n          \n           }\n          \n         \n        \n        \n         \n          \n           smooth\n          \n          \n           ⁡\n          \n         \n         \n          \n           L\n          \n          \n           1\n          \n         \n        \n        \n         \n          (\n         \n         \n          \n           t\n          \n          \n           i\n          \n          \n           u\n          \n         \n         \n          −\n         \n         \n          \n           v\n          \n          \n           i\n          \n         \n         \n          )\n         \n        \n       \n       \n         L_{\\mathrm{loc}}\\left(t^{u}, v\\right)=\\sum_{i \\in\\{\\mathrm{x}, \\mathrm{y}, \\mathrm{w}, \\mathrm{h}\\}} \\operatorname{smooth}_{L_{1}}\\left(t_{i}^{u}-v_{i}\\right) \n       \n      \n     </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3361em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">loc</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top: 0em;\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.7144em;\"><span class=\"\" style=\"top: -3.113em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">v</span><span class=\"mclose delimcenter\" style=\"top: 0em;\">)</span></span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 2.566em; vertical-align: -1.516em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.05em;\"><span class=\"\" style=\"top: -1.809em; margin-left: 0em;\"><span class=\"pstrut\" style=\"height: 3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">∈</span><span class=\"mopen mtight\">{<!-- --></span><span class=\"mord mathrm mtight\">x</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathrm mtight\" style=\"margin-right: 0.0139em;\">y</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathrm mtight\" style=\"margin-right: 0.0139em;\">w</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathrm mtight\">h</span><span class=\"mclose mtight\">}</span></span></span></span><span class=\"\" style=\"top: -3.05em;\"><span class=\"pstrut\" style=\"height: 3.05em;\"></span><span class=\"\"><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.516em;\"><span class=\"\"></span></span></span></span></span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mop\"><span class=\"mop\"><span class=\"mord mathrm\">smooth</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3283em;\"><span class=\"\" style=\"top: -2.55em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3173em;\"><span class=\"\" style=\"top: -2.357em; margin-left: 0em; margin-right: 0.0714em;\"><span class=\"pstrut\" style=\"height: 2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.143em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2501em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top: 0em;\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.7144em;\"><span class=\"\" style=\"top: -2.453em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"\" style=\"top: -3.113em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.247em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3117em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top: 0em;\">)</span></span></span></span></span></span></span><br/> 暂先不管<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         \n          smooth\n         \n         \n          ⁡\n         \n        \n        \n         \n          L\n         \n         \n          1\n         \n        \n       \n      \n      \n       \\operatorname{smooth}_{L_{1}}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.9445em; vertical-align: -0.2501em;\"></span><span class=\"mop\"><span class=\"mop\"><span class=\"mord mathrm\">smooth</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3283em;\"><span class=\"\" style=\"top: -2.55em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3173em;\"><span class=\"\" style=\"top: -2.357em; margin-left: 0em; margin-right: 0.0714em;\"><span class=\"pstrut\" style=\"height: 2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.143em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2501em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>。对于一个候选框样本，它有一个定位信息<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        i\n       \n      \n      \n       i\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.6595em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span></span>，它相对于类别<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        u\n       \n      \n      \n       u\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.4306em;\"></span><span class=\"mord mathnormal\">u</span></span></span></span></span>的GT框会产生一个真实的偏移系数<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         v\n        \n        \n         i\n        \n       \n      \n      \n       v_i\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.5806em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3117em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>，还会根据回归器预测出一个偏移系数<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         t\n        \n        \n         i\n        \n        \n         u\n        \n       \n      \n      \n       t_{i}^{u}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.9231em; vertical-align: -0.2587em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.6644em;\"><span class=\"\" style=\"top: -2.4413em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"\" style=\"top: -3.063em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2587em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>，从它俩之间构建损失函数。</p>\n<p>在R-CNN的bbox回归问题中（详见我<a href=\"https://blog.csdn.net/takedachia/article/details/126146828\">这篇文章</a>），回归器的损失函数是平方和，即<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         L\n        \n        \n         2\n        \n       \n      \n      \n       L_2\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.8333em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>损失函数，它可以求得参数的解析解。</p>\n<p>但是，Fast R-CNN中，我们需要同时优化两个任务（分类+定位）以构建端到端的训练，并且使用两者加和的方式转化为单目标优化问题。如果使用<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         L\n        \n        \n         2\n        \n       \n      \n      \n       L_2\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.8333em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>损失，一旦<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         t\n        \n        \n         i\n        \n        \n         u\n        \n       \n      \n      \n       t_{i}^{u}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.9231em; vertical-align: -0.2587em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.6644em;\"><span class=\"\" style=\"top: -2.4413em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"\" style=\"top: -3.063em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2587em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>和<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         v\n        \n        \n         i\n        \n       \n      \n      \n       v_i\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.5806em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3117em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>相差过大，因为平方项的关系，Loss值会非常大，可能会大交叉熵的Loss几个数量级。这样可能会造成学习困难，所以两个任务的Loss最好保持在一个量度上。<br/> 但是如果使用<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         L\n        \n        \n         1\n        \n       \n      \n      \n       L_1\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.8333em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>损失的话，零点处又难以求导。所以作者采用了<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         \n          smooth\n         \n         \n          ⁡\n         \n        \n        \n         \n          L\n         \n         \n          1\n         \n        \n       \n      \n      \n       \\operatorname{smooth}_{L_{1}}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.9445em; vertical-align: -0.2501em;\"></span><span class=\"mop\"><span class=\"mop\"><span class=\"mord mathrm\">smooth</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3283em;\"><span class=\"\" style=\"top: -2.55em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3173em;\"><span class=\"\" style=\"top: -2.357em; margin-left: 0em; margin-right: 0.0714em;\"><span class=\"pstrut\" style=\"height: 2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.143em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2501em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span> 损失函数，在靠近0的区间范围内（<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        [\n       \n       \n        −\n       \n       \n        1\n       \n       \n        ,\n       \n       \n        1\n       \n       \n        ]\n       \n      \n      \n       [-1,1]\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">−</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">]</span></span></span></span></span>），函数变成弯曲的形状便于求导。三者的函数图像如下图所示：</p>\n<p><img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\9a35e808a6154510be130d95c839f0e2.png\"/></p>\n<p><span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         \n          smooth\n         \n         \n          ⁡\n         \n        \n        \n         \n          L\n         \n         \n          1\n         \n        \n       \n      \n      \n       \\operatorname{smooth}_{L_{1}}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.9445em; vertical-align: -0.2501em;\"></span><span class=\"mop\"><span class=\"mop\"><span class=\"mord mathrm\">smooth</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3283em;\"><span class=\"\" style=\"top: -2.55em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3173em;\"><span class=\"\" style=\"top: -2.357em; margin-left: 0em; margin-right: 0.0714em;\"><span class=\"pstrut\" style=\"height: 2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.143em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2501em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>写成函数表达如下：<br/> <span class=\"katex--display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\">\n     \n      \n       \n        \n         \n          \n           smooth\n          \n          \n           ⁡\n          \n         \n         \n          \n           L\n          \n          \n           1\n          \n         \n        \n        \n         (\n        \n        \n         x\n        \n        \n         )\n        \n        \n         =\n        \n        \n         \n          {\n         \n         \n          \n           \n            \n             \n              \n               0.5\n              \n              \n               \n                x\n               \n               \n                2\n               \n              \n             \n            \n           \n           \n            \n             \n              \n                if \n              \n              \n               ∣\n              \n              \n               x\n              \n              \n               ∣\n              \n              \n               &lt;\n              \n              \n               1\n              \n             \n            \n           \n          \n          \n           \n            \n             \n              \n               ∣\n              \n              \n               x\n              \n              \n               ∣\n              \n              \n               −\n              \n              \n               0.5\n              \n             \n            \n           \n           \n            \n             \n               otherwise \n             \n            \n           \n          \n         \n        \n       \n       \n         \\operatorname{smooth}_{L_{1}}(x)= \\begin{cases}0.5 x^{2} &amp; \\text { if }|x|&lt;1 \\\\ |x|-0.5 &amp; \\text { otherwise }\\end{cases} \n       \n      \n     </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 1.0001em; vertical-align: -0.2501em;\"></span><span class=\"mop\"><span class=\"mop\"><span class=\"mord mathrm\">smooth</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3283em;\"><span class=\"\" style=\"top: -2.55em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3173em;\"><span class=\"\" style=\"top: -2.357em; margin-left: 0em; margin-right: 0.0714em;\"><span class=\"pstrut\" style=\"height: 2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.143em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2501em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 3em; vertical-align: -1.25em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top: 0em;\"><span class=\"delimsizing size4\">{<!-- --></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.69em;\"><span class=\"\" style=\"top: -3.69em;\"><span class=\"pstrut\" style=\"height: 3.008em;\"></span><span class=\"mord\"><span class=\"mord\">0.5</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.8141em;\"><span class=\"\" style=\"top: -3.063em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span><span class=\"\" style=\"top: -2.25em;\"><span class=\"pstrut\" style=\"height: 3.008em;\"></span><span class=\"mord\"><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">∣</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mord\">0.5</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.19em;\"><span class=\"\"></span></span></span></span></span><span class=\"arraycolsep\" style=\"width: 1em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.69em;\"><span class=\"\" style=\"top: -3.69em;\"><span class=\"pstrut\" style=\"height: 3.008em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\"> if </span></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">∣</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mord\">1</span></span></span><span class=\"\" style=\"top: -2.25em;\"><span class=\"pstrut\" style=\"height: 3.008em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\"> otherwise </span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.19em;\"><span class=\"\"></span></span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></span></p>\n<p>作者在论文中也提到，Smooth L1损失相较于L2对远端值不敏感。如果回归的范围没有限制，使用L2损失就需要仔细调学习率以防梯度爆炸。</p>\n<p>最后，<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         L\n        \n        \n         \n          l\n         \n         \n          o\n         \n         \n          c\n         \n        \n       \n      \n      \n       L_{\\mathrm{loc}}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.8333em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3361em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">loc</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>前面的<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        [\n       \n       \n        u\n       \n       \n        ≥\n       \n       \n        1\n       \n       \n        ]\n       \n      \n      \n       [u \\geq 1]\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\">u</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">≥</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">]</span></span></span></span></span>项表示，类别号在大于等于1时，这个项为1。如<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        u\n       \n       \n        =\n       \n       \n        0\n       \n      \n      \n       u=0\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.4306em;\"></span><span class=\"mord mathnormal\">u</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.6444em;\"></span><span class=\"mord\">0</span></span></span></span></span>，即为背景类时，这个项等于0，即不计算回归损失。<br/> 再前面的<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        λ\n       \n      \n      \n       \\lambda\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.6944em;\"></span><span class=\"mord mathnormal\">λ</span></span></span></span></span>是加和系数，是超参数，作者认为将其设为1可平衡两个Loss任务。</p>\n<hr/>\n<p>综上，损失函数的设计将Fast R-CNN的训练变成了单阶段的训练，训练效率大大提升。<br/> 这是相较于R-CNN、SPPnet的一大创新点。</p>\n<h3><a id=\"_RoI_pooling__211\"></a>关于 RoI pooling 层的梯度反传</h3>\n<p>论文这部分讲到了一个训练时的细节，即RoI pooling是怎么进行梯度反传的？</p>\n<hr/>\n<p>因为RoI pooling是一类Max pooling，我们<strong>先要知道Max pooling是怎么反向传播梯度的</strong>：<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\ca4082aa1ca647d5badf371366e52e19.png\"/></p>\n<p>上图中我们对一张4×4的feature map做大小为2×2，步长为2的最大池化，得到一个2×2的output。并且这4个输出值<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         y\n        \n        \n         1\n        \n       \n       \n        ,\n       \n       \n        \n         y\n        \n        \n         2\n        \n       \n       \n        ,\n       \n       \n        \n         y\n        \n        \n         3\n        \n       \n       \n        ,\n       \n       \n        \n         y\n        \n        \n         4\n        \n       \n      \n      \n       y_1,y_2,y_3,y_4\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.625em; vertical-align: -0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">3</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">4</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>应当带上它在原feature map中的位置信息，即<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         x\n        \n        \n         6\n        \n       \n       \n        ,\n       \n       \n        \n         x\n        \n        \n         8\n        \n       \n       \n        ,\n       \n       \n        \n         x\n        \n        \n         9\n        \n       \n       \n        ,\n       \n       \n        \n         x\n        \n        \n         16\n        \n       \n      \n      \n       x_6,x_8,x_9,x_{16}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.625em; vertical-align: -0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">6</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">8</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">9</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">16</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>）。<br/> 对这个output求导后，各个位置的梯度大小应该反向赋给原feature map的对应位置，然后其余位置梯度都是0。<br/> 这就是max pooling的反向传梯度的过程。</p>\n<p>在实际问题中，我们经常会碰到池化核区域重叠的情况，即<strong>步长＜尺寸</strong>。<br/> 当上一个池化核扫到的最大值 和 下一个扫到的最大值都是同一个位置的数，都位于重叠区域时，梯度又该怎么算呢？<br/> 如下图。<br/> 我们在算<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         x\n        \n        \n         4\n        \n       \n      \n      \n       x_4\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.5806em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">4</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>处的梯度应当是最后<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         y\n        \n        \n         1\n        \n       \n       \n        ,\n       \n       \n        \n         y\n        \n        \n         2\n        \n       \n      \n      \n       y_1,y_2\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.625em; vertical-align: -0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>两处偏导数的和，因为它们对应的原feature map的位置是同一个。</p>\n<p><img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\c716366c0d714c5da5d40d42109ee2ff.png\"/></p>\n<hr/>\n<p>在RoI pooling中，梯度的反传和上面讲到的Max pooling机制是一样的。<br/> 有一点新的不同就是，Fast R-CNN训练时传入的一个mini-batch是从2张图片中各随机选取的64候选框。<br/> 我们以1张图片抽取64个候选框为例，我们需要在1张图片生成的conv feature map上抠出大大小小64张小feature map，再传入RoI pooling层。所以这些小feature map的位置一定会有重叠的情况，如果我们的max pooling取到的最大值恰好在重叠区域又怎么办呢？</p>\n<p>见下图：<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\e022e5755c5a49298894ac3ed221544d.png\"/></p>\n<p>一张图片的conv feature map中有2个候选框映射，它们各自需要输出2×2的output。<br/> 假设它们之间如图位置有一重合处（对于框1为<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         x\n        \n        \n         169\n        \n       \n      \n      \n       x_{169}\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.5806em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">169</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>；对于框2为<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         x\n        \n        \n         1\n        \n       \n      \n      \n       x_1\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.5806em; vertical-align: -0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>），在各自max pooling后都选取了这个数作为对于位置的输出（框1输出在<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         y\n        \n        \n         4\n        \n       \n      \n      \n       y_4\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.625em; vertical-align: -0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">4</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>，框2输出在<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        \n         y\n        \n        \n         1\n        \n       \n      \n      \n       y_1\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.625em; vertical-align: -0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3011em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span></span></span>）。<br/> 本质上这是来自同一个位置的<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        x\n       \n      \n      \n       x\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span></span>，那么此处的偏导也是框1、框2各自输出的对应位置<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        y\n       \n      \n      \n       y\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.625em; vertical-align: -0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">y</span></span></span></span></span>的偏导的和。</p>\n<p>所以，在Fast R-CNN的RoI pooling层中，对一个输入的<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        x\n       \n      \n      \n       x\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span></span>位置求偏导经历了两次加和，如output之间存在同源最大值，反向传播时就需要在源位置将梯度加和。</p>\n<p>故论文中这个难懂的公式讲的就是上面这个意思，只要懂这个公式在做一件什么事即可：<br/> <span class=\"katex--display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\">\n     \n      \n       \n        \n         \n          \n           ∂\n          \n          \n           L\n          \n         \n         \n          \n           ∂\n          \n          \n           \n            x\n           \n           \n            i\n           \n          \n         \n        \n        \n         =\n        \n        \n         \n          ∑\n         \n         \n          r\n         \n        \n        \n         \n          ∑\n         \n         \n          j\n         \n        \n        \n         \n          [\n         \n         \n          i\n         \n         \n          =\n         \n         \n          \n           i\n          \n          \n           ∗\n          \n         \n         \n          (\n         \n         \n          r\n         \n         \n          ,\n         \n         \n          j\n         \n         \n          )\n         \n         \n          ]\n         \n        \n        \n         \n          \n           ∂\n          \n          \n           L\n          \n         \n         \n          \n           ∂\n          \n          \n           \n            y\n           \n           \n            \n             r\n            \n            \n             j\n            \n           \n          \n         \n        \n       \n       \n         \\frac{\\partial L}{\\partial x_{i}}=\\sum_{r} \\sum_{j}\\left[i=i^{*}(r, j)\\right] \\frac{\\partial L}{\\partial y_{r j}} \n       \n      \n     </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 2.2074em; vertical-align: -0.836em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.3714em;\"><span class=\"\" style=\"top: -2.314em;\"><span class=\"pstrut\" style=\"height: 3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right: 0.0556em;\">∂</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3117em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span></span></span><span class=\"\" style=\"top: -3.23em;\"><span class=\"pstrut\" style=\"height: 3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width: 0.04em;\"></span></span><span class=\"\" style=\"top: -3.677em;\"><span class=\"pstrut\" style=\"height: 3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right: 0.0556em;\">∂</span><span class=\"mord mathnormal\">L</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.836em;\"><span class=\"\"></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 2.7852em; vertical-align: -1.4138em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.05em;\"><span class=\"\" style=\"top: -1.9em; margin-left: 0em;\"><span class=\"pstrut\" style=\"height: 3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0278em;\">r</span></span></span></span><span class=\"\" style=\"top: -3.05em;\"><span class=\"pstrut\" style=\"height: 3.05em;\"></span><span class=\"\"><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.25em;\"><span class=\"\"></span></span></span></span></span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.05em;\"><span class=\"\" style=\"top: -1.8723em; margin-left: 0em;\"><span class=\"pstrut\" style=\"height: 3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0572em;\">j</span></span></span></span><span class=\"\" style=\"top: -3.05em;\"><span class=\"pstrut\" style=\"height: 3.05em;\"></span><span class=\"\"><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.4138em;\"><span class=\"\"></span></span></span></span></span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top: 0em;\">[</span><span class=\"mord mathnormal\">i</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">i</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.7387em;\"><span class=\"\" style=\"top: -3.113em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">∗</span></span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right: 0.0278em;\">r</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.0572em;\">j</span><span class=\"mclose\">)</span><span class=\"mclose delimcenter\" style=\"top: 0em;\">]</span></span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 1.3714em;\"><span class=\"\" style=\"top: -2.314em;\"><span class=\"pstrut\" style=\"height: 3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right: 0.0556em;\">∂</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.0359em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.3117em;\"><span class=\"\" style=\"top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0278em;\">r</span><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.0572em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2861em;\"><span class=\"\"></span></span></span></span></span></span></span></span><span class=\"\" style=\"top: -3.23em;\"><span class=\"pstrut\" style=\"height: 3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width: 0.04em;\"></span></span><span class=\"\" style=\"top: -3.677em;\"><span class=\"pstrut\" style=\"height: 3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right: 0.0556em;\">∂</span><span class=\"mord mathnormal\">L</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.9721em;\"><span class=\"\"></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></span></p>\n<h3><a id=\"_251\"></a>关于尺度不变性</h3>\n<p>尺度不变性是目标检测模型的一个评估指标。<br/> 如果给你一张经过缩放的大图和小图，模型都能辨别出图片中有<em>一匹马，马上骑着一个人</em>的话，那么模型的尺度不变性比较好。</p>\n<p>提高模型的尺度不变性，可以在训练时将所有图片固定在一个尺寸，测试时也使用同一个尺寸，我们的目标是让模型自己去学习其中的尺度不变性。这称为<strong>单尺度</strong>目标检测。<br/> 此外，还可以将图片在训练时随机缩放成1种预设的尺寸进行训练，这也是一种数据增强的方法。在测试时，将图片缩放成所有预设的尺寸，形成一个“图像金字塔”，对每个尺寸下的生成的候选框进行尺度归一化。这称为<strong>多尺度</strong>目标检测。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\e85628fedd3d4a7abb91d72604295096.png\"/></p>\n<p>论文后面（试验结果2部分）作者会做一些实验来探索尺度不变性。</p>\n<h2><a id=\"_261\"></a>模型的测试细节</h2>\n<p>模型fine tuning训练好后，就可以进行测试环节了。<br/> 在对一张图片进行测试前，一样需要随机生成（通过selective search）大约2000个候选框的位置信息。<br/> 然后进入前向计算，之后会对结果用非极大值抑制算法处理多余的候选框（和R-CNN一样）。</p>\n<h3><a id=\"_SVD__266\"></a>利用 截断性的SVD 加速推理</h3>\n<p>我们知道在Fast R-CNN模型的最后存在着大量的全连接层，如下图。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\1e80c196784145b7a9140c140ad1dcc6.png\"/></p>\n<p>我们知道Fast R-CNN传入一张图片后，仅需要在CNN网络进行<strong>1次</strong>前向计算，再通过RoI project投影出对应的feature map，进入RoI池化。然后，输出的每一个特征向量（约2000个）都会经历一次全连接层的前向运算，再进入softmax和bbox regression。</p>\n<p>故测试时的全连接层运算量远大于训练时。<br/> 作者指出，事实上测试时大量的算力都会耗费在全连接层的计算上。<br/> 我们知道全连接层的计算都是矩阵运算，对于一个矩阵<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        W\n       \n      \n      \n       W\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.1389em;\">W</span></span></span></span></span>，我们可以对其进行奇异值分解（SVD），将一个臃肿的矩阵简化成三个矩阵的乘积，我们可以对三个矩阵进行“瘦身”，取奇异值大小排名靠前的对应部分组成新的矩阵，且几乎不损耗原矩阵<span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n    \n     \n      \n       \n        W\n       \n      \n      \n       W\n      \n     \n    </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.1389em;\">W</span></span></span></span></span>的信息：<br/> <span class=\"katex--display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\">\n     \n      \n       \n        \n         W\n        \n        \n         ≈\n        \n        \n         U\n        \n        \n         \n          Σ\n         \n         \n          t\n         \n        \n        \n         \n          V\n         \n         \n          T\n         \n        \n       \n       \n         W \\approx U \\Sigma_{t} V^{T} \n       \n      \n     </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.1389em;\">W</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">≈</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 1.0413em; vertical-align: -0.15em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.109em;\">U</span><span class=\"mord\"><span class=\"mord\">Σ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.2806em;\"><span class=\"\" style=\"top: -2.55em; margin-left: 0em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.15em;\"><span class=\"\"></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right: 0.2222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height: 0.8913em;\"><span class=\"\" style=\"top: -3.113em; margin-right: 0.05em;\"><span class=\"pstrut\" style=\"height: 2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right: 0.1389em;\">T</span></span></span></span></span></span></span></span></span></span></span></span></span></span><br/> 经过瘦身的矩阵运算速度将会大大提升。具体提升多少见后面的实验结果部分。<br/> （SVD这部分具体知识细节可参考线性代数）</p>\n<h2><a id=\"1_282\"></a>实验结果1</h2>\n<p>模型的以下3项主要实验结果支持了Fast R-CNN的贡献：</p>\n<ul><li>在VOC2007、2010、2012数据集上有出色的精度</li><li>比R-CNN、SPPnet更快的训练、测试速度</li><li>使用主干网络为VGG-16模型时，微调卷积层的权重提升了检测精度</li></ul>\n<p>实验中作者主要使用了三种CNN模型作为Fast R-CNN的主干网络。<br/> 将以AlexNet为主干网络的模型称为<strong>S</strong>模型（small）；<br/> 将以VGG_CNN_M_1024为主干网络的模型称为<strong>M</strong>模型（medium），它与S模型同深度，但更宽；<br/> 将以VGG-16为主干网络的模型称为<strong>L</strong>模型（largest），是最大的模型。</p>\n<h3><a id=\"VOC_294\"></a>在VOC数据集上的结果</h3>\n<p>下面3张表依次是VOC2007、2010、2012上的结果，并细分到了各个类别的精度。所有模型的主干网络都使用了VGG-16（即L模型）。<br/> 可以看到Fast R-CNN（文中的FRCN [ours]）的总体精度是最高的。可以粗略看到右下角的mAP值都比上面的要高。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\7762c3d4d582473ea6344604eb9ca5ec.png\"/></p>\n<h3><a id=\"_299\"></a>训练和测试速度的提升</h3>\n<p>作者做了下表，展示了 Fast R-CNN 、R-CNN、SPPnet的训练、测试时长比较。3类不同大小的模型都进行了比较。<br/> 可以看到在S模型中，Fast R-CNN的训练速度比R-CNN快了18.3倍。<br/> 在L模型中，测试速度快了146倍；如果测试时使用前面提到的SVD技术，速度提升了213倍。同时精度是最高的。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\373e1e0171164c09862db1f75e94a94c.png\"/><br/> 这里作者就指出测试时使用了SVD会提速30%多，但精度只下降了0.3%。<br/> 并且SVD是个单独的trick，不需要进行事后的再fine tuning。</p>\n<h3><a id=\"finefune__307\"></a>fine-fune 卷积层的权重提升检测精度</h3>\n<p>如前所述，SPPnet难以更新模型权重，所以当时SPPnet使用了冻结全连接层之前所有层权重的方法，以提高效率，并且准确率也还不错。<br/> 但作者认为，前面卷积层的参数也很重要，fine-tune卷积可提升模型精度。<br/> 于是作者用L模型做了冻结层参数的实验，证明了这个结论。</p>\n<p>下图显示，微调全连接层fc6的精度不如微调卷积层conv3_1、conv2_1的精度，微调的层越深，效果越好。</p>\n<p><img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\2c2101df8c344662a1089255ed482800.png\"/><br/> 但是不是所有的卷积层都有必要被fine-tune呢？作者认为也不一定。<br/> 作者发现在更小的模型中（如S模型和M模型），微调第一个卷积层conv1对精度没什么影响，反而让模型训练效率下降（比如需要更大的GPU显存）。从第二个卷积层开始微调相对比较合适。</p>\n<h2><a id=\"2Fast_RCNN_318\"></a>实验结果2——验证Fast R-CNN设计的有效性</h2>\n<p>作者接着就Fast R-CNN模型本身的设计，做了一些实验论证其有效性。<br/> <em>我觉得这是非常值得学习的科研思路，不仅需要对一个新方法的结果和老方法进行对比论证，还需要对它的新的设计论证其科学性，更好地解释论证新方法的科学价值。</em></p>\n<h3><a id=\"_322\"></a>多任务训练真的有用吗？</h3>\n<p>我们知道R-CNN和SPPnet使用了多任务训练的方法，对一个模型分别训练1个CNN模型用于提取特征、1个线性SVM分类器用于候选框图像分类、1个bbox回归器用于调整框的定位。<br/> 而Fast R-CNN设计了一个损失函数将多任务优化成了单任务，优化一个损失函数一步到位进行训练。这样降低了训练资源消耗，提升了效率。</p>\n<p>但是多任务训练是否真的提升了模型的精度呢？作者设计了实验，在Fast R-CNN模型上，人工将这3个任务（特征提取、分类、回归）各自分开、组合训练，进行对比。<br/> 下图左边依次是 <strong>多任务训练</strong>、<strong>单阶段训练</strong>、<strong>测试时使用bbox回归</strong>，打上勾的为训练测试的组合。<br/> 而第一列我用黑框框出来(即都没有打勾)的表示只使用图像分类（交叉熵损失）进行训练得到的结果。<br/> 实验结果用VOC07数据集上的mAP表示。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\6c8468a252d44261a1628fe224090a00.png\"/><br/> 可以看到使用多任务训练+bbox回归是精度提升最高的。证明多任务训练是work的。</p>\n<h3><a id=\"_333\"></a>关于尺度不变性</h3>\n<p>前面介绍了两种关于尺度不变性的目标检测策略。<br/> 作者做了实验来对比两种策略的结果，对比了两者的性能和精度差异。</p>\n<p>下图scales中：1表示单尺度策略（都缩放成600像素大小），5表示多尺度策略（有480、576、688、864、1200五种预设尺度）。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\9169ba1337f444ffa98f691523a66729.png\"/></p>\n<p>可看到多尺度策略下检测精度比单尺度高一点，但是单尺度检测速率比多尺度要快很多。<br/> 所以看上去单尺度性价比高，实际情况下需要进行取舍。</p>\n<h3><a id=\"_343\"></a>是否需要更多的训练数据</h3>\n<p>作者看到传统的目标检测算法（如DPM，也是原作者自己的工作）有一个精度饱和（mAP saturate）的现象。<br/> 即训练数据到了一定程度后，模型的精度上不去了。<br/> 那么Fast R-CNN是否有这样的现象呢？</p>\n<p>作者做实验对VOC07数据集进行了扩充，将VOC2012扩充进训练集，发现精度上升（66.9% → 70.0%）。<br/> 因此，Fast R-CNN模型在训练数据增大的时候，模型的精度也提升，没有精度饱和现象，这是一个优秀的目标检测模型应有的特性。</p>\n<h3><a id=\"SVM_vs_softmax_351\"></a>SVM分类器 vs softmax</h3>\n<p>我们知道R-CNN和SPPnet使用了SVM分类器对候选框进行图像分类。<br/> 但Fast R-CNN设计成了softmax分类，这和整个模型的设计目的有关，为了进行多任务训练以及模型的fine-tuning。</p>\n<p>就分类效果上，SVM和softmax相比又如何呢？作者做了如下实验。</p>\n<p>我们发现在Fast R-CNN中，softmax比SVM更有用。<br/> 并且虽然在小模型上R-CNN中的SVM精度高一点，但随着模型增大，softmax依然是最有用的那一个。<br/> 考虑到多任务训练的效率等因素，softmax是优于SVM的。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\b0a1ed54d24046a59552815926fa2874.png\"/></p>\n<h3><a id=\"_362\"></a>候选框越多越好吗？</h3>\n<p>目前主要有两种框生成算法，一种是生成比较稀疏分布的selective search，一种是基于DPM的生成稠密候选框的算法。</p>\n<p>简单看一下下面这张图，可看到使用了selective search（图中蓝色实线）随着候选框数量增多，精度也会发生波动，所以<strong>并不是候选框越多越好</strong>。<br/> 使用了selective search+随机生成稠密框的组合算法，随着框数量增加，精度大幅下降。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\4c96df5d23e3418584bef6f3f9010cea.png\"/></p>\n<p>（完）</p>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>", "first_tag": "Others", "cpp": 0, "csharp": 0, "python": 0, "javascript": 0, "java": 0, "sql": 0, "php": 0, "time": "2022-08-23 11:54:41", "summary": "《》是同一个作者基于自己之前的工作的改进。也是基于深度卷积神经网络用于计算机视觉任务主要用于目标检测的算法。他在的基础上进行了大幅创新，比如将目标的分类和定位的步骤进行了统一，实现了端到端的训练、预测"}