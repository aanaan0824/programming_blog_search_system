{"blogid": "120930016", "writerAge": "ç é¾„3å¹´", "writerBlogNum": "46", "writerCollect": "358", "writerComment": "35", "writerFan": "32", "writerGrade": "3çº§", "writerIntegral": "574", "writerName": "@ç§‹é‡", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_120930016.jpg", "writerRankTotal": "92633", "writerRankWeekly": "363650", "writerThumb": "77", "writerVisitNum": "61882", "blog_read_count": "12299", "blog_time": "äºÂ 2021-10-24 09:38:23Â å‘å¸ƒ", "blog_title": "YOLOV5æºç çš„è¯¦ç»†è§£è¯»", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-atom-one-dark\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<h2><a id=\"YOLOv5_0\"></a>YOLOv5ç›®å½•ç»“æ„</h2>\n<p>â”œâ”€â”€ dataï¼šä¸»è¦æ˜¯å­˜æ”¾ä¸€äº›è¶…å‚æ•°çš„é…ç½®æ–‡ä»¶ï¼ˆè¿™äº›æ–‡ä»¶ï¼ˆyamlæ–‡ä»¶ï¼‰æ˜¯ç”¨æ¥é…ç½®è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿˜æœ‰éªŒè¯é›†çš„è·¯å¾„çš„ï¼Œå…¶ä¸­è¿˜åŒ…æ‹¬ç›®æ ‡æ£€æµ‹çš„ç§ç±»æ•°å’Œç§ç±»çš„åç§°ï¼‰ï¼›è¿˜æœ‰ä¸€äº›å®˜æ–¹æä¾›æµ‹è¯•çš„å›¾ç‰‡ã€‚å¦‚æœæ˜¯è®­ç»ƒè‡ªå·±çš„æ•°æ®é›†çš„è¯ï¼Œé‚£ä¹ˆå°±éœ€è¦ä¿®æ”¹å…¶ä¸­çš„yamlæ–‡ä»¶ã€‚ä½†æ˜¯è‡ªå·±çš„æ•°æ®é›†ä¸å»ºè®®æ”¾åœ¨è¿™ä¸ªè·¯å¾„ä¸‹é¢ï¼Œè€Œæ˜¯å»ºè®®æŠŠæ•°æ®é›†æ”¾åˆ°yolov5é¡¹ç›®çš„åŒçº§ç›®å½•ä¸‹é¢ã€‚</p>\n<p>|â€”â€”dataset :å­˜æ”¾è‡ªå·±çš„æ•°æ®é›†ï¼Œåˆ†ä¸ºimageså’Œlabelsä¸¤éƒ¨åˆ†</p>\n<p>â”œâ”€â”€ modelsï¼šé‡Œé¢ä¸»è¦æ˜¯ä¸€äº›ç½‘ç»œæ„å»ºçš„é…ç½®æ–‡ä»¶å’Œå‡½æ•°ï¼Œå…¶ä¸­åŒ…å«äº†è¯¥é¡¹ç›®çš„å››ä¸ªä¸åŒçš„ç‰ˆæœ¬ï¼Œåˆ†åˆ«ä¸ºæ˜¯sã€mã€lã€xã€‚ä»åå­—å°±å¯ä»¥çœ‹å‡ºï¼Œè¿™å‡ ä¸ªç‰ˆæœ¬çš„å¤§å°ã€‚ä»–ä»¬çš„æ£€æµ‹æµ‹åº¦åˆ†åˆ«éƒ½æ˜¯ä»å¿«åˆ°æ…¢ï¼Œä½†æ˜¯ç²¾ç¡®åº¦åˆ†åˆ«æ˜¯ä»ä½åˆ°é«˜ã€‚è¿™å°±æ˜¯æ‰€è°“çš„é±¼å’Œç†ŠæŒä¸å¯å…¼å¾—ã€‚å¦‚æœè®­ç»ƒè‡ªå·±çš„æ•°æ®é›†çš„è¯ï¼Œå°±éœ€è¦ä¿®æ”¹è¿™é‡Œé¢ç›¸å¯¹åº”çš„yamlæ–‡ä»¶æ¥è®­ç»ƒè‡ªå·±æ¨¡å‹ã€‚</p>\n<p>â”œâ”€â”€ utilsï¼šå­˜æ”¾çš„æ˜¯å·¥å…·ç±»çš„å‡½æ•°ï¼Œé‡Œé¢æœ‰losså‡½æ•°ï¼Œmetricså‡½æ•°ï¼Œplotså‡½æ•°ç­‰ç­‰ã€‚</p>\n<p>â”œâ”€â”€ weightsï¼šæ”¾ç½®è®­ç»ƒå¥½çš„æƒé‡å‚æ•°ptæ–‡ä»¶ã€‚</p>\n<p>â”œâ”€â”€ detect.pyï¼šåˆ©ç”¨è®­ç»ƒå¥½çš„æƒé‡å‚æ•°è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¯ä»¥è¿›è¡Œå›¾åƒã€è§†é¢‘å’Œæ‘„åƒå¤´çš„æ£€æµ‹ã€‚</p>\n<p>â”œâ”€â”€ train.pyï¼šè®­ç»ƒè‡ªå·±çš„æ•°æ®é›†çš„å‡½æ•°ã€‚</p>\n<p>â”œâ”€â”€ test.pyï¼šæµ‹è¯•è®­ç»ƒçš„ç»“æœçš„å‡½æ•°ã€‚</p>\n<p><strong>|â€”â€”</strong> hubconf.py:pytorch hub ç›¸å…³ä»£ç </p>\n<p><strong>|â€”â€”</strong> sotabench.py: cocoæ•°æ®é›†æµ‹è¯•è„šæœ¬</p>\n<p><strong>|â€”â€”</strong> tutorial.ipynb: jupyter notebook æ¼”ç¤ºæ–‡ä»¶</p>\n<p>â”œâ”€â”€requirements.txtï¼šè¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œé‡Œé¢å†™ç€ä½¿ç”¨yolov5é¡¹ç›®çš„ç¯å¢ƒä¾èµ–åŒ…çš„ä¸€äº›ç‰ˆæœ¬ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–‡æœ¬å¯¼å…¥ç›¸åº”ç‰ˆæœ¬çš„åŒ…ã€‚</p>\n<p>|----runæ—¥å¿—æ–‡ä»¶ï¼Œæ¯æ¬¡è®­ç»ƒçš„æ•°æ®ï¼ŒåŒ…å«æƒé‡æ–‡ä»¶ï¼Œè®­ç»ƒæ•°æ®ï¼Œç›´æ–¹å›¾ç­‰</p>\n<p>|â€”â€”LICENCE ç‰ˆæƒæ–‡ä»¶</p>\n<p>ä»¥ä¸Šå°±æ˜¯yolov5é¡¹ç›®ä»£ç çš„æ•´ä½“ä»‹ç»ã€‚æˆ‘ä»¬è®­ç»ƒå’Œæµ‹è¯•è‡ªå·±çš„æ•°æ®é›†åŸºæœ¬å°±æ˜¯åˆ©ç”¨åˆ°å¦‚ä¸Šçš„ä»£ç ã€‚</p>\n<h2><a id=\"data_31\"></a>dataæ–‡ä»¶å¤¹</h2>\n<ul><li>yamlå¤šç§æ•°æ®é›†çš„é…ç½®æ–‡ä»¶ï¼Œå¦‚cocoï¼Œcoco128ï¼Œpascalvocç­‰</li><li>hyps è¶…å‚æ•°å¾®è°ƒé…ç½®æ–‡ä»¶</li><li>scriptsæ–‡ä»¶å¤¹å­˜æ”¾ç€ä¸‹è½½æ•°æ®é›†é¢shellå‘½ä»¤</li></ul>\n<p>åœ¨åˆ©ç”¨è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒæ—¶ï¼Œéœ€è¦å°†é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„è¿›è¡Œä¿®æ”¹ï¼Œæ”¹æˆè‡ªå·±å¯¹åº”çš„æ•°æ®é›†æ‰€åœ¨ç›®å½•ï¼Œæœ€å¥½å¤åˆ¶+é‡å‘½åã€‚</p>\n<pre><code>train: E:/project/yolov5/yolov5-master/dataset/images/train # train images \nval: E:/project/yolov5/yolov5-master/dataset/images/val  # val images \n</code></pre>\n<h2><a id=\"dataset_42\"></a>datasetæ–‡ä»¶å¤¹</h2>\n<p>å­˜æ”¾ç€è‡ªå·±çš„æ•°æ®é›†ï¼Œä½†åº”æŒ‰ç…§imageå’Œlabelåˆ†å¼€ï¼ŒåŒæ—¶æ¯ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹ï¼Œåˆåº”è¯¥åˆ†ä¸ºtrainï¼Œvalã€‚<br/> .cacheæ–‡ä»¶ä¸ºç¼“å­˜æ–‡ä»¶ï¼Œå°†æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œæ–¹ä¾¿ä¸‹æ¬¡è°ƒç”¨å¿«é€Ÿã€‚<br/> <img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"..\\..\\static\\image\\3fd4daee2a7146dcbffd8e84dc88c0f9.png\"/></p>\n<h2><a id=\"model_46\"></a>modelæ–‡ä»¶å¤¹</h2>\n<p><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"..\\..\\static\\image\\b877068599e44ef68533d67cece3ff21.png\"/><br/> <img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"..\\..\\static\\image\\9f728a04294446c6a38df0f00692e785.png\"/></p>\n<h3><a id=\"commonpy__51\"></a><strong>common.py ç½‘ç»œç»„ä»¶æ¨¡å—</strong></h3>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nCommon modules\n\"\"\"\n\nimport logging\nimport math\nimport warnings\nfrom copy import copy\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport requests\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom torch.cuda import amp\n\nfrom utils.datasets import exif_transpose, letterbox\nfrom utils.general import colorstr, increment_path, make_divisible, non_max_suppression, save_one_box, \\\n    scale_coords, xyxy2xywh\nfrom utils.plots import Annotator, colors\nfrom utils.torch_utils import time_sync\n\nLOGGER = logging.getLogger(__name__)\n\n#ä¸ºsameå·ç§¯æˆ–sameæ± åŒ–è‡ªåŠ¨æ‰©å……\ndef autopad(k, p=None):  # kernel, padding\n    # Pad to 'same'\n    if p is None:\n        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n    return p\n\n   #æ ‡å‡†çš„å·ç§¯ conv+BN+hardswish\nclass Conv(nn.Module):\n    # Standard convolution\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, å·ç§¯æ ¸kernel, æ­¥é•¿stride, padding, groups\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n\n    def forward(self, x):#æ­£å‘ä¼ æ’­\n        return self.act(self.bn(self.conv(x)))\n\n    def forward_fuse(self, x):\n        return self.act(self.conv(x))\n\n#æ·±åº¦å¯åˆ†ç¦»å·ç§¯ç½‘ç»œ\nclass DWConv(Conv):\n    # Depth-wise convolution class\n    def __init__(self, c1, c2, k=1, s=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), act=act)\n\n\nclass TransformerLayer(nn.Module):\n    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)\n    def __init__(self, c, num_heads):\n        super().__init__()\n        self.q = nn.Linear(c, c, bias=False)\n        self.k = nn.Linear(c, c, bias=False)\n        self.v = nn.Linear(c, c, bias=False)\n        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)\n        self.fc1 = nn.Linear(c, c, bias=False)\n        self.fc2 = nn.Linear(c, c, bias=False)\n\n    def forward(self, x):\n        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x\n        x = self.fc2(self.fc1(x)) + x\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    # Vision Transformer https://arxiv.org/abs/2010.11929\n    def __init__(self, c1, c2, num_heads, num_layers):\n        super().__init__()\n        self.conv = None\n        if c1 != c2:\n            self.conv = Conv(c1, c2)\n        self.linear = nn.Linear(c2, c2)  # learnable position embedding\n        self.tr = nn.Sequential(*[TransformerLayer(c2, num_heads) for _ in range(num_layers)])\n        self.c2 = c2\n\n    def forward(self, x):\n        if self.conv is not None:\n            x = self.conv(x)\n        b, _, w, h = x.shape\n        p = x.flatten(2).unsqueeze(0).transpose(0, 3).squeeze(3)\n        return self.tr(p + self.linear(p)).unsqueeze(3).transpose(0, 3).reshape(b, self.c2, w, h)\n\n\nclass Bottleneck(nn.Module):\n    # Standard bottleneck\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n\n\nclass BottleneckCSP(nn.Module):\n    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n        self.cv4 = Conv(2 * c_, c2, 1, 1)\n        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n        self.act = nn.LeakyReLU(0.1, inplace=True)\n        #*æŠŠlistæ‹†åˆ†ä¸ºä¸€ä¸ªä¸ªå…ƒç´ \n        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\n\n    def forward(self, x):\n        y1 = self.cv3(self.m(self.cv1(x)))\n        y2 = self.cv2(x)\n        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))\n\n\nclass C3(nn.Module):\n    # CSP Bottleneck with 3 convolutions\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c1, c_, 1, 1)\n        self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)\n        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\n        # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])\n\n    def forward(self, x):\n        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n\n\nclass C3TR(C3):\n    # C3 module with TransformerBlock()\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)\n        self.m = TransformerBlock(c_, c_, 4, n)\n\n\nclass C3SPP(C3):\n    # C3 module with SPP()\n    def __init__(self, c1, c2, k=(5, 9, 13), n=1, shortcut=True, g=1, e=0.5):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)\n        self.m = SPP(c_, c_, k)\n\n\nclass C3Ghost(C3):\n    # C3 module with GhostBottleneck()\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)  # hidden channels\n        self.m = nn.Sequential(*[GhostBottleneck(c_, c_) for _ in range(n)])\n\n#é‡‘å­—å¡”æ± åŒ–\nclass SPP(nn.Module):\n    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729\n    def __init__(self, c1, c2, k=(5, 9, 13)):\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n\n    def forward(self, x):\n        x = self.cv1(x)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning\n            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n\n\nclass SPPF(nn.Module):\n    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher\n    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n\n    def forward(self, x):\n        x = self.cv1(x)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning\n            y1 = self.m(x)\n            y2 = self.m(y1)\n            return self.cv2(torch.cat([x, y1, y2, self.m(y2)], 1))\n\n#æŠŠå®½å’Œé«˜å¡«å……æ•´åˆåˆ°cç©ºé—´ä¸­\nclass Focus(nn.Module):\n    # Focus wh information into c-space\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super().__init__()\n        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)\n        # self.contract = Contract(gain=2)\n\n    def forward(self, x):  # x(b,c,w,h) -&gt; y(b,4c,w/2,h/2)\n        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n        # return self.conv(self.contract(x))\n\n\nclass GhostConv(nn.Module):\n    # Ghost Convolution https://github.com/huawei-noah/ghostnet\n    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups\n        super().__init__()\n        c_ = c2 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, k, s, None, g, act)\n        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act)\n\n    def forward(self, x):\n        y = self.cv1(x)\n        return torch.cat([y, self.cv2(y)], 1)\n\n\nclass GhostBottleneck(nn.Module):\n    # Ghost Bottleneck https://github.com/huawei-noah/ghostnet\n    def __init__(self, c1, c2, k=3, s=1):  # ch_in, ch_out, kernel, stride\n        super().__init__()\n        c_ = c2 // 2\n        self.conv = nn.Sequential(GhostConv(c1, c_, 1, 1),  # pw\n                                  DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw\n                                  GhostConv(c_, c2, 1, 1, act=False))  # pw-linear\n        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False),\n                                      Conv(c1, c2, 1, 1, act=False)) if s == 2 else nn.Identity()\n\n    def forward(self, x):\n        return self.conv(x) + self.shortcut(x)\n\n\nclass Contract(nn.Module):\n    # Contract width-height into channels, i.e. x(1,64,80,80) to x(1,256,40,40)\n    def __init__(self, gain=2):\n        super().__init__()\n        self.gain = gain\n\n    def forward(self, x):\n        b, c, h, w = x.size()  # assert (h / s == 0) and (W / s == 0), 'Indivisible gain'\n        s = self.gain\n        x = x.view(b, c, h // s, s, w // s, s)  # x(1,64,40,2,40,2)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # x(1,2,2,64,40,40)\n        return x.view(b, c * s * s, h // s, w // s)  # x(1,256,40,40)\n\n\nclass Expand(nn.Module):\n    # Expand channels into width-height, i.e. x(1,64,80,80) to x(1,16,160,160)\n    def __init__(self, gain=2):\n        super().__init__()\n        self.gain = gain\n\n    def forward(self, x):\n        b, c, h, w = x.size()  # assert C / s ** 2 == 0, 'Indivisible gain'\n        s = self.gain\n        x = x.view(b, s, s, c // s ** 2, h, w)  # x(1,2,2,16,80,80)\n        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # x(1,16,80,2,80,2)\n        return x.view(b, c // s ** 2, h * s, w * s)  # x(1,16,160,160)\n\n\nclass Concat(nn.Module):\n    # Concatenate a list of tensors along dimension\n    def __init__(self, dimension=1):\n        super().__init__()\n        self.d = dimension\n\n    def forward(self, x):\n        return torch.cat(x, self.d)\n\n#è‡ªåŠ¨è°ƒæ•´å¤§å°\nclass AutoShape(nn.Module):\n    # YOLOv5 input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS\n    conf = 0.25  # NMS confidence threshold\n    iou = 0.45  # NMS IoU threshold\n    classes = None  # (optional list) filter by class\n    multi_label = False  # NMS multiple labels per box\n    max_det = 1000  # maximum number of detections per image\n\n    def __init__(self, model):\n        super().__init__()\n        self.model = model.eval()\n\n    def autoshape(self):\n        LOGGER.info('AutoShape already enabled, skipping... ')  # model already converted to model.autoshape()\n        return self\n\n    def _apply(self, fn):\n        # Apply to(), cpu(), cuda(), half() to model tensors that are not parameters or registered buffers\n        self = super()._apply(fn)\n        m = self.model.model[-1]  # Detect()\n        m.stride = fn(m.stride)\n        m.grid = list(map(fn, m.grid))\n        if isinstance(m.anchor_grid, list):\n            m.anchor_grid = list(map(fn, m.anchor_grid))\n        return self\n\n    @torch.no_grad()\n    def forward(self, imgs, size=640, augment=False, profile=False):\n        # Inference from various sources. For height=640, width=1280, RGB images example inputs are:\n        #   file:       imgs = 'data/images/zidane.jpg'  # str or PosixPath\n        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'\n        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)\n        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)\n        #   numpy:           = np.zeros((640,1280,3))  # HWC\n        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)\n        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images\n\n        t = [time_sync()]\n        p = next(self.model.parameters())  # for device and type\n        if isinstance(imgs, torch.Tensor):  # torch\n            with amp.autocast(enabled=p.device.type != 'cpu'):\n                return self.model(imgs.to(p.device).type_as(p), augment, profile)  # inference\n\n        # Pre-process\n        n, imgs = (len(imgs), imgs) if isinstance(imgs, list) else (1, [imgs])  # number of images, list of images\n        shape0, shape1, files = [], [], []  # image and inference shapes, filenames\n        for i, im in enumerate(imgs):\n            f = f'image{i}'  # filename\n            if isinstance(im, (str, Path)):  # filename or uri\n                im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im), im\n                im = np.asarray(exif_transpose(im))\n            elif isinstance(im, Image.Image):  # PIL Image\n                im, f = np.asarray(exif_transpose(im)), getattr(im, 'filename', f) or f\n            files.append(Path(f).with_suffix('.jpg').name)\n            if im.shape[0] &lt; 5:  # image in CHW\n                im = im.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)\n            im = im[..., :3] if im.ndim == 3 else np.tile(im[..., None], 3)  # enforce 3ch input\n            s = im.shape[:2]  # HWC\n            shape0.append(s)  # image shape\n            g = (size / max(s))  # gain\n            shape1.append([y * g for y in s])\n            imgs[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # update\n        shape1 = [make_divisible(x, int(self.stride.max())) for x in np.stack(shape1, 0).max(0)]  # inference shape\n        x = [letterbox(im, new_shape=shape1, auto=False)[0] for im in imgs]  # pad\n        x = np.stack(x, 0) if n &gt; 1 else x[0][None]  # stack\n        x = np.ascontiguousarray(x.transpose((0, 3, 1, 2)))  # BHWC to BCHW\n        x = torch.from_numpy(x).to(p.device).type_as(p) / 255.  # uint8 to fp16/32\n        t.append(time_sync())\n\n        with amp.autocast(enabled=p.device.type != 'cpu'):\n            # Inference\n            y = self.model(x, augment, profile)[0]  # forward\n            t.append(time_sync())\n\n            # Post-process\n            y = non_max_suppression(y, self.conf, iou_thres=self.iou, classes=self.classes,\n                                    multi_label=self.multi_label, max_det=self.max_det)  # NMS\n            for i in range(n):\n                scale_coords(shape1, y[i][:, :4], shape0[i])\n\n            t.append(time_sync())\n            return Detections(imgs, y, files, t, self.names, x.shape)\n\n\nclass Detections:\n    # YOLOv5 detections class for inference results\n    def __init__(self, imgs, pred, files, times=None, names=None, shape=None):\n        super().__init__()\n        d = pred[0].device  # device\n        gn = [torch.tensor([*[im.shape[i] for i in [1, 0, 1, 0]], 1., 1.], device=d) for im in imgs]  # normalizations\n        self.imgs = imgs  # list of images as numpy arrays\n        self.pred = pred  # list of tensors pred[0] = (xyxy, conf, cls)\n        self.names = names  # class names\n        self.files = files  # image filenames\n        self.xyxy = pred  # xyxy pixels\n        self.xywh = [xyxy2xywh(x) for x in pred]  # xywh pixels\n        self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy normalized\n        self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh normalized\n        self.n = len(self.pred)  # number of images (batch size)\n        self.t = tuple((times[i + 1] - times[i]) * 1000 / self.n for i in range(3))  # timestamps (ms)\n        self.s = shape  # inference BCHW shape\n\n    def display(self, pprint=False, show=False, save=False, crop=False, render=False, save_dir=Path('')):\n        crops = []\n        for i, (im, pred) in enumerate(zip(self.imgs, self.pred)):\n            s = f'image {i + 1}/{len(self.pred)}: {im.shape[0]}x{im.shape[1]} '  # string\n            if pred.shape[0]:\n                for c in pred[:, -1].unique():\n                    n = (pred[:, -1] == c).sum()  # detections per class\n                    s += f\"{n} {self.names[int(c)]}{'s' * (n &gt; 1)}, \"  # add to string\n                if show or save or render or crop:\n                    annotator = Annotator(im, example=str(self.names))\n                    for *box, conf, cls in reversed(pred):  # xyxy, confidence, class\n                        label = f'{self.names[int(cls)]} {conf:.2f}'\n                        if crop:\n                            file = save_dir / 'crops' / self.names[int(cls)] / self.files[i] if save else None\n                            crops.append({'box': box, 'conf': conf, 'cls': cls, 'label': label,\n                                          'im': save_one_box(box, im, file=file, save=save)})\n                        else:  # all others\n                            annotator.box_label(box, label, color=colors(cls))\n                    im = annotator.im\n            else:\n                s += '(no detections)'\n\n            im = Image.fromarray(im.astype(np.uint8)) if isinstance(im, np.ndarray) else im  # from np\n            if pprint:\n                LOGGER.info(s.rstrip(', '))\n            if show:\n                im.show(self.files[i])  # show\n            if save:\n                f = self.files[i]\n                im.save(save_dir / f)  # save\n                if i == self.n - 1:\n                    LOGGER.info(f\"Saved {self.n} image{'s' * (self.n &gt; 1)} to {colorstr('bold', save_dir)}\")\n            if render:\n                self.imgs[i] = np.asarray(im)\n        if crop:\n            if save:\n                LOGGER.info(f'Saved results to {save_dir}\\n')\n            return crops\n\n    def print(self):\n        self.display(pprint=True)  # print results\n        LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {tuple(self.s)}' %\n                    self.t)\n\n    def show(self):\n        self.display(show=True)  # show results\n\n    def save(self, save_dir='runs/detect/exp'):\n        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True)  # increment save_dir\n        self.display(save=True, save_dir=save_dir)  # save results\n\n    def crop(self, save=True, save_dir='runs/detect/exp'):\n        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True) if save else None\n        return self.display(crop=True, save=save, save_dir=save_dir)  # crop results\n\n    def render(self):\n        self.display(render=True)  # render results\n        return self.imgs\n\n    def pandas(self):\n        # return detections as pandas DataFrames, i.e. print(results.pandas().xyxy[0])\n        new = copy(self)  # return copy\n        ca = 'xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class', 'name'  # xyxy columns\n        cb = 'xcenter', 'ycenter', 'width', 'height', 'confidence', 'class', 'name'  # xywh columns\n        for k, c in zip(['xyxy', 'xyxyn', 'xywh', 'xywhn'], [ca, ca, cb, cb]):\n            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]  # update\n            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])\n        return new\n\n    def tolist(self):\n        # return a list of Detections objects, i.e. 'for result in results.tolist():'\n        x = [Detections([self.imgs[i]], [self.pred[i]], self.names, self.s) for i in range(self.n)]\n        for d in x:\n            for k in ['imgs', 'pred', 'xyxy', 'xyxyn', 'xywh', 'xywhn']:\n                setattr(d, k, getattr(d, k)[0])  # pop out of list\n        return x\n\n    def __len__(self):\n        return self.n\n\n#ç”¨äºäºŒçº§åˆ†ç±»\nclass Classify(nn.Module):\n    # Classification head, i.e. x(b,c1,20,20) to x(b,c2)\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1):  # ch_in, ch_out, kernel, stride, padding, groups\n        super().__init__()\n        self.aap = nn.AdaptiveAvgPool2d(1)  # to x(b,c1,1,1)\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g)  # to x(b,c2,1,1)\n        self.flat = nn.Flatten()\n\n    def forward(self, x):\n        z = torch.cat([self.aap(y) for y in (x if isinstance(x, list) else [x])], 1)  # cat if list\n        return self.flat(self.conv(z))  # flatten to x(b,c2)\n\n</code></pre>\n<h3><a id=\"experimentalpy__526\"></a><strong>experimental.py å®éªŒæ€§è´¨ä»£ç </strong></h3>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nExperimental modules\n\"\"\"\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom models.common import Conv\nfrom utils.downloads import attempt_download\n\n\nclass CrossConv(nn.Module):\n    # Cross Convolution Downsample\n    def __init__(self, c1, c2, k=3, s=1, g=1, e=1.0, shortcut=False):\n        # ch_in, ch_out, kernel, stride, groups, expansion, shortcut\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, (1, k), (1, s))\n        self.cv2 = Conv(c_, c2, (k, 1), (s, 1), g=g)\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n\n\nclass Sum(nn.Module):\n    # Weighted sum of 2 or more layers https://arxiv.org/abs/1911.09070\n    def __init__(self, n, weight=False):  # n: number of inputs\n        super().__init__()\n        self.weight = weight  # apply weights boolean\n        self.iter = range(n - 1)  # iter object\n        if weight:\n            self.w = nn.Parameter(-torch.arange(1., n) / 2, requires_grad=True)  # layer weights\n\n    def forward(self, x):\n        y = x[0]  # no weight\n        if self.weight:\n            w = torch.sigmoid(self.w) * 2\n            for i in self.iter:\n                y = y + x[i + 1] * w[i]\n        else:\n            for i in self.iter:\n                y = y + x[i + 1]\n        return y\n\n\nclass MixConv2d(nn.Module):\n    # Mixed Depth-wise Conv https://arxiv.org/abs/1907.09595\n    def __init__(self, c1, c2, k=(1, 3), s=1, equal_ch=True):\n        super().__init__()\n        groups = len(k)\n        if equal_ch:  # equal c_ per group\n            i = torch.linspace(0, groups - 1E-6, c2).floor()  # c2 indices\n            c_ = [(i == g).sum() for g in range(groups)]  # intermediate channels\n        else:  # equal weight.numel() per group\n            b = [c2] + [0] * groups\n            a = np.eye(groups + 1, groups, k=-1)\n            a -= np.roll(a, 1, axis=1)\n            a *= np.array(k) ** 2\n            a[0] = 1\n            c_ = np.linalg.lstsq(a, b, rcond=None)[0].round()  # solve for equal weight indices, ax = b\n\n        self.m = nn.ModuleList([nn.Conv2d(c1, int(c_[g]), k[g], s, k[g] // 2, bias=False) for g in range(groups)])\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = nn.LeakyReLU(0.1, inplace=True)\n\n    def forward(self, x):\n        return x + self.act(self.bn(torch.cat([m(x) for m in self.m], 1)))\n\n#æ¨¡å‹é›†æˆ\nclass Ensemble(nn.ModuleList):\n    # Ensemble of models\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, augment=False, profile=False, visualize=False):\n        y = []\n        for module in self:\n            y.append(module(x, augment, profile, visualize)[0])\n        # y = torch.stack(y).max(0)[0]  # max ensemble\n        # y = torch.stack(y).mean(0)  # mean ensemble\n        y = torch.cat(y, 1)  # nms ensemble\n        return y, None  # inference, train output\n\n#åŠ è½½æ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œå¹¶æ„é€ æ¨¡å‹\ndef attempt_load(weights, map_location=None, inplace=True, fuse=True):\n    from models.yolo import Detect, Model\n\n    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\n    model = Ensemble()\n    for w in weights if isinstance(weights, list) else [weights]:\n        ckpt = torch.load(attempt_download(w), map_location=map_location)  # loadï¼Œä¸‹è½½ä¸åœ¨æœ¬åœ°çš„æƒé‡æ–‡ä»¶\n        if fuse:\n            model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model\n        else:\n            model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().eval())  # without layer fuse\n\n\n    # Compatibility updates\n    for m in model.modules():\n        if type(m) in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU, Detect, Model]:\n            m.inplace = inplace  # pytorch 1.7.0 compatibility\n            if type(m) is Detect:\n                if not isinstance(m.anchor_grid, list):  # new Detect Layer compatibility\n                    delattr(m, 'anchor_grid')\n                    setattr(m, 'anchor_grid', [torch.zeros(1)] * m.nl)\n        elif type(m) is Conv:\n            m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatibility\n\n    if len(model) == 1:\n        return model[-1]  # return model\n    else:\n        print(f'Ensemble created with {weights}\\n')\n        for k in ['names']:\n            setattr(model, k, getattr(model[-1], k))\n        model.stride = model[torch.argmax(torch.tensor([m.stride.max() for m in model])).int()].stride  # max stride\n        return model  # return ensemble\n\n</code></pre>\n<h3><a id=\"tfpy_TensorFlow_Keras_and_TFLite_versions_of_YOLOv5_651\"></a><strong>tf.py æ¨¡å‹å¯¼å‡ºè„šæœ¬ï¼Œè´Ÿè´£å°†æ¨¡å‹è½¬åŒ–ï¼ŒTensorFlow, Keras and TFLite versions of YOLOv5</strong></h3>\n<h3><a id=\"yolopy__652\"></a><strong>yolo.py æ•´ä½“ç½‘ç»œä»£ç </strong></h3>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nYOLO-specific modules\n\nUsage:\n    $ python path/to/models/yolo.py --cfg yolov5s.yaml\n\"\"\"\n\nimport argparse\nimport sys\nfrom copy import deepcopy\nfrom pathlib import Path\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\n# ROOT = ROOT.relative_to(Path.cwd())  # relative\n\nfrom models.common import *\nfrom models.experimental import *\nfrom utils.autoanchor import check_anchor_order\nfrom utils.general import check_yaml, make_divisible, print_args, set_logging\nfrom utils.plots import feature_visualization\nfrom utils.torch_utils import copy_attr, fuse_conv_and_bn, initialize_weights, model_info, scale_img, \\\n    select_device, time_sync\n\ntry:\n    import thop  # for FLOPs computation\nexcept ImportError:\n    thop = None\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass Detect(nn.Module):\n    stride = None  # strides computed during build\n    onnx_dynamic = False  # ONNX export parameter\n\n    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer\n        super().__init__()\n        self.nc = nc  # number of classes\n        self.no = nc + 5  # number of outputs per anchor\n        self.nl = len(anchors)  # number of detection layers\n        self.na = len(anchors[0]) // 2  # number of anchors\n        self.grid = [torch.zeros(1)] * self.nl  # init grid\n        #æ¨¡å‹ä¸­ä¿å­˜çš„å‚æ•°æœ‰ä¸¤ç§ï¼šyä¸€ç§æ˜¯åå‘ä¼ æ’­éœ€è¦è¢«optimizeræ›´æ–°çš„ï¼Œç§°ä¹‹ä¸ºparameter\n        #ä¸€ç§æ˜¯åå‘ä¼ æ’­ä¸éœ€è¦è¢«optimizeræ›´æ–°ï¼Œç§°ä¸ºbuffer\n        #ç¬¬äºŒç§å‚æ•°éœ€è¦åˆ›å»ºtensorï¼Œç„¶åå°†tensoré€šè¿‡registerâ€”â€”bufferè¿›è¡Œæ³¨å†Œ\n        #å¯ä»¥é€šè¿‡modelã€‚buffersï¼ˆï¼‰è¿”å›ï¼Œæ³¨å†Œå®Œæˆåå‚æ•°ä¹Ÿä¼šè‡ªåŠ¨ä¿å­˜åˆ°orderdictä¸­å»\n        #optim.step åªèƒ½æ›´æ–°nn.parameter ç±»å‹çš„å‚æ•°\n\n        self.anchor_grid = [torch.zeros(1)] * self.nl  # init anchor grid\n        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv  1*1 å·ç§¯\n        self.inplace = inplace  # use in-place ops (e.g. slice assignment)\n\n    def forward(self, x):\n        z = []  # inference output\n        for i in range(self.nl):\n            x[i] = self.m[i](x[i])  # conv\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n            if not self.training:  # inference\n                if self.grid[i].shape[2:4] != x[i].shape[2:4] or self.onnx_dynamic:\n                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n\n                y = x[i].sigmoid()\n                if self.inplace:\n                    y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n                    xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                    y = torch.cat((xy, wh, y[..., 4:]), -1)\n                z.append(y.view(bs, -1, self.no))\n\n        return x if self.training else (torch.cat(z, 1), x)#é¢„æµ‹æ¡†åæ ‡ä¿¡æ¯\n\n    def _make_grid(self, nx=20, ny=20, i=0):\n        d = self.anchors[i].device\n        yv, xv = torch.meshgrid([torch.arange(ny).to(d), torch.arange(nx).to(d)])\n        grid = torch.stack((xv, yv), 2).expand((1, self.na, ny, nx, 2)).float()\n        anchor_grid = (self.anchors[i].clone() * self.stride[i]) \\\n            .view((1, self.na, 1, 1, 2)).expand((1, self.na, ny, nx, 2)).float()\n        return grid, anchor_grid\n\n\n#ç½‘ç»œæ¨¡å‹ç±»\nclass Model(nn.Module):\n    def __init__(self, cfg='yolov5s.yaml', ch=3, nc=None, anchors=None):  # model, input channels, number of classes\n        super().__init__()\n        if isinstance(cfg, dict):\n            self.yaml = cfg  # model dict\n        else:  # is *.yaml\n            import yaml  # for torch hub\n            self.yaml_file = Path(cfg).name\n            with open(cfg, errors='ignore') as f:\n                self.yaml = yaml.safe_load(f)  # model dict\n\n        # Define model\n        ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels\n        if nc and nc != self.yaml['nc']:\n            LOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\n            self.yaml['nc'] = nc  # override yaml value\n        if anchors:\n            LOGGER.info(f'Overriding model.yaml anchors with anchors={anchors}')\n            self.yaml['anchors'] = round(anchors)  # override yaml value\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist\n        self.names = [str(i) for i in range(self.yaml['nc'])]  # default names\n        self.inplace = self.yaml.get('inplace', True)\n\n        # Build strides, anchors\n        m = self.model[-1]  # Detect()\n        if isinstance(m, Detect):\n            s = 256  # 2x min stride\n            m.inplace = self.inplace\n            m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\n            m.anchors /= m.stride.view(-1, 1, 1)\n            check_anchor_order(m)\n            self.stride = m.stride\n            self._initialize_biases()  # only run onceï¼Œåˆå§‹åŒ–åæ‰§\n\n        # Init weights, biases\n        initialize_weights(self)#åˆå§‹åŒ–æƒé‡\n        self.info()#æ¯ä¸€å±‚çš„ä¿¡æ¯\n        LOGGER.info('')\n\n    def forward(self, x, augment=False, profile=False, visualize=False):\n        if augment:\n            return self._forward_augment(x)  # augmented inference, None\n        return self._forward_once(x, profile, visualize)  # single-scale inference, train\n\n    def _forward_augment(self, x):\n        img_size = x.shape[-2:]  # height, width\n        s = [1, 0.83, 0.67]  # scales\n        f = [None, 3, None]  # flips (2-ud, 3-lr)\n        y = []  # outputs\n        for si, fi in zip(s, f):\n            xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))\n            yi = self._forward_once(xi)[0]  # forward\n            # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save\n            yi = self._descale_pred(yi, fi, si, img_size)\n            y.append(yi)\n        y = self._clip_augmented(y)  # clip augmented tails\n        return torch.cat(y, 1), None  # augmented inference, train\n\n    def _forward_once(self, x, profile=False, visualize=False):\n        y, dt = [], []  # outputs\n        for m in self.model:\n            if m.f != -1:  # if not from previous layer\n                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n            if profile:\n                self._profile_one_layer(m, x, dt)\n            x = m(x)  # run\n            y.append(x if m.i in self.save else None)  # save output\n            if visualize:\n                feature_visualization(x, m.type, m.i, save_dir=visualize)\n        return x\n\n    def _descale_pred(self, p, flips, scale, img_size):\n        # de-scale predictions following augmented inference (inverse operation)\n        if self.inplace:\n            p[..., :4] /= scale  # de-scale\n            if flips == 2:\n                p[..., 1] = img_size[0] - p[..., 1]  # de-flip ud\n            elif flips == 3:\n                p[..., 0] = img_size[1] - p[..., 0]  # de-flip lr\n        else:\n            x, y, wh = p[..., 0:1] / scale, p[..., 1:2] / scale, p[..., 2:4] / scale  # de-scale\n            if flips == 2:\n                y = img_size[0] - y  # de-flip ud\n            elif flips == 3:\n                x = img_size[1] - x  # de-flip lr\n            p = torch.cat((x, y, wh, p[..., 4:]), -1)\n        return p\n\n    def _clip_augmented(self, y):\n        # Clip YOLOv5 augmented inference tails\n        nl = self.model[-1].nl  # number of detection layers (P3-P5)\n        g = sum(4 ** x for x in range(nl))  # grid points\n        e = 1  # exclude layer count\n        i = (y[0].shape[1] // g) * sum(4 ** x for x in range(e))  # indices\n        y[0] = y[0][:, :-i]  # large\n        i = (y[-1].shape[1] // g) * sum(4 ** (nl - 1 - x) for x in range(e))  # indices\n        y[-1] = y[-1][:, i:]  # small\n        return y\n\n    def _profile_one_layer(self, m, x, dt):\n        c = isinstance(m, Detect)  # is final layer, copy input as inplace fix\n        o = thop.profile(m, inputs=(x.copy() if c else x,), verbose=False)[0] / 1E9 * 2 if thop else 0  # FLOPs\n        t = time_sync()\n        for _ in range(10):\n            m(x.copy() if c else x)\n        dt.append((time_sync() - t) * 100)\n        if m == self.model[0]:\n            LOGGER.info(f\"{'time (ms)':&gt;10s} {'GFLOPs':&gt;10s} {'params':&gt;10s}  {'module'}\")\n        LOGGER.info(f'{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}')\n        if c:\n            LOGGER.info(f\"{sum(dt):10.2f} {'-':&gt;10s} {'-':&gt;10s}  Total\")\n\n    def _initialize_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency\n        # https://arxiv.org/abs/1708.02002 section 3.3\n        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.\n        m = self.model[-1]  # Detect() module\n        for mi, s in zip(m.m, m.stride):  # from\n            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)\n            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\n            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\n            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\n\n    def _print_biases(self):\n        m = self.model[-1]  # Detect() module\n        for mi in m.m:  # from\n            b = mi.bias.detach().view(m.na, -1).T  # conv.bias(255) to (3,85)\n            LOGGER.info(\n                ('%6g Conv2d.bias:' + '%10.3g' * 6) % (mi.weight.shape[1], *b[:5].mean(1).tolist(), b[5:].mean()))\n\n    # def _print_weights(self):\n    #     for m in self.model.modules():\n    #         if type(m) is Bottleneck:\n    #             LOGGER.info('%10.3g' % (m.w.detach().sigmoid() * 2))  # shortcut weights\n\n    def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers\n        LOGGER.info('Fusing layers... ')\n        for m in self.model.modules():\n            if isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):\n                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\n                delattr(m, 'bn')  # remove batchnorm\n                m.forward = m.forward_fuse  # update forward\n        self.info()\n        return self\n\n    def autoshape(self):  # add AutoShape module\n        LOGGER.info('Adding AutoShape... ')\n        m = AutoShape(self)  # wrap model\n        copy_attr(m, self, include=('yaml', 'nc', 'hyp', 'names', 'stride'), exclude=())  # copy attributes\n        return m\n\n    def info(self, verbose=False, img_size=640):  # print model information\n        model_info(self, verbose, img_size)\n\n    def _apply(self, fn):\n        # Apply to(), cpu(), cuda(), half() to model tensors that are not parameters or registered buffers\n        self = super()._apply(fn)\n        m = self.model[-1]  # Detect()\n        if isinstance(m, Detect):\n            m.stride = fn(m.stride)\n            m.grid = list(map(fn, m.grid))\n            if isinstance(m.anchor_grid, list):\n                m.anchor_grid = list(map(fn, m.anchor_grid))\n        return self\n\n#è§£æç½‘ç»œæ¨¡å‹é…ç½®æ–‡ä»¶ï¼Œå¹¶æ„å»ºæ¨¡å‹\ndef parse_model(d, ch):  # model_dict, input_channels(3)\n    LOGGER.info('\\n%3s%18s%3s%10s  %-40s%-30s' % ('', 'from', 'n', 'params', 'module', 'arguments'))\n    #å°†æ¨¡å‹ç»“æ„çš„depth_multiple,width_multipleæå–å‡ºæ¥ï¼Œèµ‹å€¼ç»™gd,gw\n    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\n    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\n    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n\n    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n        m = eval(m) if isinstance(m, str) else m  # eval strings\n        for j, a in enumerate(args):\n            try:\n                args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n            except NameError:\n                pass\n        # æ§åˆ¶æ·±åº¦\n        n = n_ = max(round(n * gd), 1) if n &gt; 1 else n  # depth gain\n        if m in [Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, MixConv2d, Focus, CrossConv,\n                 BottleneckCSP, C3, C3TR, C3SPP, C3Ghost]:\n            c1, c2 = ch[f], args[0]  #c1:3\n\n            if c2 != no:  # if not output\n                #æ§åˆ¶å®½åº¦ï¼ˆå·ç§¯æ ¸ä¸ªæ•°ï¼‰\n                c2 = make_divisible(c2 * gw, 8)\n\n            args = [c1, c2, *args[1:]]\n            if m in [BottleneckCSP, C3, C3TR, C3Ghost]:\n                args.insert(2, n)  # number of repeats\n                n = 1\n        elif m is nn.BatchNorm2d:\n            args = [ch[f]]\n        elif m is Concat:\n            c2 = sum([ch[x] for x in f])\n        elif m is Detect:\n            args.append([ch[x] for x in f])\n            if isinstance(args[1], int):  # number of anchors\n                args[1] = [list(range(args[1] * 2))] * len(f)\n        elif m is Contract:\n            c2 = ch[f] * args[0] ** 2\n        elif m is Expand:\n            c2 = ch[f] // args[0] ** 2\n        else:\n            c2 = ch[f]\n        #*argè¡¨ç¤ºä¼šæ¥æ”¶ä»»æ„æ•°é‡ä¸ªå‚æ•°ï¼Œè°ƒç”¨æ—¶æ‰“åŒ…ä¸ºä¸€ä¸ªå…ƒç»„ä¼ å…¥å®å‚\n        m_ = nn.Sequential(*[m(*args) for _ in range(n)]) if n &gt; 1 else m(*args)  # module\n        t = str(m)[8:-2].replace('__main__.', '')  # module type\n        np = sum([x.numel() for x in m_.parameters()])  # number params\n        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\n        LOGGER.info('%3s%18s%3s%10.0f  %-40s%-30s' % (i, f, n_, np, t, args))  # print\n        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n        layers.append(m_)\n        if i == 0:\n            ch = []\n        ch.append(c2)\n    return nn.Sequential(*layers), sorted(save)\n\n\nif __name__ == '__main__':\n    #å»ºç«‹å‚æ•°è§£æå¯¹è±¡\n    parser = argparse.ArgumentParser()\n    #æ·»åŠ å±æ€§\n    parser.add_argument('--cfg', type=str, default='yolov5s.yaml', help='model.yaml')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--profile', action='store_true', help='profile model speed')\n    opt = parser.parse_args()\n    opt.cfg = check_yaml(opt.cfg)  # check YAML\n    print_args(FILE.stem, opt)\n    set_logging()\n    device = select_device(opt.device)\n\n    # Create model\n    model = Model(opt.cfg).to(device)\n    model.train()\n\n    # Profile\n    if opt.profile:\n        img = torch.rand(8 if torch.cuda.is_available() else 1, 3, 640, 640).to(device)\n        y = model(img, profile=True)\n\n    # Tensorboard (not working https://github.com/ultralytics/yolov5/issues/2898)\n    # from torch.utils.tensorboard import SummaryWriter\n    # tb_writer = SummaryWriter('.')\n    # LOGGER.info(\"Run 'tensorboard --logdir=models' to view tensorboard at http://localhost:6006/\")\n    # tb_writer.add_graph(torch.jit.trace(model, img, strict=False), [])  # add model graph\n\n</code></pre>\n<h3><a id=\"yolo5syaml__997\"></a><strong>yolo5s.yaml ç½‘ç»œæ¨¡å‹é…ç½®æ–‡ä»¶</strong></h3>\n<p><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"..\\..\\static\\image\\e6db4e7b16f04b169f9405aa9a7bff63.png\"/></p>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\n# Parameters\nnc: 80  # number of classes  ç±»åˆ«æ•°\ndepth_multiple: 0.33  # model depth multiple æ§åˆ¶æ¨¡å‹çš„æ·±åº¦\nwidth_multiple: 0.50  # layer channel multiple æ§åˆ¶conv é€šé“çš„ä¸ªæ•°ï¼Œå·ç§¯æ ¸æ•°é‡\n#depth_multiple: è¡¨ç¤ºBottleneckCSPæ¨¡å—çš„å±‚ç¼©æ”¾å› å­ï¼Œå°†æ‰€æœ‰çš„BotleneckCSPæ¨¡å—çš„B0ttleneckä¹˜ä¸Šè¯¥å‚æ•°å¾—åˆ°æœ€ç»ˆä¸ªæ•°\n#width_multipleè¡¨ç¤ºå·ç§¯é€šé“çš„ç¼©æ”¾å› å­ï¼Œå°±æ˜¯å°†é…ç½®é‡Œçš„backboneå’Œheadéƒ¨åˆ†æœ‰å…³convé€šé“çš„è®¾ç½®ï¼Œå…¨éƒ¨ä¹˜ä»¥è¯¥ç³»æ•°\nanchors:\n  - [10,13, 16,30, 33,23]  # P3/8\n  - [30,61, 62,45, 59,119]  # P4/16\n  - [116,90, 156,198, 373,326]  # P5/32\n\n# YOLOv5 v6.0 backbone\nbackbone:\n  # [from, number, module, args]\n  #from åˆ—å‚æ•° ï¼šå½“å‰æ¨¡å—è¾“å…¥æ¥è‡ªé‚£ä¸€å±‚è¾“å‡ºï¼Œ-1 è¡¨ç¤ºæ˜¯ä»ä¸Šä¸€å±‚è·å¾—çš„è¾“å…¥\n  #number åˆ—å‚æ•°ï¼šæœ¬æ¨¡å—é‡å¤æ¬¡æ•°ï¼Œ1 è¡¨ç¤ºåªæœ‰ä¸€ä¸ªï¼Œ3 è¡¨ç¤ºæœ‰ä¸‰ä¸ªç›¸åŒçš„æ¨¡å—\n  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4 128è¡¨ç¤º128ä¸ªå·ç§¯æ ¸ï¼Œ3 è¡¨ç¤º3*3å·ç§¯æ ¸ï¼Œ2è¡¨ç¤ºæ­¥é•¿ä¸º2\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 6, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 3, C3, [1024]],\n   [-1, 1, SPPF, [1024, 5]],  # 9\n  ]\n\n# YOLOv5 v6.0 head\n#ä½œè€…æ²¡æœ‰åˆ†neckæ¨¡å—ï¼Œæ‰€ä»¥headéƒ¨åˆ†åŒ…å«äº†panet+detectéƒ¨åˆ†\nhead:\n  [[-1, 1, Conv, [512, 1, 1]], #ä¸Šé‡‡æ ·\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, C3, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n\n   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n  ]\n</code></pre>\n<h2><a id=\"run__1056\"></a>run æ–‡ä»¶å¤¹</h2>\n<p><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"..\\..\\static\\image\\540f3e7fbf854992b903939bdf1f8b9c.png\"/><br/> trainæ–‡ä»¶å¤¹å­˜æ”¾ç€è®­ç»ƒæ•°æ®æ—¶è®°å½•çš„æ•°æ®è¿‡ç¨‹<br/> detectæ–‡ä»¶å¤¹å­˜æ”¾ç€ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ¯æ¬¡é¢„æµ‹åˆ¤æ–­çš„æ•°æ®</p>\n<h2><a id=\"utils_1061\"></a>utilsæ–‡ä»¶å¤¹</h2>\n<h3><a id=\"_1062\"></a>ç›®æ ‡æ£€æµ‹æ€§èƒ½æŒ‡æ ‡</h3>\n<h4><a id=\"_1063\"></a>æ£€æµ‹ç²¾åº¦</h4>\n<ul><li>precisionï¼Œrecallï¼Œf1 score</li><li>iouï¼ˆintersection over unionï¼‰äº¤å¹¶æ¯”</li><li>P-R curve (precision-recall curve)</li><li>AP (average precison)</li><li>mAP (mean ap)</li></ul>\n<h4><a id=\"_1069\"></a>æ£€æµ‹é€Ÿåº¦</h4>\n<ul><li>å‰ä¼ è€—æ—¶</li><li>æ¯ç§’å¸§æ•°FPS</li><li>æµ®ç‚¹è¿ç®—é‡ FLOPS</li></ul>\n<h3><a id=\"activationpy___1075\"></a>activation.py æ¿€æ´»å‡½æ•°ç›¸å…³ä»£ç </h3>\n<h3><a id=\"augmentationspy___1076\"></a>augmentations.py å›¾ç‰‡æ‰©å±•å˜æ¢ç›¸å…³å‡½æ•°</h3>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nImage augmentation functions\n\"\"\"\n\nimport logging\nimport math\nimport random\n\nimport cv2\nimport numpy as np\n\nfrom utils.general import colorstr, segment2box, resample_segments, check_version\nfrom utils.metrics import bbox_ioa\n\n\nclass Albumentations:\n    # YOLOv5 Albumentations class (optional, only used if package is installed)\n    def __init__(self):\n        self.transform = None\n        try:\n            import albumentations as A\n            check_version(A.__version__, '1.0.3')  # version requirement\n\n            self.transform = A.Compose([\n                A.Blur(p=0.01),\n                A.MedianBlur(p=0.01),\n                A.ToGray(p=0.01),\n                A.CLAHE(p=0.01),\n                A.RandomBrightnessContrast(p=0.0),\n                A.RandomGamma(p=0.0),\n                A.ImageCompression(quality_lower=75, p=0.0)],\n                bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n\n            logging.info(colorstr('albumentations: ') + ', '.join(f'{x}' for x in self.transform.transforms if x.p))\n        except ImportError:  # package not installed, skip\n            pass\n        except Exception as e:\n            logging.info(colorstr('albumentations: ') + f'{e}')\n\n    def __call__(self, im, labels, p=1.0):\n        if self.transform and random.random() &lt; p:\n            new = self.transform(image=im, bboxes=labels[:, 1:], class_labels=labels[:, 0])  # transformed\n            im, labels = new['image'], np.array([[c, *b] for c, b in zip(new['class_labels'], new['bboxes'])])\n        return im, labels\n\n\ndef augment_hsv(im, hgain=0.5, sgain=0.5, vgain=0.5):\n    # HSV color-space augmentation\n    if hgain or sgain or vgain:\n        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n        hue, sat, val = cv2.split(cv2.cvtColor(im, cv2.COLOR_BGR2HSV))\n        dtype = im.dtype  # uint8\n\n        x = np.arange(0, 256, dtype=r.dtype)\n        lut_hue = ((x * r[0]) % 180).astype(dtype)\n        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n\n        im_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n        cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=im)  # no return needed\n\n\ndef hist_equalize(im, clahe=True, bgr=False):\n    # Equalize histogram on BGR image 'im' with im.shape(n,m,3) and range 0-255\n    yuv = cv2.cvtColor(im, cv2.COLOR_BGR2YUV if bgr else cv2.COLOR_RGB2YUV)\n    if clahe:\n        c = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        yuv[:, :, 0] = c.apply(yuv[:, :, 0])\n    else:\n        yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])  # equalize Y channel histogram\n    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR if bgr else cv2.COLOR_YUV2RGB)  # convert YUV image to RGB\n\n\ndef replicate(im, labels):\n    # Replicate labels\n    h, w = im.shape[:2]\n    boxes = labels[:, 1:].astype(int)\n    x1, y1, x2, y2 = boxes.T\n    s = ((x2 - x1) + (y2 - y1)) / 2  # side length (pixels)\n    for i in s.argsort()[:round(s.size * 0.5)]:  # smallest indices\n        x1b, y1b, x2b, y2b = boxes[i]\n        bh, bw = y2b - y1b, x2b - x1b\n        yc, xc = int(random.uniform(0, h - bh)), int(random.uniform(0, w - bw))  # offset x, y\n        x1a, y1a, x2a, y2a = [xc, yc, xc + bw, yc + bh]\n        im[y1a:y2a, x1a:x2a] = im[y1b:y2b, x1b:x2b]  # im4[ymin:ymax, xmin:xmax]\n        labels = np.append(labels, [[labels[i, 0], x1a, y1a, x2a, y2a]], axis=0)\n\n    return im, labels\n\n#ç¼©æ”¾å›¾ç‰‡ï¼Œresizeä¿æŒå›¾ç‰‡çš„å®½é«˜æ¯”ï¼Œå‰©ä¸‹çš„éƒ¨åˆ†æœ‰ç°è‰²å¡«å……\ndef letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n    # Resize and pad image while meeting stride-multiple constraints\n    shape = im.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n\n    # Scale ratio (new / old)\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    #ç¼©æ”¾åˆ°è¾“å…¥å¤§å°img_sizeçš„æ—¶å€™ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®ä¸Šé‡‡æ ·ï¼Œåˆ™åªèƒ½ä¸‹é‡‡æ ·\n    #ä¸Šé‡‡ç”¨ä¼šä½¿å›¾ç‰‡å˜å¾—æ¨¡ç³Šï¼Œå½±å“æ€§èƒ½\n    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    ratio = r, r  # width, height ratios\n    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n    if auto:  # minimum rectangle è·å–æœ€å°çš„çŸ©å½¢å¡«å……\n        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n    #scaleFillä¸ºTrueåˆ™ä¸è¿›è¡Œå¡«å……\n    elif scaleFill:  # stretch\n        dw, dh = 0.0, 0.0\n        new_unpad = (new_shape[1], new_shape[0])\n        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n    #è®¡ç®—ä¸Šä¸‹å·¦å³å¡«å……å¤§å°\n    dw /= 2  # divide padding into 2 sides\n    dh /= 2\n\n    if shape[::-1] != new_unpad:  # resize\n        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n    return im, ratio, (dw, dh)\n\n#éšæœºé€è§†å˜æ¢\n#è®¡ç®—æ–¹æ³•ä¸ºåæ ‡å‘é‡å’Œå˜æ¢çŸ©é˜µçš„ä¹˜ç§¯\ndef random_perspective(im, targets=(), segments=(), degrees=10, translate=.1, scale=.1, shear=10, perspective=0.0,\n                       border=(0, 0)):\n    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))\n    # targets = [cls, xyxy]\n\n    height = im.shape[0] + border[0] * 2  # shape(h,w,c)\n    width = im.shape[1] + border[1] * 2\n\n    # Center\n    C = np.eye(3)\n    C[0, 2] = -im.shape[1] / 2  # x translation (pixels)\n    C[1, 2] = -im.shape[0] / 2  # y translation (pixels)\n\n    # Perspective\n    P = np.eye(3)\n    P[2, 0] = random.uniform(-perspective, perspective)  # x perspective (about y)\n    P[2, 1] = random.uniform(-perspective, perspective)  # y perspective (about x)\n\n    # Rotation and Scale\n    R = np.eye(3)\n    a = random.uniform(-degrees, degrees)\n    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n    s = random.uniform(1 - scale, 1 + scale)\n    # s = 2 ** random.uniform(-scale, scale)\n    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n\n    # Shear\n    S = np.eye(3)\n    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n\n    # Translation\n    T = np.eye(3)\n    T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width  # x translation (pixels)\n    T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height  # y translation (pixels)\n\n    # Combined rotation matrix\n    M = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\n    if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\n        if perspective:\n            im = cv2.warpPerspective(im, M, dsize=(width, height), borderValue=(114, 114, 114))\n        else:  # affine\n            im = cv2.warpAffine(im, M[:2], dsize=(width, height), borderValue=(114, 114, 114))\n\n    # Visualize\n    # import matplotlib.pyplot as plt\n    # ax = plt.subplots(1, 2, figsize=(12, 6))[1].ravel()\n    # ax[0].imshow(im[:, :, ::-1])  # base\n    # ax[1].imshow(im2[:, :, ::-1])  # warped\n\n    # Transform label coordinates\n    n = len(targets)\n    if n:\n        use_segments = any(x.any() for x in segments)\n        new = np.zeros((n, 4))\n        if use_segments:  # warp segments\n            segments = resample_segments(segments)  # upsample\n            for i, segment in enumerate(segments):\n                xy = np.ones((len(segment), 3))\n                xy[:, :2] = segment\n                xy = xy @ M.T  # transform\n                xy = xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]  # perspective rescale or affine\n\n                # clip\n                new[i] = segment2box(xy, width, height)\n\n        else:  # warp boxes\n            xy = np.ones((n * 4, 3))\n            xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n            xy = xy @ M.T  # transform\n            xy = (xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine\n\n            # create new boxes\n            x = xy[:, [0, 2, 4, 6]]\n            y = xy[:, [1, 3, 5, 7]]\n            new = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n\n            # clip\n            new[:, [0, 2]] = new[:, [0, 2]].clip(0, width)\n            new[:, [1, 3]] = new[:, [1, 3]].clip(0, height)\n\n        # filter candidates\n        i = box_candidates(box1=targets[:, 1:5].T * s, box2=new.T, area_thr=0.01 if use_segments else 0.10)\n        targets = targets[i]\n        targets[:, 1:5] = new[i]\n\n    return im, targets\n\n\ndef copy_paste(im, labels, segments, p=0.5):\n    # Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy)\n    n = len(segments)\n    if p and n:\n        h, w, c = im.shape  # height, width, channels\n        im_new = np.zeros(im.shape, np.uint8)\n        for j in random.sample(range(n), k=round(p * n)):\n            l, s = labels[j], segments[j]\n            box = w - l[3], l[2], w - l[1], l[4]\n            ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area\n            if (ioa &lt; 0.30).all():  # allow 30% obscuration of existing labels\n                labels = np.concatenate((labels, [[l[0], *box]]), 0)\n                segments.append(np.concatenate((w - s[:, 0:1], s[:, 1:2]), 1))\n                cv2.drawContours(im_new, [segments[j].astype(np.int32)], -1, (255, 255, 255), cv2.FILLED)\n\n        result = cv2.bitwise_and(src1=im, src2=im_new)\n        result = cv2.flip(result, 1)  # augment segments (flip left-right)\n        i = result &gt; 0  # pixels to replace\n        # i[:, :] = result.max(2).reshape(h, w, 1)  # act over ch\n        im[i] = result[i]  # cv2.imwrite('debug.jpg', im)  # debug\n\n    return im, labels, segments\n\n#cutoutæ•°æ®å¢å¼ºï¼Œå¡«å……éšæœºé¢œè‰²\ndef cutout(im, labels, p=0.5):\n    # Applies image cutout augmentation https://arxiv.org/abs/1708.04552\n    if random.random() &lt; p:\n        h, w = im.shape[:2]\n        scales = [0.5] * 1 + [0.25] * 2 + [0.125] * 4 + [0.0625] * 8 + [0.03125] * 16  # image size fraction\n        for s in scales:\n            mask_h = random.randint(1, int(h * s))  # create random masks\n            mask_w = random.randint(1, int(w * s))\n\n            # box\n            xmin = max(0, random.randint(0, w) - mask_w // 2)\n            ymin = max(0, random.randint(0, h) - mask_h // 2)\n            xmax = min(w, xmin + mask_w)\n            ymax = min(h, ymin + mask_h)\n\n            # apply random color mask\n            im[ymin:ymax, xmin:xmax] = [random.randint(64, 191) for _ in range(3)]\n\n            # return unobscured labels\n            if len(labels) and s &gt; 0.03:\n                box = np.array([xmin, ymin, xmax, ymax], dtype=np.float32)\n                ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area\n                labels = labels[ioa &lt; 0.60]  # remove &gt;60% obscured labels\n\n    return labels\n\n\ndef mixup(im, labels, im2, labels2):\n    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n    r = np.random.beta(32.0, 32.0)  # mixup ratio, alpha=beta=32.0\n    im = (im * r + im2 * (1 - r)).astype(np.uint8)\n    labels = np.concatenate((labels, labels2), 0)\n    return im, labels\n\n#æŒ‘é€‰åˆé€‚çš„ç›®æ ‡æ¡†é€å…¥è®­ç»ƒï¼Œé˜ˆå€¼\ndef box_candidates(box1, box2, wh_thr=2, ar_thr=20, area_thr=0.1, eps=1e-16):  # box1(4,n), box2(4,n)\n    # Compute candidate boxes: box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio\n    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n    ar = np.maximum(w2 / (h2 + eps), h2 / (w2 + eps))  # aspect ratio\n    return (w2 &gt; wh_thr) &amp; (h2 &gt; wh_thr) &amp; (w2 * h2 / (w1 * h1 + eps) &gt; area_thr) &amp; (ar &lt; ar_thr)  # candidates\n\n</code></pre>\n<h3><a id=\"autoanchorpy___1364\"></a><strong>autoanchor.py è‡ªåŠ¨æè¾¹ç›¸å…³å‡½æ•°</strong></h3>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nAuto-anchor utils\n\"\"\"\n\nimport random\n\nimport numpy as np\nimport torch\nimport yaml\nfrom tqdm import tqdm\n\nfrom utils.general import colorstr\n\n#æ£€æŸ¥anchoré¡ºåºå’Œstrideçš„é¡ºåºæ˜¯å¦ä¸€è‡´\ndef check_anchor_order(m):\n    # Check anchor order against stride order for YOLOv5 Detect() module m, and correct if necessary\n    a = m.anchors.prod(-1).view(-1)  # anchor area\n    da = a[-1] - a[0]  # delta a\n    ds = m.stride[-1] - m.stride[0]  # delta s\n    if da.sign() != ds.sign():  # same order\n        print('Reversing anchor order')\n        m.anchors[:] = m.anchors.flip(0)\n\n#è‡ªåŠ¨ç»˜åˆ¶å¤–è¾¹æ¡†ï¼Œææ¡†\ndef check_anchors(dataset, model, thr=4.0, imgsz=640):\n    # Check anchor fit to data, recompute if necessary\n    prefix = colorstr('autoanchor: ')\n    print(f'\\n{prefix}Analyzing anchors... ', end='')\n    m = model.module.model[-1] if hasattr(model, 'module') else model.model[-1]  # Detect()\n    shapes = imgsz * dataset.shapes / dataset.shapes.max(1, keepdims=True)\n    scale = np.random.uniform(0.9, 1.1, size=(shapes.shape[0], 1))  # augment scale\n    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(shapes * scale, dataset.labels)])).float()  # wh\n\n    def metric(k):  # compute metric\n        r = wh[:, None] / k[None]\n        x = torch.min(r, 1. / r).min(2)[0]  # ratio metric  wh æ–¹å‘æ‰¾æœ€å°å€¼\n        best = x.max(1)[0]  # best_x  nx9ä¸ªanchorä¸­æ‰¾æœ€å¤§å€¼\n        aat = (x &gt; 1. / thr).float().sum(1).mean()  # anchors above threshold\n        bpr = (best &gt; 1. / thr).float().mean()  # best possible recall\n        return bpr, aat\n\n    anchors = m.anchors.clone() * m.stride.to(m.anchors.device).view(-1, 1, 1)  # current anchors\n    bpr, aat = metric(anchors.cpu().view(-1, 2))\n    print(f'anchors/target = {aat:.2f}, Best Possible Recall (BPR) = {bpr:.4f}', end='')\n    if bpr &lt; 0.98:  # threshold to recomputeï¼Œå¦‚æœbprå°äº98ï¼Œåˆ™æ ¹æ®k-meanç®—æ³•èšç±»æ–°çš„ææ¡†\n        print('. Attempting to improve anchors, please wait...')\n        na = m.anchors.numel() // 2  # number of anchors\n        try:\n            anchors = kmean_anchors(dataset, n=na, img_size=imgsz, thr=thr, gen=1000, verbose=False)\n        except Exception as e:\n            print(f'{prefix}ERROR: {e}')\n        new_bpr = metric(anchors)[0]\n        if new_bpr &gt; bpr:  # replace anchors\n            anchors = torch.tensor(anchors, device=m.anchors.device).type_as(m.anchors)\n            m.anchors[:] = anchors.clone().view_as(m.anchors) / m.stride.to(m.anchors.device).view(-1, 1, 1)  # loss\n            check_anchor_order(m)\n            print(f'{prefix}New anchors saved to model. Update model *.yaml to use these anchors in the future.')\n        else:\n            print(f'{prefix}Original anchors better than new anchors. Proceeding with original anchors.')\n    print('')  # newline\n\n#anchoråškmeanå¤„ç†ï¼Œç”¨åˆ°äº†è¿›åŒ–ç®—æ³•\ndef kmean_anchors(dataset='./data/coco128.yaml', n=9, img_size=640, thr=4.0, gen=1000, verbose=True):\n    \"\"\" Creates kmeans-evolved anchors from training dataset\n\n        Arguments:\n            dataset: path to data.yaml, or a loaded dataset\n            n: number of anchors\n            img_size: image size used for training\n            thr: anchor-label wh ratio threshold hyperparameter hyp['anchor_t'] used for training, default=4.0\n            gen: generations to evolve anchors using genetic algorithm\n            verbose: print all results\n\n        Return:\n            k: kmeans evolved anchors\n\n        Usage:\n            from utils.autoanchor import *; _ = kmean_anchors()\n    \"\"\"\n    from scipy.cluster.vq import kmeans\n\n    thr = 1. / thr\n    prefix = colorstr('autoanchor: ')\n\n    def metric(k, wh):  # compute metrics\n        r = wh[:, None] / k[None]\n        x = torch.min(r, 1. / r).min(2)[0]  # ratio metric\n        # x = wh_iou(wh, torch.tensor(k))  # iou metric\n        return x, x.max(1)[0]  # x, best_x\n\n    def anchor_fitness(k):  # mutation fitness\n        _, best = metric(torch.tensor(k, dtype=torch.float32), wh)\n        return (best * (best &gt; thr).float()).mean()  # fitness\n\n    def print_results(k):\n        k = k[np.argsort(k.prod(1))]  # sort small to large\n        x, best = metric(k, wh0)\n        bpr, aat = (best &gt; thr).float().mean(), (x &gt; thr).float().mean() * n  # best possible recall, anch &gt; thr\n        print(f'{prefix}thr={thr:.2f}: {bpr:.4f} best possible recall, {aat:.2f} anchors past thr')\n        print(f'{prefix}n={n}, img_size={img_size}, metric_all={x.mean():.3f}/{best.mean():.3f}-mean/best, '\n              f'past_thr={x[x &gt; thr].mean():.3f}-mean: ', end='')\n        for i, x in enumerate(k):\n            print('%i,%i' % (round(x[0]), round(x[1])), end=',  ' if i &lt; len(k) - 1 else '\\n')  # use in *.cfg\n        return k\n\n    if isinstance(dataset, str):  # *.yaml file\n        with open(dataset, errors='ignore') as f:\n            data_dict = yaml.safe_load(f)  # model dict\n        from utils.datasets import LoadImagesAndLabels\n        dataset = LoadImagesAndLabels(data_dict['train'], augment=True, rect=True)\n\n    # Get label wh\n    shapes = img_size * dataset.shapes / dataset.shapes.max(1, keepdims=True)\n    wh0 = np.concatenate([l[:, 3:5] * s for s, l in zip(shapes, dataset.labels)])  # wh\n\n    # Filter\n    i = (wh0 &lt; 3.0).any(1).sum()\n    if i:\n        print(f'{prefix}WARNING: Extremely small objects found. {i} of {len(wh0)} labels are &lt; 3 pixels in size.')\n    wh = wh0[(wh0 &gt;= 2.0).any(1)]  # filter &gt; 2 pixels\n    # wh = wh * (np.random.rand(wh.shape[0], 1) * 0.9 + 0.1)  # multiply by random scale 0-1\n\n    # Kmeans calculation\n    print(f'{prefix}Running kmeans for {n} anchors on {len(wh)} points...')\n    s = wh.std(0)  # sigmas for whitening\n    k, dist = kmeans(wh / s, n, iter=30)  # points, mean distanceï¼Œä½¿ç”¨scipyåŒ…å«çš„kmeanså‡½æ•°\n    assert len(k) == n, f'{prefix}ERROR: scipy.cluster.vq.kmeans requested {n} points but returned only {len(k)}'\n    k *= s\n    wh = torch.tensor(wh, dtype=torch.float32)  # filtered\n    wh0 = torch.tensor(wh0, dtype=torch.float32)  # unfiltered\n    k = print_results(k)\n\n    # Plot\n    # k, d = [None] * 20, [None] * 20\n    # for i in tqdm(range(1, 21)):\n    #     k[i-1], d[i-1] = kmeans(wh / s, i)  # points, mean distance\n    # fig, ax = plt.subplots(1, 2, figsize=(14, 7), tight_layout=True)\n    # ax = ax.ravel()\n    # ax[0].plot(np.arange(1, 21), np.array(d) ** 2, marker='.')\n    # fig, ax = plt.subplots(1, 2, figsize=(14, 7))  # plot wh\n    # ax[0].hist(wh[wh[:, 0]&lt;100, 0],400)\n    # ax[1].hist(wh[wh[:, 1]&lt;100, 1],400)\n    # fig.savefig('wh.png', dpi=200)\n\n    # Evolve\n    npr = np.random\n    f, sh, mp, s = anchor_fitness(k), k.shape, 0.9, 0.1  # fitness, generations, mutation prob, sigma\n    pbar = tqdm(range(gen), desc=f'{prefix}Evolving anchors with Genetic Algorithm:')  # progress bar\n    for _ in pbar:\n        v = np.ones(sh)\n        while (v == 1).all():  # mutate until a change occurs (prevent duplicates)\n            v = ((npr.random(sh) &lt; mp) * random.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0)\n        kg = (k.copy() * v).clip(min=2.0)\n        fg = anchor_fitness(kg)\n        if fg &gt; f:#æœ‰æ›´å¥½çš„fitness\n            f, k = fg, kg.copy()#ä½¿ç”¨è¿›åŒ–åçš„ææ¡†å€¼\n            pbar.desc = f'{prefix}Evolving anchors with Genetic Algorithm: fitness = {f:.4f}'\n            if verbose:\n                print_results(k)\n\n    return print_results(k)\n\n</code></pre>\n<h5><a id=\"callbackpy_1532\"></a>callback.py</h5>\n<h5><a id=\"datasetspy__1533\"></a><strong>datasets.py è¯»å–æ•°æ®é›†ï¼Œå¹¶åšå¤„ç†çš„ç›¸å…³å‡½æ•°</strong></h5>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nDataloaders and dataset utils\n\"\"\"\n\nimport glob\nimport hashlib\nimport json\nimport logging\nimport os\nimport random\nimport shutil\nimport time\nfrom itertools import repeat\nfrom multiprocessing.pool import ThreadPool, Pool\nfrom pathlib import Path\nfrom threading import Thread\nfrom zipfile import ZipFile\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport yaml\nfrom PIL import Image, ExifTags\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\nfrom utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective\nfrom utils.general import check_dataset, check_requirements, check_yaml, clean_str, segments2boxes, \\\n    xywh2xyxy, xywhn2xyxy, xyxy2xywhn, xyn2xy\nfrom utils.torch_utils import torch_distributed_zero_first\n\n# Parameters\nHELP_URL = 'https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data'\n#å›¾ç‰‡æ ¼å¼\nIMG_FORMATS = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']  # acceptable image suffixes\n#è§†é¢‘æ ¼å¼\nVID_FORMATS = ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']  # acceptable video suffixes\nNUM_THREADS = min(8, os.cpu_count())  # number of multiprocessing threads\n\n# Get orientation exif tag\n# æ˜¯ä¸“é—¨ä¸ºæ•°ç ç›¸æœºç›¸ç‰‡è®¾è®¡çš„\nfor orientation in ExifTags.TAGS.keys():\n    if ExifTags.TAGS[orientation] == 'Orientation':\n        break\n\n#è¿”å›æ–‡ä»¶åˆ—è¡¨çš„hashå€¼\ndef get_hash(paths):\n    # Returns a single hash value of a list of paths (files or dirs)\n    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\n    h = hashlib.md5(str(size).encode())  # hash sizes\n    h.update(''.join(paths).encode())  # hash paths\n    return h.hexdigest()  # return hash\n\n#è·å–å›¾ç‰‡çš„å®½é«˜ä¿¡æ¯\ndef exif_size(img):\n    # Returns exif-corrected PIL size\n    s = img.size  # (width, height)\n    try:\n        rotation = dict(img._getexif().items())[orientation]#è°ƒæ•´æ•°æ®ç›¸æœºç…§ç‰‡æ–¹å‘\n        if rotation == 6:  # rotation 270\n            s = (s[1], s[0])\n        elif rotation == 8:  # rotation 90\n            s = (s[1], s[0])\n    except:\n        pass\n\n    return s\n\n\ndef exif_transpose(image):\n    \"\"\"\n    Transpose a PIL image accordingly if it has an EXIF Orientation tag.\n    From https://github.com/python-pillow/Pillow/blob/master/src/PIL/ImageOps.py\n\n    :param image: The image to transpose.\n    :return: An image.\n    \"\"\"\n    exif = image.getexif()\n    orientation = exif.get(0x0112, 1)  # default 1\n    if orientation &gt; 1:\n        method = {2: Image.FLIP_LEFT_RIGHT,\n                  3: Image.ROTATE_180,\n                  4: Image.FLIP_TOP_BOTTOM,\n                  5: Image.TRANSPOSE,\n                  6: Image.ROTATE_270,\n                  7: Image.TRANSVERSE,\n                  8: Image.ROTATE_90,\n                  }.get(orientation)\n        if method is not None:\n            image = image.transpose(method)\n            del exif[0x0112]\n            image.info[\"exif\"] = exif.tobytes()\n    return image\n\n\ndef create_dataloader(path, imgsz, batch_size, stride, single_cls=False, hyp=None, augment=False, cache=False, pad=0.0,\n                      rect=False, rank=-1, workers=8, image_weights=False, quad=False, prefix=''):\n    # Make sure only the first process in DDP process the dataset first, and the following others can use the cache\n    with torch_distributed_zero_first(rank):\n        dataset = LoadImagesAndLabels(path, imgsz, batch_size,\n                                      augment=augment,  # augment images\n                                      hyp=hyp,  # augmentation hyperparameters\n                                      rect=rect,  # rectangular training\n                                      cache_images=cache,\n                                      single_cls=single_cls,\n                                      stride=int(stride),\n                                      pad=pad,\n                                      image_weights=image_weights,\n                                      prefix=prefix)\n\n    batch_size = min(batch_size, len(dataset))\n    nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, workers])  # number of workers\n    sampler = torch.utils.data.distributed.DistributedSampler(dataset) if rank != -1 else None\n    loader = torch.utils.data.DataLoader if image_weights else InfiniteDataLoader\n    # Use torch.utils.data.DataLoader() if dataset.properties will update during training else InfiniteDataLoader()\n    dataloader = loader(dataset,\n                        batch_size=batch_size,\n                        num_workers=0,\n                        sampler=sampler,\n                        pin_memory=True,\n                        collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn)\n    return dataloader, dataset\n\n\nclass InfiniteDataLoader(torch.utils.data.dataloader.DataLoader):\n    \"\"\" Dataloader that reuses workers\n\n    Uses same syntax as vanilla DataLoader\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))\n        self.iterator = super().__iter__()\n\n    def __len__(self):\n        return len(self.batch_sampler.sampler)\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield next(self.iterator)\n\n\nclass _RepeatSampler(object):\n    \"\"\" Sampler that repeats forever\n\n    Args:\n        sampler (Sampler)\n    \"\"\"\n\n    def __init__(self, sampler):\n        self.sampler = sampler\n\n    def __iter__(self):\n        while True:\n            yield from iter(self.sampler)\n\n#å®šä¹‰è¿­ä»£å™¨ï¼Œç”¨äºdetect.py\nclass LoadImages:\n    # YOLOv5 image/video dataloader, i.e. `python detect.py --source image.jpg/vid.mp4`\n    def __init__(self, path, img_size=640, stride=32, auto=True):\n        p = str(Path(path).resolve())  # os-agnostic absolute path\n        #é‡‡ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥æå–å›¾ç‰‡ï¼Œè§†é¢‘ï¼Œå¯ä»¥è·å–æ–‡ä»¶è·¯å¾„\n        if '*' in p:\n            files = sorted(glob.glob(p, recursive=True))  # glob\n        elif os.path.isdir(p):\n            files = sorted(glob.glob(os.path.join(p, '*.*')))  # dir\n        elif os.path.isfile(p):\n            files = [p]  # files\n        else:\n            raise Exception(f'ERROR: {p} does not exist')\n        #æå–ç…§ç‰‡å’Œè§†é¢‘çš„æ–‡ä»¶è·¯å¾„\n        images = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]\n        videos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]\n        # è·å–å›¾ç‰‡ä¸è§†é¢‘çš„æ•°é‡\n        ni, nv = len(images), len(videos)\n\n        self.img_size = img_size#å›¾ç‰‡çš„å¤§å°\n        self.stride = stride\n        self.files = images + videos#æ•´åˆåˆ°ä¸€ä¸ªåˆ—è¡¨\n        self.nf = ni + nv  # number of files\n        self.video_flag = [False] * ni + [True] * nv\n        #åˆå§‹åŒ–æ¨¡å—ä¿¡æ¯ï¼Œå¯¹äºimageå’Œvideoåšä¸åŒçš„å¤„ç†\n        self.mode = 'image'\n        self.auto = auto\n        if any(videos):#å¦‚æœåŒ…å«è§†é¢‘æ–‡ä»¶ï¼Œå°±åˆå§‹åŒ–OpenCVä¸­çš„è§†é¢‘æ¨¡å—\n            self.new_video(videos[0])  # new video\n        else:\n            self.cap = None\n        #æ‰“å°æç¤ºä¿¡æ¯\n        assert self.nf &gt; 0, f'No images or videos found in {p}. ' \\\n                            f'Supported formats are:\\nimages: {IMG_FORMATS}\\nvideos: {VID_FORMATS}'\n\n    def __iter__(self):\n        self.count = 0\n        return self\n\n    def __next__(self):\n        if self.count == self.nf:#æ•°æ®æ˜¯å¦è¯»å®Œäº†\n            raise StopIteration\n        path = self.files[self.count]\n\n        if self.video_flag[self.count]:#å¦‚æœæ˜¯è§†é¢‘\n            # Read video\n            self.mode = 'video'\n            ret_val, img0 = self.cap.read()\n            if not ret_val:\n                self.count += 1\n                self.cap.release()#é‡Šæ”¾å¯¹è±¡\n                if self.count == self.nf:  # last video\n                    raise StopIteration\n                else:\n                    path = self.files[self.count]\n                    self.new_video(path)\n                    ret_val, img0 = self.cap.read()\n\n            self.frame += 1\n            print(f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: ', end='')\n\n        else:\n            # Read image\n            self.count += 1\n            img0 = cv2.imread(path)  # BGR\n            assert img0 is not None, 'Image Not Found ' + path\n            print(f'image {self.count}/{self.nf} {path}: ', end='')\n\n        # Padded resize\n        img = letterbox(img0, self.img_size, stride=self.stride, auto=self.auto)[0]\n\n        # Convert\n        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n        img = np.ascontiguousarray(img)#å°†æ•°ç»„å†…å­˜è½¬åŒ–ä¸ºè¿ç»­ï¼Œæé«˜è¿è¡Œé€Ÿåº¦\n\n        return path, img, img0, self.cap#è¿”å›resize+padå›¾ç‰‡ï¼ŒåŸå§‹å›¾ç‰‡ï¼Œè§†é¢‘å¯¹è±¡\n\n    def new_video(self, path):\n        self.frame = 0#è®°å½•å¸§æ•°\n        self.cap = cv2.VideoCapture(path)#åˆå§‹åŒ–è§†é¢‘å¯¹è±¡\n        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))#è·å–æ€»å¸§æ•°\n\n    def __len__(self):\n        return self.nf  # number of files\n\n#æœªä½¿ç”¨\nclass LoadWebcam:  # for inference\n    # YOLOv5 local webcam dataloader, i.e. `python detect.py --source 0`\n    def __init__(self, pipe='0', img_size=640, stride=32):\n        # self.mode = 'cap'\n        self.img_size = img_size\n        self.stride = stride\n        self.pipe = eval(pipe) if pipe.isnumeric() else pipe\n        self.cap = cv2.VideoCapture(self.pipe)  # video capture object\n        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)  # set buffer size\n\n    def __iter__(self):\n        self.count = -1\n        return self\n\n    def __next__(self):\n        self.count += 1\n        if cv2.waitKey(1) == ord('q'):  # q to quit\n            self.cap.release()\n            cv2.destroyAllWindows()\n            raise StopIteration\n\n        # Read frame\n        ret_val, img0 = self.cap.read()\n        img0 = cv2.flip(img0, 1)  # flip left-right\n\n        # Print\n        assert ret_val, f'Camera Error {self.pipe}'\n        img_path = 'webcam.jpg'\n        print(f'webcam {self.count}: ', end='')\n\n        # Padded resize\n        img = letterbox(img0, self.img_size, stride=self.stride)[0]\n\n        # Convert\n        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n        img = np.ascontiguousarray(img)\n\n        return img_path, img, img0, self.cap\n\n    def __len__(self):\n        return 0\n\n#å®šä¹‰è¿­ä»£å™¨ï¼Œç”¨äºdetect.pyæ–‡ä»¶ï¼Œå¤„ç†æ‘„åƒå¤´\n'''\ncv2è§†é¢‘å‡½æ•°;\ncap.grap()è·å–è§†é¢‘çš„ä¸‹ä¸€å¸§ï¼Œè¿”å›T/F\ncap.retrieve()åœ¨grapåä½¿ç”¨ï¼Œå¯¹è·å–çš„å¸§è¿›è¡Œè§£ç ï¼Œè¿”å›T/F\ncap.read(frame)ç»“åˆäº†grapå’Œretrieveçš„åŠŸèƒ½ï¼ŒæŠ“å–ä¸‹ä¸€å¸§å¹¶è§£ç \n'''\nclass LoadStreams:\n    #å¤šä¸ªipæˆ–è€…rtspæ‘„åƒå¤´\n    # YOLOv5 streamloader, i.e. `python detect.py --source 'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP streams`\n    def __init__(self, sources='streams.txt', img_size=640, stride=32, auto=True):\n        self.mode = 'stream'\n        self.img_size = img_size\n        self.stride = stride\n        #å¦‚æœsourcesæ˜¯ä¸€ä¸ªä¿å­˜äº†å¤šä¸ªè§†é¢‘æµçš„æ–‡ä»¶\n        #è·å–æ¯ä¸€ä¸ªè§†é¢‘æµï¼Œä¿å­˜ä¸ºä¸€ä¸ªæ–‡ä»¶\n        if os.path.isfile(sources):\n            with open(sources, 'r') as f:\n                sources = [x.strip() for x in f.read().strip().splitlines() if len(x.strip())]\n        else:\n            sources = [sources]\n\n        n = len(sources)\n        self.imgs, self.fps, self.frames, self.threads = [None] * n, [0] * n, [0] * n, [None] * n\n        self.sources = [clean_str(x) for x in sources]  # clean source names for laterï¼Œè§†é¢‘æµä¸ªæ•°\n        self.auto = auto\n        for i, s in enumerate(sources):  # index, source\n            # Start thread to read frames from video stream\n            print(f'{i + 1}/{n}: {s}... ', end='')#æ‰“å°å½“å‰è§†é¢‘ï¼Œæ€»è§†é¢‘æ•°ï¼Œè§†é¢‘æµåœ°å€\n            if 'youtube.com/' in str(s) or 'youtu.be/' in str(s):  # if source is YouTube video\n                check_requirements(('pafy', 'youtube_dl'))\n                import pafy\n                s = pafy.new(s).getbest(preftype=\"mp4\").url  # YouTube URL\n            s = eval(s) if s.isnumeric() else s  # i.e. s = '0' local webcam #å¦‚æœsource = 0 å°±æ‰“å¼€æ‘„åƒå¤´ï¼Œå¦åˆ™æ‰“å¼€è§†é¢‘æµçš„åœ°å€\n            cap = cv2.VideoCapture(s)\n            assert cap.isOpened(), f'Failed to open {s}'\n            w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))#è§†é¢‘å®½é«˜\n            h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n            self.fps[i] = max(cap.get(cv2.CAP_PROP_FPS) % 100, 0) or 30.0  # 30 FPS fallbackï¼Œè§†é¢‘å¸§ç‡\n            self.frames[i] = max(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), 0) or float('inf')  # infinite stream fallback\n\n            _, self.imgs[i] = cap.read()  # guarantee first frame\n            #å¤šçº¿ç¨‹è¯»å–ï¼Œdaemon=True è¡¨ç¤ºå®ˆæŠ¤çº¿ç¨‹\n            self.threads[i] = Thread(target=self.update, args=([i, cap, s]), daemon=True)\n            print(f\" success ({self.frames[i]} frames {w}x{h} at {self.fps[i]:.2f} FPS)\")\n            self.threads[i].start()\n        print('')  # newline\n\n        # check for common shapes\n        #è·å–è¿›è¡Œresize+padä¹‹åçš„shapeï¼Œletterå‡½æ•°é»˜è®¤æŒ‰ç…§çŸ©å½¢æ¨ç†å¡«å……\n        s = np.stack([letterbox(x, self.img_size, stride=self.stride, auto=self.auto)[0].shape for x in self.imgs])\n        self.rect = np.unique(s, axis=0).shape[0] == 1  # rect inference if all shapes equal\n        if not self.rect:#ç”»é¢ä¸åŒå½¢çŠ¶è­¦å‘Š\n            print('WARNING: Different stream shapes detected. For optimal performance supply similarly-shaped streams.')\n\n    def update(self, i, cap, stream):\n        # Read stream `i` frames in daemon thread\n        n, f, read = 0, self.frames[i], 1  # frame number, frame array, inference every 'read' frame\n        while cap.isOpened() and n &lt; f:\n            n += 1\n            # _, self.imgs[index] = cap.read()\n            cap.grab()\n            if n % read == 0:#\n                success, im = cap.retrieve()\n                if success:\n                    self.imgs[i] = im\n                else:\n                    print('WARNING: Video stream unresponsive, please check your IP camera connection.')\n                    self.imgs[i] *= 0\n                    cap.open(stream)  # re-open stream if signal was lost\n            time.sleep(1 / self.fps[i])  # wait time\n\n    def __iter__(self):\n        self.count = -1\n        return self\n\n    def __next__(self):\n        self.count += 1\n        #qé”®é€€å‡º\n        if not all(x.is_alive() for x in self.threads) or cv2.waitKey(1) == ord('q'):  # q to quit\n            cv2.destroyAllWindows()\n            raise StopIteration\n\n        # Letterbox\n        img0 = self.imgs.copy()\n        # letterboxå¯¹å›¾ç‰‡è¿›è¡Œç¼©æ”¾\n        img = [letterbox(x, self.img_size, stride=self.stride, auto=self.rect and self.auto)[0] for x in img0]\n\n        # Stack\n        #tå›¾ç‰‡æ‹¼æ¥ä»¥å‰\n        img = np.stack(img, 0)\n\n        # Convert\n        img = img[..., ::-1].transpose((0, 3, 1, 2))  # BGR to RGB, BHWC to BCHW\n        img = np.ascontiguousarray(img)\n\n        return self.sources, img, img0, None\n\n    def __len__(self):\n        return len(self.sources)  # 1E12 frames = 32 streams at 30 FPS for 30 years\n\n#è¿”å›æ ‡è®°æ–‡ä»¶ï¼Œã€‚txt\ndef img2label_paths(img_paths):\n    # Define label paths as a function of image paths\n    sa, sb = os.sep + 'images' + os.sep, os.sep + 'labels' + os.sep  # /images/, /labels/ substrings\n    return [sb.join(x.rsplit(sa, 1)).rsplit('.', 1)[0] + '.txt' for x in img_paths]\n\n#è‡ªå®šä¹‰æ•°æ®é›†ï¼Œé‡å†™lenï¼Œgetitemæ–¹æ³•\nclass LoadImagesAndLabels(Dataset):\n    # YOLOv5 train_loader/val_loader, loads images and labels for training and validation\n    cache_version = 0.6  # dataset labels *.cache version\n\n    def __init__(self, path, img_size=640, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,\n                 cache_images=False, single_cls=False, stride=32, pad=0.0, prefix=''):\n        self.img_size = img_size#è¾“å…¥å›¾ç‰‡å¤§å°\n        self.augment = augment#æ•°æ®å¢å¼º\n        self.hyp = hyp#è¶…å‚æ•°\n        self.image_weights = image_weights#å›¾ç‰‡é‡‡æ ·æƒé‡\n        self.rect = False if image_weights else rect#çŸ©å½¢è®­ç»ƒ\n        #mosaicæ•°æ®å¢å¼º\n        self.mosaic = self.augment and not self.rect  # load 4 images at a time into a mosaic (only during training)\n        #mosicæ•°æ®è¾¹ç•Œå¢å¼º\n        self.mosaic_border = [-img_size // 2, -img_size // 2]\n        self.stride = stride#æ¨¡å‹ä¸‹é‡‡æ ·æ­¥é•¿\n        self.path = path\n        self.albumentations = Albumentations() if augment else None\n\n        try:\n            f = []  # image files\n            for p in path if isinstance(path, list) else [path]:\n                #è·å–æ•°æ®é›†è·¯å¾„ï¼ŒåŒ…å«å›¾ç‰‡è·¯å¾„txtæ–‡ä»¶æˆ–è€…å›¾ç‰‡çš„æ–‡ä»¶å¤¹\n                p = Path(p)  # os-agnostic\n                if p.is_dir():  # dir\n                    f += glob.glob(str(p / '**' / '*.*'), recursive=True)#è·å–æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶è·¯å¾„\n                    # f = list(p.rglob('**/*.*'))  # pathlib\n                elif p.is_file():  # file\n                    with open(p, 'r') as t:\n                        t = t.read().strip().splitlines() #è·å–å›¾ç‰‡è·¯å¾„ï¼Œåˆ‡æ¢ç›¸å¯¹è·¯å¾„\n                        parent = str(p.parent) + os.sep#è·å–ä¸Šçº§çˆ¶ç›®å½•ï¼Œos.sepä¸ºè‡ªé€‚åº”ç³»ç»Ÿåˆ†éš”ç¬¦ï¼ŒWindows \\  linuxä¸º /  mac ä¸ºï¼š\n                        f += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # local to global path\n                        # f += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\n                else:\n                    raise Exception(f'{prefix}{p} does not exist')\n                #å›¾ç‰‡æ’åº\n            self.img_files = sorted([x.replace('/', os.sep) for x in f if x.split('.')[-1].lower() in IMG_FORMATS])\n            # self.img_files = sorted([x for x in f if x.suffix[1:].lower() in img_formats])  # pathlib\n            assert self.img_files, f'{prefix}No images found'\n        except Exception as e:\n            raise Exception(f'{prefix}Error loading data from {path}: {e}\\nSee {HELP_URL}')\n\n        # Check cache\n        self.label_files = img2label_paths(self.img_files)  # labelsï¼Œè¿”å›æ ‡ç­¾\n        cache_path = (p if p.is_file() else Path(self.label_files[0]).parent).with_suffix('.cache')#cache labelsï¼Œå°†æ ‡ç­¾åŠ è½½åˆ°å†…å­˜ä¸­ä¸å¿…æ¯æ¬¡åœ¨è¯»å–\n        try:\n            cache, exists = np.load(cache_path, allow_pickle=True).item(), True  # load dict\n            assert cache['version'] == self.cache_version  # same version\n            assert cache['hash'] == get_hash(self.label_files + self.img_files)  # same hash\n        except:\n            cache, exists = self.cache_labels(cache_path, prefix), False  # cache\n\n        # Display cache\n        nf, nm, ne, nc, n = cache.pop('results')  # found, missing, empty, corrupted, total\n        if exists:\n            d = f\"Scanning '{cache_path}' images and labels... {nf} found, {nm} missing, {ne} empty, {nc} corrupted\"\n            tqdm(None, desc=prefix + d, total=n, initial=n)  # display cache results\n            if cache['msgs']:\n                logging.info('\\n'.join(cache['msgs']))  # display warnings\n        assert nf &gt; 0 or not augment, f'{prefix}No labels in {cache_path}. Can not train without labels. See {HELP_URL}'\n\n        # Read cache\n        [cache.pop(k) for k in ('hash', 'version', 'msgs')]  # remove items\n        labels, shapes, self.segments = zip(*cache.values())#è§£å‹cacha\n        self.labels = list(labels)\n        self.shapes = np.array(shapes, dtype=np.float64)\n        #æ ¹æ®ç´¢å¼•æ’åºæ•°æ®é›†ä¸æ ‡ç­¾è·¯å¾„ï¼Œshapeï¼Œh/w\n        self.img_files = list(cache.keys())  # update\n        self.label_files = img2label_paths(cache.keys())  # update\n        n = len(shapes)  # number of images\n        bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # batch index\n        nb = bi[-1] + 1  # number of batches\n        self.batch = bi  # batch index of image\n        self.n = n\n        self.indices = range(n)\n\n        # Update labels\n        include_class = []  # filter labels to include only these classes (optional)\n        include_class_array = np.array(include_class).reshape(1, -1)\n        for i, (label, segment) in enumerate(zip(self.labels, self.segments)):\n            if include_class:\n                j = (label[:, 0:1] == include_class_array).any(1)\n                self.labels[i] = label[j]\n                if segment:\n                    self.segments[i] = segment[j]\n            if single_cls:  # single-class training, merge all classes into 0\n                self.labels[i][:, 0] = 0\n                if segment:\n                    self.segments[i][:, 0] = 0\n\n        # Rectangular Training\n        if self.rect:\n            # Sort by aspect ratio\n            s = self.shapes  # wh\n            ar = s[:, 1] / s[:, 0]  # aspect ratio\n            irect = ar.argsort()\n            self.img_files = [self.img_files[i] for i in irect]\n            self.label_files = [self.label_files[i] for i in irect]\n            self.labels = [self.labels[i] for i in irect]\n            self.shapes = s[irect]  # wh\n            ar = ar[irect]\n\n            # Set training image shapes\n            shapes = [[1, 1]] * nb\n            for i in range(nb):\n                ari = ar[bi == i]\n                mini, maxi = ari.min(), ari.max()\n                if maxi &lt; 1:\n                    shapes[i] = [maxi, 1]\n                elif mini &gt; 1:\n                    shapes[i] = [1, 1 / mini]\n\n            self.batch_shapes = np.ceil(np.array(shapes) * img_size / stride + pad).astype(np.int) * stride\n\n        # Cache images into memory for faster training (WARNING: large datasets may exceed system RAM)\n        self.imgs, self.img_npy = [None] * n, [None] * n\n        if cache_images:\n            if cache_images == 'disk':\n                self.im_cache_dir = Path(Path(self.img_files[0]).parent.as_posix() + '_npy')\n                self.img_npy = [self.im_cache_dir / Path(f).with_suffix('.npy').name for f in self.img_files]\n                self.im_cache_dir.mkdir(parents=True, exist_ok=True)\n            gb = 0  # Gigabytes of cached images\n            self.img_hw0, self.img_hw = [None] * n, [None] * n\n            results = ThreadPool(NUM_THREADS).imap(lambda x: load_image(*x), zip(repeat(self), range(n)))\n            pbar = tqdm(enumerate(results), total=n)\n            for i, x in pbar:\n                if cache_images == 'disk':\n                    if not self.img_npy[i].exists():\n                        np.save(self.img_npy[i].as_posix(), x[0])\n                    gb += self.img_npy[i].stat().st_size\n                else:\n                    self.imgs[i], self.img_hw0[i], self.img_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)\n                    gb += self.imgs[i].nbytes\n                pbar.desc = f'{prefix}Caching images ({gb / 1E9:.1f}GB {cache_images})'\n            pbar.close()\n\n    def cache_labels(self, path=Path('./labels.cache'), prefix=''):\n        # Cache dataset labels, check images and read shapes\n        x = {}  # dict\n        nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # number missing, found, empty, corrupt, messages\n        desc = f\"{prefix}Scanning '{path.parent / path.stem}' images and labels...\"\n        with Pool(NUM_THREADS) as pool:\n            pbar = tqdm(pool.imap(verify_image_label, zip(self.img_files, self.label_files, repeat(prefix))),\n                        desc=desc, total=len(self.img_files))\n            for im_file, l, shape, segments, nm_f, nf_f, ne_f, nc_f, msg in pbar:\n                nm += nm_f\n                nf += nf_f\n                ne += ne_f\n                nc += nc_f\n                if im_file:\n                    x[im_file] = [l, shape, segments]\n                if msg:\n                    msgs.append(msg)\n                pbar.desc = f\"{desc}{nf} found, {nm} missing, {ne} empty, {nc} corrupted\"\n\n        pbar.close()\n        if msgs:\n            logging.info('\\n'.join(msgs))\n        if nf == 0:\n            logging.info(f'{prefix}WARNING: No labels found in {path}. See {HELP_URL}')\n        x['hash'] = get_hash(self.label_files + self.img_files)\n        x['results'] = nf, nm, ne, nc, len(self.img_files)\n        x['msgs'] = msgs  # warnings\n        x['version'] = self.cache_version  # cache version\n        try:\n            np.save(path, x)  # save cache for next time\n            path.with_suffix('.cache.npy').rename(path)  # remove .npy suffix\n            logging.info(f'{prefix}New cache created: {path}')\n        except Exception as e:\n            logging.info(f'{prefix}WARNING: Cache directory {path.parent} is not writeable: {e}')  # path not writeable\n        return x\n\n    def __len__(self):\n        return len(self.img_files)\n\n    # def __iter__(self):\n    #     self.count = -1\n    #     print('ran dataset iter')\n    #     #self.shuffled_vector = np.random.permutation(self.nF) if self.augment else np.arange(self.nF)\n    #     return self\n\n    def __getitem__(self, index):\n        index = self.indices[index]  # linear, shuffled, or image_weights\n\n        hyp = self.hyp#è¶…å‚æ•°\n        mosaic = self.mosaic and random.random() &lt; hyp['mosaic']\n        if mosaic:\n            # Load mosaic\n            img, labels = load_mosaic(self, index)\n            shapes = None\n\n            # MixUp augmentationï¼Œæ•°æ®å¢å¼º\n            if random.random() &lt; hyp['mixup']:\n                img, labels = mixup(img, labels, *load_mosaic(self, random.randint(0, self.n - 1)))\n\n        else:\n            # Load image\n            img, (h0, w0), (h, w) = load_image(self, index)\n\n            # Letterbox\n            shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size  # final letterboxed shape\n            img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)\n            shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling\n\n            labels = self.labels[index].copy()\n            #è°ƒæ•´æ ‡ç­¾åæ ‡ä¸åœ¨æ˜¯å½’ä¸€åŒ–çš„å€¼\n            if labels.size:  # normalized xywh to pixel xyxy format\n                labels[:, 1:] = xywhn2xyxy(labels[:, 1:], ratio[0] * w, ratio[1] * h, padw=pad[0], padh=pad[1])\n\n            if self.augment:#éšæœºé€è§†å˜æ¢\n                img, labels = random_perspective(img, labels,\n                                                 degrees=hyp['degrees'],\n                                                 translate=hyp['translate'],\n                                                 scale=hyp['scale'],\n                                                 shear=hyp['shear'],\n                                                 perspective=hyp['perspective'])\n\n        nl = len(labels)  # number of labels\n        if nl:#è°ƒæ•´æ ‡ç­¾æ¡†åæ ‡å½’ä¸€åŒ–0-1\n            labels[:, 1:5] = xyxy2xywhn(labels[:, 1:5], w=img.shape[1], h=img.shape[0], clip=True, eps=1E-3)\n\n        if self.augment:#å›¾ç‰‡éšæœºæ—‹è½¬ï¼Œä¸Šä¸‹\n            # Albumentations\n            img, labels = self.albumentations(img, labels)\n            nl = len(labels)  # update after albumentations\n\n            # HSV color-spaceï¼Œéšæœºæ”¹å˜å›¾ç‰‡çš„è‰²è°ƒï¼ˆHï¼‰,é¥±å’Œåº¦ï¼ˆsï¼‰,äº®åº¦ï¼ˆVï¼‰\n            augment_hsv(img, hgain=hyp['hsv_h'], sgain=hyp['hsv_s'], vgain=hyp['hsv_v'])\n\n            # Flip up-down\n            if random.random() &lt; hyp['flipud']:\n                img = np.flipud(img)\n                if nl:\n                    labels[:, 2] = 1 - labels[:, 2]\n\n            # Flip left-right\n            if random.random() &lt; hyp['fliplr']:\n                img = np.fliplr(img)\n                if nl:\n                    labels[:, 1] = 1 - labels[:, 1]\n\n            # Cutouts\n            # labels = cutout(img, labels, p=0.5)\n        #åˆå§‹åŒ–æ ‡ç­¾æ¡†å¯¹åº”çš„å›¾ç‰‡åºå·ï¼Œæ–¹ä¾¿collate-fnçš„ä½¿ç”¨\n        labels_out = torch.zeros((nl, 6))\n        if nl:\n            labels_out[:, 1:] = torch.from_numpy(labels)\n\n        # Convert\n        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n        img = np.ascontiguousarray(img)\n\n        return torch.from_numpy(img), labels_out, self.img_files[index], shapes\n\n    @staticmethod\n    #æ•´ç†å‡½æ•°ï¼Œå¦‚ä½•å–æ ·ï¼Œå¯ä»¥å®šä¹‰è‡ªå·±çš„å‡½æ•°æ¥å®ç°åŠŸèƒ½\n    def collate_fn(batch):\n        img, label, path, shapes = zip(*batch)  # transposed\n        for i, l in enumerate(label):\n            l[:, 0] = i  # add target image index for build_targets()\n        return torch.stack(img, 0), torch.cat(label, 0), path, shapes\n\n    @staticmethod\n    def collate_fn4(batch):\n        img, label, path, shapes = zip(*batch)  # transposed\n        n = len(shapes) // 4\n        img4, label4, path4, shapes4 = [], [], path[:n], shapes[:n]\n\n        ho = torch.tensor([[0., 0, 0, 1, 0, 0]])\n        wo = torch.tensor([[0., 0, 1, 0, 0, 0]])\n        s = torch.tensor([[1, 1, .5, .5, .5, .5]])  # scale\n        for i in range(n):  # zidane torch.zeros(16,3,720,1280)  # BCHW\n            i *= 4\n            if random.random() &lt; 0.5:\n                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2., mode='bilinear', align_corners=False)[\n                    0].type(img[i].type())\n                l = label[i]\n            else:\n                im = torch.cat((torch.cat((img[i], img[i + 1]), 1), torch.cat((img[i + 2], img[i + 3]), 1)), 2)\n                l = torch.cat((label[i], label[i + 1] + ho, label[i + 2] + wo, label[i + 3] + ho + wo), 0) * s\n            img4.append(im)\n            label4.append(l)\n\n        for i, l in enumerate(label4):\n            l[:, 0] = i  # add target image index for build_targets()\n\n        return torch.stack(img4, 0), torch.cat(label4, 0), path4, shapes4\n\n\n# Ancillary functions --------------------------------------------------------------------------------------------------\n#åŠ è½½å›¾ç‰‡å¹¶æ ¹æ®è®¾å®šçš„è¾“å…¥å¤§å°ä¸åŸå›¾ç‰‡å¤§å°æ¯”ä¾‹ratioè¿›è¡Œresize\ndef load_image(self, i):\n    # loads 1 image from dataset index 'i', returns im, original hw, resized hw\n    im = self.imgs[i]\n    if im is None:  # not cached in ram\n        npy = self.img_npy[i]\n        if npy and npy.exists():  # load npy\n            im = np.load(npy)\n        else:  # read image\n            path = self.img_files[i]\n            im = cv2.imread(path)  # BGR\n            assert im is not None, 'Image Not Found ' + path\n        h0, w0 = im.shape[:2]  # orig hw\n        r = self.img_size / max(h0, w0)  # ratio\n        #æ ¹æ®ratioè¿›è¡Œä¸åŒçš„æ’å€¼\n        if r != 1:  # if sizes are not equal\n            im = cv2.resize(im, (int(w0 * r), int(h0 * r)),\n                            interpolation=cv2.INTER_AREA if r &lt; 1 and not self.augment else cv2.INTER_LINEAR)\n        return im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized\n    else:\n        return self.imgs[i], self.img_hw0[i], self.img_hw[i]  # im, hw_original, hw_resized\n\n#å¼•å…¥ä¸‰å¼ éšæœºç…§ç‰‡ï¼Œç”Ÿæˆä¸€ä¸ªå›¾åƒå¢å¼ºå›¾ç‰‡\ndef load_mosaic(self, index):\n    # YOLOv5 4-mosaic loader. Loads 1 image + 3 random images into a 4-image mosaic\n    labels4, segments4 = [], []\n    s = self.img_size\n    #éšæœºå–mosaicä¸­å¿ƒç‚¹\n    yc, xc = [int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border]  # mosaic center x, y\n    #éšæœºå–ä¸‰å¼ å›¾ç‰‡çš„ç´¢å¼•\n    indices = [index] + random.choices(self.indices, k=3)  # 3 additional image indices\n    random.shuffle(indices)\n    for i, index in enumerate(indices):\n        # Load image\n        #åŠ è½½å›¾ç‰‡å¹¶æ ¹æ®è®¾å®šçš„è¾“å…¥å¤§å°ä¸å›¾ç‰‡åŸå¤§å°çš„æ¯”ä¾‹ratioè¿›è¡Œresize\n        img, _, (h, w) = load_image(self, index)\n\n        # place img in img4\n        if i == 0:  # top left\n            #åˆå§‹åŒ–å¤§å›¾\n            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n            #è®¾ç½®å¤§å›¾ä¸Šçš„ä½ç½®ï¼ˆå·¦ä¸Šè§’ï¼‰\n            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n            #é€‰å–å°å›¾ä¸Šä½ç½®\n            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n        elif i == 1:  # top right\n            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n        elif i == 2:  # bottom left\n            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\n        elif i == 3:  # bottom right\n            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n\n        #å°†å°å›¾ä¸Šæˆªå–çš„éƒ¨åˆ†è´´åˆ°å¤§å›¾ä¸Š\n        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n        #è®¡ç®—å°å›¾åˆ°å¤§å›¾ä¸Šæ—¶æ‰€äº§ç”Ÿçš„åç§»ï¼Œç”¨æ¥è®¡ç®—mosaicå¢å¼ºåçš„æ ‡ç­¾æ¡†ä½ç½®\n        padw = x1a - x1b\n        padh = y1a - y1b\n\n        # Labels\n        labels, segments = self.labels[index].copy(), self.segments[index].copy()\n        if labels.size:\n            #é‡æ–°è°ƒæ•´æ ‡ç­¾æ¡†åæ ‡ä¿¡æ¯\n            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padw, padh)  # normalized xywh to pixel xyxy format\n            segments = [xyn2xy(x, w, h, padw, padh) for x in segments]\n        labels4.append(labels)\n        segments4.extend(segments)\n\n    # Concat/clip labels\n    #è°ƒæ•´åæ ‡æ¡†åœ¨å›¾ç‰‡å†…éƒ¨\n    labels4 = np.concatenate(labels4, 0)\n    for x in (labels4[:, 1:], *segments4):\n        np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()\n    # img4, labels4 = replicate(img4, labels4)  # replicate\n\n    # Augment\n    img4, labels4, segments4 = copy_paste(img4, labels4, segments4, p=self.hyp['copy_paste'])\n    #è¿›è¡Œmosaicçš„æ—¶å€™å°†å››å¼ å›¾ç‰‡æ•´åˆåˆ°ä¸€èµ·ä¹‹åshapeä¸º[2*img_sizeï¼Œ2*img_size]\n    #éšæœºæ—‹è½¬å¹³ç§»ç¼©æ”¾å‰ªåˆ‡ï¼Œå¹¶resizeä¸ºè¾“å…¥å¤§å°img_size\n    img4, labels4 = random_perspective(img4, labels4, segments4,\n                                       degrees=self.hyp['degrees'],\n                                       translate=self.hyp['translate'],\n                                       scale=self.hyp['scale'],\n                                       shear=self.hyp['shear'],\n                                       perspective=self.hyp['perspective'],\n                                       border=self.mosaic_border)  # border to remove\n\n    return img4, labels4\n\n#éšæœºåŠ å…¥8å¼ ç…§ç‰‡ï¼Œæ„é€ 9å¼ ç…§ç‰‡\ndef load_mosaic9(self, index):\n    # YOLOv5 9-mosaic loader. Loads 1 image + 8 random images into a 9-image mosaic\n    labels9, segments9 = [], []\n    s = self.img_size\n    #éšæœºå–ä¸‰å¼ å›¾ç‰‡çš„ç´¢å¼•\n    indices = [index] + random.choices(self.indices, k=8)  # 8 additional image indices\n    random.shuffle(indices)\n    for i, index in enumerate(indices):\n        # Load image\n        #åŠ è½½å›¾ç‰‡å¹¶æ ¹æ®è®¾å®šçš„è¾“å…¥å¤§å°ä¸å›¾ç‰‡åŸå¤§å°çš„æ¯”ä¾‹ratioè¿›è¡Œresize\n        img, _, (h, w) = load_image(self, index)\n\n        # place img in img9\n        if i == 0:  # center\n            #åˆå§‹åŒ–å¤§å›¾\n            img9 = np.full((s * 3, s * 3, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n            h0, w0 = h, w\n            c = s, s, s + w, s + h  # xmin, ymin, xmax, ymax (base) coordinates\n        elif i == 1:  # top\n            c = s, s - h, s + w, s\n        elif i == 2:  # top right\n            c = s + wp, s - h, s + wp + w, s\n        elif i == 3:  # right\n            c = s + w0, s, s + w0 + w, s + h\n        elif i == 4:  # bottom right\n            c = s + w0, s + hp, s + w0 + w, s + hp + h\n        elif i == 5:  # bottom\n            c = s + w0 - w, s + h0, s + w0, s + h0 + h\n        elif i == 6:  # bottom left\n            c = s + w0 - wp - w, s + h0, s + w0 - wp, s + h0 + h\n        elif i == 7:  # left\n            c = s - w, s + h0 - h, s, s + h0\n        elif i == 8:  # top left\n            c = s - w, s + h0 - hp - h, s, s + h0 - hp\n\n        padx, pady = c[:2]\n        x1, y1, x2, y2 = [max(x, 0) for x in c]  # allocate coords\n\n        # Labels\n        labels, segments = self.labels[index].copy(), self.segments[index].copy()\n        if labels.size:\n            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padx, pady)  # normalized xywh to pixel xyxy format\n            segments = [xyn2xy(x, w, h, padx, pady) for x in segments]\n        labels9.append(labels)\n        segments9.extend(segments)\n\n        # Image\n        img9[y1:y2, x1:x2] = img[y1 - pady:, x1 - padx:]  # img9[ymin:ymax, xmin:xmax]\n        hp, wp = h, w  # height, width previous\n\n    # Offset\n    #éšæœºå–mosaicä¸­å¿ƒ\n    yc, xc = [int(random.uniform(0, s)) for _ in self.mosaic_border]  # mosaic center x, y\n    img9 = img9[yc:yc + 2 * s, xc:xc + 2 * s]\n\n    # Concat/clip labels\n    labels9 = np.concatenate(labels9, 0)\n    labels9[:, [1, 3]] -= xc\n    labels9[:, [2, 4]] -= yc\n    c = np.array([xc, yc])  # centers\n    segments9 = [x - c for x in segments9]\n\n    for x in (labels9[:, 1:], *segments9):\n        np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()\n    # img9, labels9 = replicate(img9, labels9)  # replicate\n\n    # Augment\n    img9, labels9 = random_perspective(img9, labels9, segments9,\n                                       degrees=self.hyp['degrees'],\n                                       translate=self.hyp['translate'],\n                                       scale=self.hyp['scale'],\n                                       shear=self.hyp['shear'],\n                                       perspective=self.hyp['perspective'],\n                                       border=self.mosaic_border)  # border to remove\n\n    return img9, labels9\n\n\ndef create_folder(path='./new'):\n    # Create folder\n    if os.path.exists(path):\n        shutil.rmtree(path)  # delete output folder\n    os.makedirs(path)  # make new output folder\n\n\ndef flatten_recursive(path='../datasets/coco128'):\n    # Flatten a recursive directory by bringing all files to top level\n    new_path = Path(path + '_flat')\n    create_folder(new_path)\n    for file in tqdm(glob.glob(str(Path(path)) + '/**/*.*', recursive=True)):\n        shutil.copyfile(file, new_path / Path(file).name)\n\n\ndef extract_boxes(path='../datasets/coco128'):  # from utils.datasets import *; extract_boxes()\n    # Convert detection dataset into classification dataset, with one directory per class\n    path = Path(path)  # images dir\n    shutil.rmtree(path / 'classifier') if (path / 'classifier').is_dir() else None  # remove existing\n    files = list(path.rglob('*.*'))\n    n = len(files)  # number of files\n    for im_file in tqdm(files, total=n):\n        if im_file.suffix[1:] in IMG_FORMATS:\n            # image\n            im = cv2.imread(str(im_file))[..., ::-1]  # BGR to RGB\n            h, w = im.shape[:2]\n\n            # labels\n            lb_file = Path(img2label_paths([str(im_file)])[0])\n            if Path(lb_file).exists():\n                with open(lb_file, 'r') as f:\n                    lb = np.array([x.split() for x in f.read().strip().splitlines()], dtype=np.float32)  # labels\n\n                for j, x in enumerate(lb):\n                    c = int(x[0])  # class\n                    f = (path / 'classifier') / f'{c}' / f'{path.stem}_{im_file.stem}_{j}.jpg'  # new filename\n                    if not f.parent.is_dir():\n                        f.parent.mkdir(parents=True)\n\n                    b = x[1:] * [w, h, w, h]  # box\n                    # b[2:] = b[2:].max()  # rectangle to square\n                    b[2:] = b[2:] * 1.2 + 3  # pad\n                    b = xywh2xyxy(b.reshape(-1, 4)).ravel().astype(np.int)\n\n                    b[[0, 2]] = np.clip(b[[0, 2]], 0, w)  # clip boxes outside of image\n                    b[[1, 3]] = np.clip(b[[1, 3]], 0, h)\n                    assert cv2.imwrite(str(f), im[b[1]:b[3], b[0]:b[2]]), f'box failure in {f}'\n\n\ndef autosplit(path='../datasets/coco128/images', weights=(0.9, 0.1, 0.0), annotated_only=False):\n    \"\"\" Autosplit a dataset into train/val/test splits and save path/autosplit_*.txt files\n    Usage: from utils.datasets import *; autosplit()\n    Arguments\n        path:            Path to images directory\n        weights:         Train, val, test weights (list, tuple)\n        annotated_only:  Only use images with an annotated txt file\n    \"\"\"\n    path = Path(path)  # images dir\n    files = sum([list(path.rglob(f\"*.{img_ext}\")) for img_ext in IMG_FORMATS], [])  # image files only\n    n = len(files)  # number of files\n    random.seed(0)  # for reproducibility\n    indices = random.choices([0, 1, 2], weights=weights, k=n)  # assign each image to a split\n\n    txt = ['autosplit_train.txt', 'autosplit_val.txt', 'autosplit_test.txt']  # 3 txt files\n    [(path.parent / x).unlink(missing_ok=True) for x in txt]  # remove existing\n\n    print(f'Autosplitting images from {path}' + ', using *.txt labeled images only' * annotated_only)\n    for i, img in tqdm(zip(indices, files), total=n):\n        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # check label\n            with open(path.parent / txt[i], 'a') as f:\n                f.write('./' + img.relative_to(path.parent).as_posix() + '\\n')  # add image to txt file\n\n\ndef verify_image_label(args):\n    # Verify one image-label pair\n    im_file, lb_file, prefix = args\n    nm, nf, ne, nc, msg, segments = 0, 0, 0, 0, '', []  # number (missing, found, empty, corrupt), message, segments\n    try:\n        # verify images\n        im = Image.open(im_file)\n        im.verify()  # PIL verify\n        shape = exif_size(im)  # image size\n        assert (shape[0] &gt; 9) &amp; (shape[1] &gt; 9), f'image size {shape} &lt;10 pixels'\n        assert im.format.lower() in IMG_FORMATS, f'invalid image format {im.format}'\n        if im.format.lower() in ('jpg', 'jpeg'):\n            with open(im_file, 'rb') as f:\n                f.seek(-2, 2)\n                if f.read() != b'\\xff\\xd9':  # corrupt JPEG\n                    Image.open(im_file).save(im_file, format='JPEG', subsampling=0, quality=100)  # re-save image\n                    msg = f'{prefix}WARNING: {im_file}: corrupt JPEG restored and saved'\n\n        # verify labels\n        if os.path.isfile(lb_file):\n            nf = 1  # label found\n            with open(lb_file, 'r') as f:\n                l = [x.split() for x in f.read().strip().splitlines() if len(x)]\n                if any([len(x) &gt; 8 for x in l]):  # is segment\n                    classes = np.array([x[0] for x in l], dtype=np.float32)\n                    segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in l]  # (cls, xy1...)\n                    l = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (cls, xywh)\n                l = np.array(l, dtype=np.float32)\n            nl = len(l)\n            if nl:\n                assert l.shape[1] == 5, f'labels require 5 columns, {l.shape[1]} columns detected'\n                assert (l &gt;= 0).all(), f'negative label values {l[l &lt; 0]}'\n                assert (l[:, 1:] &lt;= 1).all(), f'non-normalized or out of bounds coordinates {l[:, 1:][l[:, 1:] &gt; 1]}'\n                l = np.unique(l, axis=0)  # remove duplicate rows\n                if len(l) &lt; nl:\n                    segments = np.unique(segments, axis=0)\n                    msg = f'{prefix}WARNING: {im_file}: {nl - len(l)} duplicate labels removed'\n            else:\n                ne = 1  # label empty\n                l = np.zeros((0, 5), dtype=np.float32)\n        else:\n            nm = 1  # label missing\n            l = np.zeros((0, 5), dtype=np.float32)\n        return im_file, l, shape, segments, nm, nf, ne, nc, msg\n    except Exception as e:\n        nc = 1\n        msg = f'{prefix}WARNING: {im_file}: ignoring corrupt image/label: {e}'\n        return [None, None, None, None, nm, nf, ne, nc, msg]\n\n\ndef dataset_stats(path='coco128.yaml', autodownload=False, verbose=False, profile=False, hub=False):\n    \"\"\" Return dataset statistics dictionary with images and instances counts per split per class\n    To run in parent directory: export PYTHONPATH=\"$PWD/yolov5\"\n    Usage1: from utils.datasets import *; dataset_stats('coco128.yaml', autodownload=True)\n    Usage2: from utils.datasets import *; dataset_stats('../datasets/coco128_with_yaml.zip')\n    Arguments\n        path:           Path to data.yaml or data.zip (with data.yaml inside data.zip)\n        autodownload:   Attempt to download dataset if not found locally\n        verbose:        Print stats dictionary\n    \"\"\"\n\n    def round_labels(labels):\n        # Update labels to integer class and 6 decimal place floats\n        return [[int(c), *[round(x, 4) for x in points]] for c, *points in labels]\n\n    def unzip(path):\n        # Unzip data.zip TODO: CONSTRAINT: path/to/abc.zip MUST unzip to 'path/to/abc/'\n        if str(path).endswith('.zip'):  # path is data.zip\n            assert Path(path).is_file(), f'Error unzipping {path}, file not found'\n            ZipFile(path).extractall(path=path.parent)  # unzip\n            dir = path.with_suffix('')  # dataset directory == zip name\n            return True, str(dir), next(dir.rglob('*.yaml'))  # zipped, data_dir, yaml_path\n        else:  # path is data.yaml\n            return False, None, path\n\n    def hub_ops(f, max_dim=1920):\n        # HUB ops for 1 image 'f': resize and save at reduced quality in /dataset-hub for web/app viewing\n        f_new = im_dir / Path(f).name  # dataset-hub image filename\n        try:  # use PIL\n            im = Image.open(f)\n            r = max_dim / max(im.height, im.width)  # ratio\n            if r &lt; 1.0:  # image too large\n                im = im.resize((int(im.width * r), int(im.height * r)))\n            im.save(f_new, quality=75)  # save\n        except Exception as e:  # use OpenCV\n            print(f'WARNING: HUB ops PIL failure {f}: {e}')\n            im = cv2.imread(f)\n            im_height, im_width = im.shape[:2]\n            r = max_dim / max(im_height, im_width)  # ratio\n            if r &lt; 1.0:  # image too large\n                im = cv2.resize(im, (int(im_width * r), int(im_height * r)), interpolation=cv2.INTER_LINEAR)\n            cv2.imwrite(str(f_new), im)\n\n    zipped, data_dir, yaml_path = unzip(Path(path))\n    with open(check_yaml(yaml_path), errors='ignore') as f:\n        data = yaml.safe_load(f)  # data dict\n        if zipped:\n            data['path'] = data_dir  # TODO: should this be dir.resolve()?\n    check_dataset(data, autodownload)  # download dataset if missing\n    hub_dir = Path(data['path'] + ('-hub' if hub else ''))\n    stats = {'nc': data['nc'], 'names': data['names']}  # statistics dictionary\n    for split in 'train', 'val', 'test':\n        if data.get(split) is None:\n            stats[split] = None  # i.e. no test set\n            continue\n        x = []\n        dataset = LoadImagesAndLabels(data[split])  # load dataset\n        for label in tqdm(dataset.labels, total=dataset.n, desc='Statistics'):\n            x.append(np.bincount(label[:, 0].astype(int), minlength=data['nc']))\n        x = np.array(x)  # shape(128x80)\n        stats[split] = {'instance_stats': {'total': int(x.sum()), 'per_class': x.sum(0).tolist()},\n                        'image_stats': {'total': dataset.n, 'unlabelled': int(np.all(x == 0, 1).sum()),\n                                        'per_class': (x &gt; 0).sum(0).tolist()},\n                        'labels': [{str(Path(k).name): round_labels(v.tolist())} for k, v in\n                                   zip(dataset.img_files, dataset.labels)]}\n\n        if hub:\n            im_dir = hub_dir / 'images'\n            im_dir.mkdir(parents=True, exist_ok=True)\n            for _ in tqdm(ThreadPool(NUM_THREADS).imap(hub_ops, dataset.img_files), total=dataset.n, desc='HUB Ops'):\n                pass\n\n    # Profile\n    stats_path = hub_dir / 'stats.json'\n    if profile:\n        for _ in range(1):\n            file = stats_path.with_suffix('.npy')\n            t1 = time.time()\n            np.save(file, stats)\n            t2 = time.time()\n            x = np.load(file, allow_pickle=True)\n            print(f'stats.npy times: {time.time() - t2:.3f}s read, {t2 - t1:.3f}s write')\n\n            file = stats_path.with_suffix('.json')\n            t1 = time.time()\n            with open(file, 'w') as f:\n                json.dump(stats, f)  # save stats *.json\n            t2 = time.time()\n            with open(file, 'r') as f:\n                x = json.load(f)  # load hyps dict\n            print(f'stats.json times: {time.time() - t2:.3f}s read, {t2 - t1:.3f}s write')\n\n    # Save, print and return\n    if hub:\n        print(f'Saving {stats_path.resolve()}...')\n        with open(stats_path, 'w') as f:\n            json.dump(stats, f)  # save stats.json\n    if verbose:\n        print(json.dumps(stats, indent=2, sort_keys=False))\n    return stats\n\n</code></pre>\n<h5><a id=\"donwloadpy__2618\"></a>donwload.py ä¸‹è½½éœ€è¦çš„æƒé‡æ–‡ä»¶ç­‰å‡½æ•°</h5>\n<h5><a id=\"generalpy__2619\"></a>general.py é¡¹ç›®é€šç”¨ä»£ç </h5>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nGeneral utils\n\"\"\"\n\nimport contextlib\nimport glob\nimport logging\nimport math\nimport os\nimport platform\nimport random\nimport re\nimport signal\nimport time\nimport urllib\nfrom itertools import repeat\nfrom multiprocessing.pool import ThreadPool\nfrom pathlib import Path\nfrom subprocess import check_output\nfrom zipfile import ZipFile\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport pkg_resources as pkg\nimport torch\nimport torchvision\nimport yaml\n\nfrom utils.downloads import gsutil_getsize\nfrom utils.metrics import box_iou, fitness\n\n# Settings\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\n#ç¦æ­¢OpenCVçš„å¤šçº¿ç¨‹ï¼Œä½¿ç”¨torchçš„æ‰€çº¿ç¨‹\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(min(os.cpu_count(), 8))  # NumExpr max threads\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\n\n\nclass Profile(contextlib.ContextDecorator):\n    # Usage: @Profile() decorator or 'with Profile():' context manager\n    def __enter__(self):\n        self.start = time.time()\n\n    def __exit__(self, type, value, traceback):\n        print(f'Profile results: {time.time() - self.start:.5f}s')\n\n\nclass Timeout(contextlib.ContextDecorator):\n    # Usage: @Timeout(seconds) decorator or 'with Timeout(seconds):' context manager\n    def __init__(self, seconds, *, timeout_msg='', suppress_timeout_errors=True):\n        self.seconds = int(seconds)\n        self.timeout_message = timeout_msg\n        self.suppress = bool(suppress_timeout_errors)\n\n    def _timeout_handler(self, signum, frame):\n        raise TimeoutError(self.timeout_message)\n\n    def __enter__(self):\n        signal.signal(signal.SIGALRM, self._timeout_handler)  # Set handler for SIGALRM\n        signal.alarm(self.seconds)  # start countdown for SIGALRM to be raised\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        signal.alarm(0)  # Cancel SIGALRM if it's scheduled\n        if self.suppress and exc_type is TimeoutError:  # Suppress TimeoutError\n            return True\n\n\ndef try_except(func):\n    # try-except function. Usage: @try_except decorator\n    def handler(*args, **kwargs):\n        try:\n            func(*args, **kwargs)\n        except Exception as e:\n            print(e)\n\n    return handler\n\n\ndef methods(instance):\n    # Get class/instance methods\n    return [f for f in dir(instance) if callable(getattr(instance, f)) and not f.startswith(\"__\")]\n\n#è®¾ç½®æ—¥å¿—çš„ä¿å­˜çº§åˆ«\ndef set_logging(rank=-1, verbose=True):\n    logging.basicConfig(\n        format=\"%(message)s\",\n        level=logging.INFO if (verbose and rank in [-1, 0]) else logging.WARN)\n\n\ndef print_args(name, opt):\n    # Print argparser arguments\n    print(colorstr(f'{name}: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))\n\n#åˆå§‹åŒ–éšæœºæ•°ç§å­\ndef init_seeds(seed=0):\n    # Initialize random number generator (RNG) seeds https://pytorch.org/docs/stable/notes/randomness.html\n    # cudnn seed 0 settings are slower and more reproducible, else faster and less reproducible\n    import torch.backends.cudnn as cudnn\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    cudnn.benchmark, cudnn.deterministic = (False, True) if seed == 0 else (True, False)\n\n# è·å–æœ€è¿‘è®­ç»ƒçš„æƒé‡æ–‡ä»¶ï¼Œlast.pt\ndef get_latest_run(search_dir='.'):\n    # Return path to most recent 'last.pt' in /runs (i.e. to --resume from)\n    last_list = glob.glob(f'{search_dir}/**/last*.pt', recursive=True)\n    return max(last_list, key=os.path.getctime) if last_list else ''\n\n\ndef user_config_dir(dir='Ultralytics', env_var='YOLOV5_CONFIG_DIR'):\n    # Return path of user configuration directory. Prefer environment variable if exists. Make dir if required.\n    env = os.getenv(env_var)\n    if env:\n        path = Path(env)  # use environment variable\n    else:\n        cfg = {'Windows': 'AppData/Roaming', 'Linux': '.config', 'Darwin': 'Library/Application Support'}  # 3 OS dirs\n        path = Path.home() / cfg.get(platform.system(), '')  # OS-specific config dir\n        path = (path if is_writeable(path) else Path('/tmp')) / dir  # GCP and AWS lambda fix, only /tmp is writeable\n    path.mkdir(exist_ok=True)  # make if required\n    return path\n\n\ndef is_writeable(dir, test=False):\n    # Return True if directory has write permissions, test opening a file with write permissions if test=True\n    if test:  # method 1\n        file = Path(dir) / 'tmp.txt'\n        try:\n            with open(file, 'w'):  # open file with write permissions\n                pass\n            file.unlink()  # remove file\n            return True\n        except IOError:\n            return False\n    else:  # method 2\n        return os.access(dir, os.R_OK)  # possible issues on Windows\n\n\ndef is_docker():\n    # Is environment a Docker container?\n    return Path('/workspace').exists()  # or Path('/.dockerenv').exists()\n\n\ndef is_colab():\n    # Is environment a Google Colab instance?\n    try:\n        import google.colab\n        return True\n    except ImportError:\n        return False\n\n\ndef is_pip():\n    # Is file in a pip package?\n    return 'site-packages' in Path(__file__).resolve().parts\n\n\ndef is_ascii(s=''):\n    # Is string composed of all ASCII (no UTF) characters? (note str().isascii() introduced in python 3.7)\n    s = str(s)  # convert list, tuple, None, etc. to str\n    return len(s.encode().decode('ascii', 'ignore')) == len(s)\n\n\ndef is_chinese(s='äººå·¥æ™ºèƒ½'):\n    # Is string composed of any Chinese characters?\n    return re.search('[\\u4e00-\\u9fff]', s)\n\n\ndef emojis(str=''):\n    # Return platform-dependent emoji-safe version of string\n    return str.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else str\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / 1E6\n    else:\n        return 0.0\n\n\ndef check_online():\n    # Check internet connectivity\n    import socket\n    try:\n        socket.create_connection((\"1.1.1.1\", 443), 5)  # check host accessibility\n        return True\n    except OSError:\n        return False\n\n\n@try_except\n#æ£€æŸ¥å½“å‰çš„åˆ†æ”¯å’ŒGitä¸Šçš„ç‰ˆæœ¬æ˜¯å¦ä¸€è‡´ï¼Œå¦åˆ™æé†’ç”¨æˆ·\ndef check_git_status():\n    # Recommend 'git pull' if code is out of date\n    msg = ', for updates see https://github.com/ultralytics/yolov5'\n    print(colorstr('github: '), end='')\n    assert Path('.git').exists(), 'skipping check (not a git repository)' + msg\n    assert not is_docker(), 'skipping check (Docker image)' + msg\n    assert check_online(), 'skipping check (offline)' + msg\n\n    cmd = 'git fetch &amp;&amp; git config --get remote.origin.url'\n    url = check_output(cmd, shell=True, timeout=5).decode().strip().rstrip('.git')  # git fetch\n    branch = check_output('git rev-parse --abbrev-ref HEAD', shell=True).decode().strip()  # checked out\n    n = int(check_output(f'git rev-list {branch}..origin/master --count', shell=True))  # commits behind\n    if n &gt; 0:\n        s = f\"âš ï¸ YOLOv5 is out of date by {n} commit{'s' * (n &gt; 1)}. Use `git pull` or `git clone {url}` to update.\"\n    else:\n        s = f'up to date with {url} âœ…'\n    print(emojis(s))  # emoji-safe\n\n\ndef check_python(minimum='3.6.2'):\n    # Check current python version vs. required python version\n    check_version(platform.python_version(), minimum, name='Python ')\n\n\ndef check_version(current='0.0.0', minimum='0.0.0', name='version ', pinned=False):\n    # Check version vs. required version\n    current, minimum = (pkg.parse_version(x) for x in (current, minimum))\n    result = (current == minimum) if pinned else (current &gt;= minimum)\n    assert result, f'{name}{minimum} required by YOLOv5, but {name}{current} is currently installed'\n\n\n@try_except\ndef check_requirements(requirements=ROOT / 'requirements.txt', exclude=(), install=True):\n    # Check installed dependencies meet requirements (pass *.txt file or list of packages)\n    prefix = colorstr('red', 'bold', 'requirements:')\n    check_python()  # check python version\n    if isinstance(requirements, (str, Path)):  # requirements.txt file\n        file = Path(requirements)\n        assert file.exists(), f\"{prefix} {file.resolve()} not found, check failed.\"\n        requirements = [f'{x.name}{x.specifier}' for x in pkg.parse_requirements(file.open()) if x.name not in exclude]\n    else:  # list or tuple of packages\n        requirements = [x for x in requirements if x not in exclude]\n\n    n = 0  # number of packages updates\n    for r in requirements:\n        try:\n            pkg.require(r)\n        except Exception as e:  # DistributionNotFound or VersionConflict if requirements not met\n            s = f\"{prefix} {r} not found and is required by YOLOv5\"\n            if install:\n                print(f\"{s}, attempting auto-update...\")\n                try:\n                    assert check_online(), f\"'pip install {r}' skipped (offline)\"\n                    print(check_output(f\"pip install '{r}'\", shell=True).decode())\n                    n += 1\n                except Exception as e:\n                    print(f'{prefix} {e}')\n            else:\n                print(f'{s}. Please install and rerun your command.')\n\n    if n:  # if packages updated\n        source = file.resolve() if 'file' in locals() else requirements\n        s = f\"{prefix} {n} package{'s' * (n &gt; 1)} updated per {source}\\n\" \\\n            f\"{prefix} âš ï¸ {colorstr('bold', 'Restart runtime or rerun command for updates to take effect')}\\n\"\n        print(emojis(s))\n\n#æ£€æŸ¥å›¾åƒçš„å°ºå¯¸æ˜¯å¦æ˜¯32çš„æ•´æ•°å€ã€‚å¦åˆ™è°ƒæ•´\ndef check_img_size(imgsz, s=32, floor=0):\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)\n    else:  # list i.e. img_size=[640, 480]\n        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]\n    if new_size != imgsz:\n        print(f'WARNING: --img-size {imgsz} must be multiple of max stride {s}, updating to {new_size}')\n    return new_size\n\n\ndef check_imshow():\n    # Check if environment supports image displays\n    try:\n        assert not is_docker(), 'cv2.imshow() is disabled in Docker environments'\n        assert not is_colab(), 'cv2.imshow() is disabled in Google Colab environments'\n        cv2.imshow('test', np.zeros((1, 1, 3)))\n        cv2.waitKey(1)\n        cv2.destroyAllWindows()\n        cv2.waitKey(1)\n        return True\n    except Exception as e:\n        print(f'WARNING: Environment does not support cv2.imshow() or PIL Image.show() image displays\\n{e}')\n        return False\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f\"{msg}{f} acceptable suffix is {suffix}\"\n\n\ndef check_yaml(file, suffix=('.yaml', '.yml')):\n    # Search/download YAML file (if necessary) and return path, checking suffix\n    return check_file(file, suffix)\n\n\ndef check_file(file, suffix=''):\n    # Search/download file (if necessary) and return path\n    check_suffix(file, suffix)  # optional\n    file = str(file)  # convert to str()\n    if Path(file).is_file() or file == '':  # exists\n        return file\n    elif file.startswith(('http:/', 'https:/')):  # download\n        url = str(Path(file)).replace(':/', '://')  # Pathlib turns :// -&gt; :/\n        file = Path(urllib.parse.unquote(file).split('?')[0]).name  # '%2F' to '/', split https://url.com/file.txt?auth\n        print(f'Downloading {url} to {file}...')\n        torch.hub.download_url_to_file(url, file)\n        assert Path(file).exists() and Path(file).stat().st_size &gt; 0, f'File download failed: {url}'  # check\n        return file\n    else:  # search\n        files = []\n        for d in 'data', 'models', 'utils':  # search directories\n            files.extend(glob.glob(str(ROOT / d / '**' / file), recursive=True))  # find file\n        assert len(files), f'File not found: {file}'  # assert file was found\n        assert len(files) == 1, f\"Multiple files match '{file}', specify exact path: {files}\"  # assert unique\n        return files[0]  # return file\n\n\ndef check_dataset(data, autodownload=True):\n    # Download and/or unzip dataset if not found locally\n    # Usage: https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128_with_yaml.zip\n\n    # Download (optional)\n    extract_dir = ''\n    if isinstance(data, (str, Path)) and str(data).endswith('.zip'):  # i.e. gs://bucket/dir/coco128.zip\n        download(data, dir='../datasets', unzip=True, delete=False, curl=False, threads=1)\n        data = next((Path('../datasets') / Path(data).stem).rglob('*.yaml'))\n        extract_dir, autodownload = data.parent, False\n\n    # Read yaml (optional)\n    if isinstance(data, (str, Path)):\n        with open(data, errors='ignore') as f:\n            data = yaml.safe_load(f)  # dictionary\n\n    # Parse yaml\n    path = extract_dir or Path(data.get('path') or '')  # optional 'path' default to '.'\n    for k in 'train', 'val', 'test':\n        if data.get(k):  # prepend path\n            data[k] = str(path / data[k]) if isinstance(data[k], str) else [str(path / x) for x in data[k]]\n\n    assert 'nc' in data, \"Dataset 'nc' key missing.\"\n    if 'names' not in data:\n        data['names'] = [f'class{i}' for i in range(data['nc'])]  # assign class names if missing\n    train, val, test, s = [data.get(x) for x in ('train', 'val', 'test', 'download')]\n    if val:\n        val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val path\n        if not all(x.exists() for x in val):\n            print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()])\n            if s and autodownload:  # download script\n                root = path.parent if 'path' in data else '..'  # unzip directory i.e. '../'\n                if s.startswith('http') and s.endswith('.zip'):  # URL\n                    f = Path(s).name  # filename\n                    print(f'Downloading {s} to {f}...')\n                    torch.hub.download_url_to_file(s, f)\n                    Path(root).mkdir(parents=True, exist_ok=True)  # create root\n                    ZipFile(f).extractall(path=root)  # unzip\n                    Path(f).unlink()  # remove zip\n                    r = None  # success\n                elif s.startswith('bash '):  # bash script\n                    print(f'Running {s} ...')\n                    r = os.system(s)\n                else:  # python script\n                    r = exec(s, {'yaml': data})  # return None\n                print(f\"Dataset autodownload {f'success, saved to {root}' if r in (0, None) else 'failure'}\\n\")\n            else:\n                raise Exception('Dataset not found.')\n\n    return data  # dictionary\n\n\ndef url2file(url):\n    # Convert URL to filename, i.e. https://url.com/file.txt?auth -&gt; file.txt\n    url = str(Path(url)).replace(':/', '://')  # Pathlib turns :// -&gt; :/\n    file = Path(urllib.parse.unquote(url)).name.split('?')[0]  # '%2F' to '/', split https://url.com/file.txt?auth\n    return file\n\n\ndef download(url, dir='.', unzip=True, delete=True, curl=False, threads=1):\n    # Multi-threaded file download and unzip function, used in data.yaml for autodownload\n    def download_one(url, dir):\n        # Download 1 file\n        f = dir / Path(url).name  # filename\n        if Path(url).is_file():  # exists in current path\n            Path(url).rename(f)  # move to dir\n        elif not f.exists():\n            print(f'Downloading {url} to {f}...')\n            if curl:\n                os.system(f\"curl -L '{url}' -o '{f}' --retry 9 -C -\")  # curl download, retry and resume on fail\n            else:\n                torch.hub.download_url_to_file(url, f, progress=True)  # torch download\n        if unzip and f.suffix in ('.zip', '.gz'):\n            print(f'Unzipping {f}...')\n            if f.suffix == '.zip':\n                ZipFile(f).extractall(path=dir)  # unzip\n            elif f.suffix == '.gz':\n                os.system(f'tar xfz {f} --directory {f.parent}')  # unzip\n            if delete:\n                f.unlink()  # remove zip\n\n    dir = Path(dir)\n    dir.mkdir(parents=True, exist_ok=True)  # make directory\n    if threads &gt; 1:\n        pool = ThreadPool(threads)\n        pool.imap(lambda x: download_one(*x), zip(url, repeat(dir)))  # multi-threaded\n        pool.close()\n        pool.join()\n    else:\n        for u in [url] if isinstance(url, (str, Path)) else url:\n            download_one(u, dir)\n\n\ndef make_divisible(x, divisor):\n    # Returns x evenly divisible by divisor\n    return math.ceil(x / divisor) * divisor\n\n\ndef clean_str(s):\n    # Cleans a string by replacing special characters with underscore _\n    return re.sub(pattern=\"[|@#!Â¡Â·$â‚¬%&amp;()=?Â¿^*;:,Â¨Â´&gt;&lt;+]\", repl=\"_\", string=s)\n\n\ndef one_cycle(y1=0.0, y2=1.0, steps=100):\n    # lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf\n    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1\n\n\ndef colorstr(*input):\n    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n    *args, string = input if len(input) &gt; 1 else ('blue', 'bold', input[0])  # color arguments, string\n    colors = {'black': '\\033[30m',  # basic colors\n              'red': '\\033[31m',\n              'green': '\\033[32m',\n              'yellow': '\\033[33m',\n              'blue': '\\033[34m',\n              'magenta': '\\033[35m',\n              'cyan': '\\033[36m',\n              'white': '\\033[37m',\n              'bright_black': '\\033[90m',  # bright colors\n              'bright_red': '\\033[91m',\n              'bright_green': '\\033[92m',\n              'bright_yellow': '\\033[93m',\n              'bright_blue': '\\033[94m',\n              'bright_magenta': '\\033[95m',\n              'bright_cyan': '\\033[96m',\n              'bright_white': '\\033[97m',\n              'end': '\\033[0m',  # misc\n              'bold': '\\033[1m',\n              'underline': '\\033[4m'}\n    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']\n\n\ndef labels_to_class_weights(labels, nc=80):\n    # Get class weights (inverse frequency) from training labels\n    if labels[0] is None:  # no labels loaded\n        return torch.Tensor()\n\n    labels = np.concatenate(labels, 0)  # labels.shape = (866643, 5) for COCO\n    classes = labels[:, 0].astype(np.int)  # labels = [class xywh]\n    weights = np.bincount(classes, minlength=nc)  # occurrences per class\n\n    # Prepend gridpoint count (for uCE training)\n    # gpi = ((320 / 32 * np.array([1, 2, 4])) ** 2 * 3).sum()  # gridpoints per image\n    # weights = np.hstack([gpi * len(labels)  - weights.sum() * 9, weights * 9]) ** 0.5  # prepend gridpoints to start\n\n    weights[weights == 0] = 1  # replace empty bins with 1\n    weights = 1 / weights  # number of targets per class\n    weights /= weights.sum()  # normalize\n    return torch.from_numpy(weights)\n\n\ndef labels_to_image_weights(labels, nc=80, class_weights=np.ones(80)):\n    # Produces image weights based on class_weights and image contents\n    class_counts = np.array([np.bincount(x[:, 0].astype(np.int), minlength=nc) for x in labels])\n    image_weights = (class_weights.reshape(1, nc) * class_counts).sum(1)\n    # index = random.choices(range(n), weights=image_weights, k=1)  # weight image sample\n    return image_weights\n\n\ndef coco80_to_coco91_class():  # converts 80-index (val2014) to 91-index (paper)\n    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\\n')\n    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\\n')\n    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco\n    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n    return x\n\n\ndef xyxy2xywh(x):\n    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n    y[:, 2] = x[:, 2] - x[:, 0]  # width\n    y[:, 3] = x[:, 3] - x[:, 1]  # height\n    return y\n\n\ndef xywh2xyxy(x):\n    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n    return y\n\n\ndef xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):\n    # Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + padw  # top left x\n    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + padh  # top left y\n    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + padw  # bottom right x\n    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + padh  # bottom right y\n    return y\n\n\ndef xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n    if clip:\n        clip_coords(x, (h - eps, w - eps))  # warning: inplace clip\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n    return y\n\n\ndef xyn2xy(x, w=640, h=640, padw=0, padh=0):\n    # Convert normalized segments into pixel segments, shape (n,2)\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = w * x[:, 0] + padw  # top left x\n    y[:, 1] = h * x[:, 1] + padh  # top left y\n    return y\n\n\ndef segment2box(segment, width=640, height=640):\n    # Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)\n    x, y = segment.T  # segment xy\n    inside = (x &gt;= 0) &amp; (y &gt;= 0) &amp; (x &lt;= width) &amp; (y &lt;= height)\n    x, y, = x[inside], y[inside]\n    return np.array([x.min(), y.min(), x.max(), y.max()]) if any(x) else np.zeros((1, 4))  # xyxy\n\n\ndef segments2boxes(segments):\n    # Convert segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh)\n    boxes = []\n    for s in segments:\n        x, y = s.T  # segment xy\n        boxes.append([x.min(), y.min(), x.max(), y.max()])  # cls, xyxy\n    return xyxy2xywh(np.array(boxes))  # cls, xywh\n\n\ndef resample_segments(segments, n=1000):\n    # Up-sample an (n,2) segment\n    for i, s in enumerate(segments):\n        x = np.linspace(0, len(s) - 1, n)\n        xp = np.arange(len(s))\n        segments[i] = np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)]).reshape(2, -1).T  # segment xy\n    return segments\n\n\ndef scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n    # Rescale coords (xyxy) from img1_shape to img0_shape\n    if ratio_pad is None:  # calculate from img0_shape\n        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n    else:\n        gain = ratio_pad[0][0]\n        pad = ratio_pad[1]\n\n    coords[:, [0, 2]] -= pad[0]  # x padding\n    coords[:, [1, 3]] -= pad[1]  # y padding\n    coords[:, :4] /= gain\n    clip_coords(coords, img0_shape)\n    return coords\n\n\ndef clip_coords(boxes, shape):\n    # Clip bounding xyxy bounding boxes to image shape (height, width)\n    if isinstance(boxes, torch.Tensor):  # faster individually\n        boxes[:, 0].clamp_(0, shape[1])  # x1\n        boxes[:, 1].clamp_(0, shape[0])  # y1\n        boxes[:, 2].clamp_(0, shape[1])  # x2\n        boxes[:, 3].clamp_(0, shape[0])  # y2\n    else:  # np.array (faster grouped)\n        boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, shape[1])  # x1, x2\n        boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, shape[0])  # y1, y2\n\n#éæå¤§å€¼æŠ‘åˆ¶\ndef non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n                        labels=(), max_det=300):\n    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n\n    Returns:\n         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n    \"\"\"\n\n    nc = prediction.shape[2] - 5  # number of classes\n    xc = prediction[..., 4] &gt; conf_thres  # candidates\n\n    # Checks\n    assert 0 &lt;= conf_thres &lt;= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n    assert 0 &lt;= iou_thres &lt;= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n\n    # Settings\n    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n    time_limit = 10.0  # seconds to quit after\n    redundant = True  # require redundant detections\n    multi_label &amp;= nc &gt; 1  # multiple labels per box (adds 0.5ms/img)\n    merge = False  # use merge-NMS\n\n    t = time.time()\n    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n    for xi, x in enumerate(prediction):  # image index, image inference\n        # Apply constraints\n        # x[((x[..., 2:4] &lt; min_wh) | (x[..., 2:4] &gt; max_wh)).any(1), 4] = 0  # width-height\n        x = x[xc[xi]]  # confidence\n\n        # Cat apriori labels if autolabelling\n        if labels and len(labels[xi]):\n            l = labels[xi]\n            v = torch.zeros((len(l), nc + 5), device=x.device)\n            v[:, :4] = l[:, 1:5]  # box\n            v[:, 4] = 1.0  # conf\n            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n            x = torch.cat((x, v), 0)\n\n        # If none remain process next image\n        if not x.shape[0]:\n            continue\n\n        # Compute conf\n        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n\n        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n        box = xywh2xyxy(x[:, :4])\n\n        # Detections matrix nx6 (xyxy, conf, cls)\n        if multi_label:\n            i, j = (x[:, 5:] &gt; conf_thres).nonzero(as_tuple=False).T\n            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n        else:  # best class only\n            conf, j = x[:, 5:].max(1, keepdim=True)\n            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) &gt; conf_thres]\n\n        # Filter by class\n        if classes is not None:\n            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n\n        # Apply finite constraint\n        # if not torch.isfinite(x).all():\n        #     x = x[torch.isfinite(x).all(1)]\n\n        # Check shape\n        n = x.shape[0]  # number of boxes\n        if not n:  # no boxes\n            continue\n        elif n &gt; max_nms:  # excess boxes\n            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n\n        # Batched NMS\n        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n        if i.shape[0] &gt; max_det:  # limit detections\n            i = i[:max_det]\n        if merge and (1 &lt; n &lt; 3E3):  # Merge NMS (boxes merged using weighted mean)\n            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n            iou = box_iou(boxes[i], boxes) &gt; iou_thres  # iou matrix\n            weights = iou * scores[None]  # box weights\n            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n            if redundant:\n                i = i[iou.sum(1) &gt; 1]  # require redundancy\n\n        output[xi] = x[i]\n        if (time.time() - t) &gt; time_limit:\n            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n            break  # time limit exceeded\n\n    return output\n\n\ndef strip_optimizer(f='best.pt', s=''):  # from utils.general import *; strip_optimizer()\n    # Strip optimizer from 'f' to finalize training, optionally save as 's'\n    x = torch.load(f, map_location=torch.device('cpu'))\n    if x.get('ema'):\n        x['model'] = x['ema']  # replace model with ema\n    for k in 'optimizer', 'training_results', 'wandb_id', 'ema', 'updates':  # keys\n        x[k] = None\n    x['epoch'] = -1\n    x['model'].half()  # to FP16\n    for p in x['model'].parameters():\n        p.requires_grad = False\n    torch.save(x, s or f)\n    mb = os.path.getsize(s or f) / 1E6  # filesize\n    print(f\"Optimizer stripped from {f},{(' saved as %s,' % s) if s else ''} {mb:.1f}MB\")\n\n\ndef print_mutation(results, hyp, save_dir, bucket):\n    evolve_csv, results_csv, evolve_yaml = save_dir / 'evolve.csv', save_dir / 'results.csv', save_dir / 'hyp_evolve.yaml'\n    keys = ('metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',\n            'val/box_loss', 'val/obj_loss', 'val/cls_loss') + tuple(hyp.keys())  # [results + hyps]\n    keys = tuple(x.strip() for x in keys)\n    vals = results + tuple(hyp.values())\n    n = len(keys)\n\n    # Download (optional)\n    if bucket:\n        url = f'gs://{bucket}/evolve.csv'\n        if gsutil_getsize(url) &gt; (os.path.getsize(evolve_csv) if os.path.exists(evolve_csv) else 0):\n            os.system(f'gsutil cp {url} {save_dir}')  # download evolve.csv if larger than local\n\n    # Log to evolve.csv\n    s = '' if evolve_csv.exists() else (('%20s,' * n % keys).rstrip(',') + '\\n')  # add header\n    with open(evolve_csv, 'a') as f:\n        f.write(s + ('%20.5g,' * n % vals).rstrip(',') + '\\n')\n\n    # Print to screen\n    print(colorstr('evolve: ') + ', '.join(f'{x.strip():&gt;20s}' for x in keys))\n    print(colorstr('evolve: ') + ', '.join(f'{x:20.5g}' for x in vals), end='\\n\\n\\n')\n\n    # Save yaml\n    with open(evolve_yaml, 'w') as f:\n        data = pd.read_csv(evolve_csv)\n        data = data.rename(columns=lambda x: x.strip())  # strip keys\n        i = np.argmax(fitness(data.values[:, :7]))  #\n        f.write('# YOLOv5 Hyperparameter Evolution Results\\n' +\n                f'# Best generation: {i}\\n' +\n                f'# Last generation: {len(data)}\\n' +\n                '# ' + ', '.join(f'{x.strip():&gt;20s}' for x in keys[:7]) + '\\n' +\n                '# ' + ', '.join(f'{x:&gt;20.5g}' for x in data.values[i, :7]) + '\\n\\n')\n        yaml.safe_dump(hyp, f, sort_keys=False)\n\n    if bucket:\n        os.system(f'gsutil cp {evolve_csv} {evolve_yaml} gs://{bucket}')  # upload\n\n\ndef apply_classifier(x, model, img, im0):\n    # Apply a second stage classifier to yolo outputs\n    im0 = [im0] if isinstance(im0, np.ndarray) else im0\n    for i, d in enumerate(x):  # per image\n        if d is not None and len(d):\n            d = d.clone()\n\n            # Reshape and pad cutouts\n            b = xyxy2xywh(d[:, :4])  # boxes\n            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square\n            b[:, 2:] = b[:, 2:] * 1.3 + 30  # pad\n            d[:, :4] = xywh2xyxy(b).long()\n\n            # Rescale boxes from img_size to im0 size\n            scale_coords(img.shape[2:], d[:, :4], im0[i].shape)\n\n            # Classes\n            pred_cls1 = d[:, 5].long()\n            ims = []\n            for j, a in enumerate(d):  # per item\n                cutout = im0[i][int(a[1]):int(a[3]), int(a[0]):int(a[2])]\n                im = cv2.resize(cutout, (224, 224))  # BGR\n                # cv2.imwrite('example%i.jpg' % j, cutout)\n\n                im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n                im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 to float32\n                im /= 255.0  # 0 - 255 to 0.0 - 1.0\n                ims.append(im)\n\n            pred_cls2 = model(torch.Tensor(ims).to(d.device)).argmax(1)  # classifier prediction\n            x[i] = x[i][pred_cls1 == pred_cls2]  # retain matching class detections\n\n    return x\n\n\ndef save_one_box(xyxy, im, file='image.jpg', gain=1.02, pad=10, square=False, BGR=False, save=True):\n    # Save image crop as {file} with crop size multiple {gain} and {pad} pixels. Save and/or return crop\n    xyxy = torch.tensor(xyxy).view(-1, 4)\n    b = xyxy2xywh(xyxy)  # boxes\n    if square:\n        b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # attempt rectangle to square\n    b[:, 2:] = b[:, 2:] * gain + pad  # box wh * gain + pad\n    xyxy = xywh2xyxy(b).long()\n    clip_coords(xyxy, im.shape)\n    crop = im[int(xyxy[0, 1]):int(xyxy[0, 3]), int(xyxy[0, 0]):int(xyxy[0, 2]), ::(1 if BGR else -1)]\n    if save:\n        cv2.imwrite(str(increment_path(file, mkdir=True).with_suffix('.jpg')), crop)\n    return crop\n\n\ndef increment_path(path, exist_ok=False, sep='', mkdir=False):\n    # Increment file or directory path, i.e. runs/exp --&gt; runs/exp{sep}2, runs/exp{sep}3, ... etc.\n    path = Path(path)  # os-agnostic\n    if path.exists() and not exist_ok:\n        suffix = path.suffix\n        path = path.with_suffix('')\n        dirs = glob.glob(f\"{path}{sep}*\")  # similar paths\n        matches = [re.search(rf\"%s{sep}(\\d+)\" % path.stem, d) for d in dirs]\n        i = [int(m.groups()[0]) for m in matches if m]  # indices\n        n = max(i) + 1 if i else 2  # increment number\n        path = Path(f\"{path}{sep}{n}{suffix}\")  # update path\n    dir = path if path.suffix == '' else path.parent  # directory\n    if not dir.exists() and mkdir:\n        dir.mkdir(parents=True, exist_ok=True)  # make directory\n    return path\n\n</code></pre>\n<h3><a id=\"losspy__3449\"></a>loss.py ç›¸å…³æŸå¤±å‡½æ•°</h3>\n<p><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"..\\..\\static\\image\\c19626ced1474fa0a5283f1cedeeffeb.png\"/><br/> <img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"..\\..\\static\\image\\3f4b7c1885de4594a6a4eac191d64f38.png\"/></p>\n<p><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"..\\..\\static\\image\\30cdaafcae2f4948864506fc4d98ed6f.png\"/><br/> <img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"..\\..\\static\\image\\903052f02c9a4f3eaaee488709e6c1dc.png\"/></p>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nLoss functions\n\"\"\"\n\nimport torch\nimport torch.nn as nn\n\nfrom utils.metrics import bbox_iou\nfrom utils.torch_utils import is_parallel\n\n\ndef smooth_BCE(eps=0.1):  # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441\n    # return positive, negative label smoothing BCE targets\n    return 1.0 - 0.5 * eps, 0.5 * eps\n\n\nclass BCEBlurWithLogitsLoss(nn.Module):\n    # BCEwithLogitLoss() with reduced missing label effects.\n    def __init__(self, alpha=0.05):\n        super(BCEBlurWithLogitsLoss, self).__init__()\n        self.loss_fcn = nn.BCEWithLogitsLoss(reduction='none')  # must be nn.BCEWithLogitsLoss()\n        self.alpha = alpha\n\n    def forward(self, pred, true):\n        loss = self.loss_fcn(pred, true)\n        pred = torch.sigmoid(pred)  # prob from logits\n        dx = pred - true  # reduce only missing label effects\n        # dx = (pred - true).abs()  # reduce missing label and false label effects\n        alpha_factor = 1 - torch.exp((dx - 1) / (self.alpha + 1e-4))\n        loss *= alpha_factor\n        return loss.mean()\n\n\nclass FocalLoss(nn.Module):\n    # Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)\n    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):\n        super(FocalLoss, self).__init__()\n        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = loss_fcn.reduction\n        self.loss_fcn.reduction = 'none'  # required to apply FL to each element\n\n    def forward(self, pred, true):\n        loss = self.loss_fcn(pred, true)\n        # p_t = torch.exp(-loss)\n        # loss *= self.alpha * (1.000001 - p_t) ** self.gamma  # non-zero power for gradient stability\n\n        # TF implementation https://github.com/tensorflow/addons/blob/v0.7.1/tensorflow_addons/losses/focal_loss.py\n        pred_prob = torch.sigmoid(pred)  # prob from logits\n        p_t = true * pred_prob + (1 - true) * (1 - pred_prob)\n        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)\n        modulating_factor = (1.0 - p_t) ** self.gamma\n        loss *= alpha_factor * modulating_factor\n\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:  # 'none'\n            return loss\n\n\nclass QFocalLoss(nn.Module):\n    # Wraps Quality focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)\n    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):\n        super(QFocalLoss, self).__init__()\n        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = loss_fcn.reduction\n        self.loss_fcn.reduction = 'none'  # required to apply FL to each element\n\n    def forward(self, pred, true):\n        loss = self.loss_fcn(pred, true)\n\n        pred_prob = torch.sigmoid(pred)  # prob from logits\n        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)\n        modulating_factor = torch.abs(true - pred_prob) ** self.gamma\n        loss *= alpha_factor * modulating_factor\n\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:  # 'none'\n            return loss\n\n\nclass ComputeLoss:\n    # Compute losses\n    def __init__(self, model, autobalance=False):\n        self.sort_obj_iou = False\n        #è·å–è®¾å¤‡\n        device = next(model.parameters()).device  # get model device\n        #è·å–è¶…å‚æ•°\n        h = model.hyp  # hyperparameters\n\n        # Define criteria\n        #å®šä¹‰ç±»åˆ«å’Œç›®æ ‡æ€§å¾—åˆ†å‡½æ•°\n        BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['cls_pw']], device=device))\n        BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['obj_pw']], device=device))\n\n        # Class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3\n        #æ ‡ç­¾å¹³æ»‘ï¼Œespé»˜è®¤æ˜¯0ï¼Œå…¶å®æ²¡æœ‰ç”¨åˆ°\n        self.cp, self.cn = smooth_BCE(eps=h.get('label_smoothing', 0.0))  # positive, negative BCE targets\n\n        # Focal loss\n        g = h['fl_gamma']  # focal loss gamma\n        if g &gt; 0:\n            BCEcls, BCEobj = FocalLoss(BCEcls, g), FocalLoss(BCEobj, g)\n\n        det = model.module.model[-1] if is_parallel(model) else model.model[-1]  # Detect() module\n        self.balance = {3: [4.0, 1.0, 0.4]}.get(det.nl, [4.0, 1.0, 0.25, 0.06, .02])  # P3-P7\n        self.ssi = list(det.stride).index(16) if autobalance else 0  # stride 16 index\n        self.BCEcls, self.BCEobj, self.gr, self.hyp, self.autobalance = BCEcls, BCEobj, 1.0, h, autobalance\n        for k in 'na', 'nc', 'nl', 'anchors':\n            setattr(self, k, getattr(det, k))\n\n    def __call__(self, p, targets):  # predictions, targets, model\n        device = targets.device\n        #åˆå§‹åŒ–å„ä¸ªéƒ¨åˆ†æŸå¤±\n        lcls, lbox, lobj = torch.zeros(1, device=device), torch.zeros(1, device=device), torch.zeros(1, device=device)\n        #è·å¾—æ ‡ç­¾åˆ†ç±»ï¼Œè¾¹æ¡†ï¼Œç´¢å¼•ï¼Œanchors\n        tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets\n\n        # Losses\n        for i, pi in enumerate(p):  # layer index, layer predictions\n            #æ ¹æ®indices è·å–æŸå¤±ï¼Œæ–¹ä¾¿æ‰¾åˆ°å¯¹äºçš„ç½‘æ ¼è¾“å‡º\n            b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx\n            tobj = torch.zeros_like(pi[..., 0], device=device)  # target obj\n\n            n = b.shape[0]  # number of targets\n            if n:\n                #æ‰¾åˆ°å¯¹äºçš„ç½‘æ ¼è¾“å‡ºï¼Œå–å‡ºå¯¹åº”çš„ä½ç½®é¢„æµ‹å€¼\n                ps = pi[b, a, gj, gi]  # prediction subset corresponding to targets\n\n                # Regression\n                #å¯¹è¾“å‡ºçš„xywhåšåç®—\n                pxy = ps[:, :2].sigmoid() * 2. - 0.5\n                pwh = (ps[:, 2:4].sigmoid() * 2) ** 2 * anchors[i]\n                pbox = torch.cat((pxy, pwh), 1)  # predicted box\n                #è®¡ç®—è¾¹æ¡†æŸå¤±ï¼Œæ³¨æ„è¿™ä¸ªCIOu = True,è®¡ç®—çš„æ˜¯ciouæŸå¤±\n                iou = bbox_iou(pbox.T, tbox[i], x1y1x2y2=False, CIoU=True)  # iou(prediction, target)\n                lbox += (1.0 - iou).mean()  # iou loss\n\n                # Objectness\n                #æ ¹æ®model.grè®¾ç½®objectnessçš„æ ‡ç­¾å€¼ï¼Œæœ‰ç›®æ ‡çš„confåˆ†æ”¯æƒé‡\n                #ä¸åŒçš„anchorå’Œgt bboxåŒ¹é…åº¦ä¸ä¸€æ ·ï¼Œé¢„æµ‹æ¡†å’Œgt bbox çš„åŒ¹é…åº¦ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚æœæƒé‡è®¾ç½®ä¸€æ ·è‚¯å®šä¸æ˜¯æœ€ä¼˜çš„\n                #æ•…å°†é¢„æµ‹æ¡†å’Œè¢«å‰¥å‰Šçš„iouä½œä¸ºæƒé‡ä¹˜åˆ°confåˆ†æ”¯ï¼Œç”¨æ¥è¡¨å¾æµ‹é‡è´¨é‡\n                score_iou = iou.detach().clamp(0).type(tobj.dtype)\n                if self.sort_obj_iou:\n                    sort_id = torch.argsort(score_iou)\n                    b, a, gj, gi, score_iou = b[sort_id], a[sort_id], gj[sort_id], gi[sort_id], score_iou[sort_id]\n                tobj[b, a, gj, gi] = (1.0 - self.gr) + self.gr * score_iou  # iou ratio\n\n                # Classification\n                #è®¾ç½®å¦‚æœç±»åˆ«æ•°å¤§äº1ï¼Œæ‰è®¡ç®—åˆ†ç±»æŸå¤±\n                if self.nc &gt; 1:  # cls loss (only if multiple classes)\n                    t = torch.full_like(ps[:, 5:], self.cn, device=device)  # targets\n                    t[range(n), tcls[i]] = self.cp\n                    lcls += self.BCEcls(ps[:, 5:], t)  # BCEï¼Œæ¯ä¸ªç±»å•ç‹¬è®¡ç®—loss\n\n                # Append targets to text file\n                # with open('targets.txt', 'a') as file:\n                #     [file.write('%11.5g ' * 4 % tuple(x) + '\\n') for x in torch.cat((txy[i], twh[i]), 1)]\n            #è®¡ç®—objectnessçš„æŸå¤±\n            obji = self.BCEobj(pi[..., 4], tobj)\n            lobj += obji * self.balance[i]  # obj loss\n            if self.autobalance:\n                self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()\n\n        if self.autobalance:\n            self.balance = [x / self.balance[self.ssi] for x in self.balance]\n        #æ ¹æ®è¶…å‚æ•°è®¾ç½®çš„å„ä¸ªéƒ¨åˆ†çš„æŸå¤±ç³»æ•°è·å–æœ€ç»ˆçš„æŸå¤±\n        lbox *= self.hyp['box']\n        lobj *= self.hyp['obj']\n        lcls *= self.hyp['cls']\n        bs = tobj.shape[0]  # batch size\n\n        return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach()\n    '''\n    build_targetå‡½æ•°ç”¨äºåœ¨è·å¾—è®­ç»ƒæ—¶è®¡ç®—losså‡½æ•°æ‰€éœ€è¦çš„çš„ç›®æ ‡æ¡†ï¼Œå³è¢«è®¤ä¸ºæ˜¯æ­£æ ·æœ¬\n    v5æ”¯æŒè·¨ç½‘ç»œé¢„æµ‹\n    å¯¹äºä»»ä½•ä¸€ä¸ªboxï¼Œä¸‰ä¸ªé¢„æµ‹è¾“å…¥æ¡†å±‚éƒ½æœ‰å¯èƒ½ä¸å…ˆéªŒæ¡†åŒ¹é…\n    è¯¥å‡½æ•°çš„è¾“å‡ºæ­£æ ·æœ¬ç›¸æ¯”äºä¼ å…¥çš„targetæ•°ç›®å¤š\n    '''\n    def build_targets(self, p, targets):\n        '''\n        :param p: ç½‘ç»œè¾“å‡º\n        :param targets: GTæ¡†ï¼Œtargets.shape= (nt,6),6:icxywh,iè¡¨ç¤ºç¬¬i+1å¼ å›¾ç‰‡ï¼Œcè¡¨ç¤ºç±»åˆ«ï¼Œåæ ‡xywh\n        :return:\n        '''\n        # Build targets for compute_loss(), input targets(image,class,x,y,w,h)\n        #anchoræ•°é‡å’Œæ ‡ç­¾æ¡†çš„æ•°é‡\n        #target nx6ï¼Œ å…¶ä¸­n æ˜¯batchå†…æ‰€æœ‰çš‚ç‰‡labelæ‹¼æ¥æˆçš„\n        #6çš„ç¬¬0ç»´è¡¨ç¤ºå½“å‰æ˜¯ç¬¬å‡ å¼ å›¾ç‰‡çš„label = indexï¼Œåé¢æ˜¯classid_xywh\n        na, nt = self.na, targets.shape[0]  # number of anchors, targets\n        tcls, tbox, indices, anch = [], [], [], []\n        gain = torch.ones(7, device=targets.device)  # normalized to gridspace gain\n        #ai.shape = (na,nt) ç”Ÿæˆanchorçš„æ‰€æœ‰\n        #anchorç´¢å¼•ï¼Œåé¢æœ‰ç”¨ï¼Œç”¨äºè¡¨ç¤ºå½“å‰bboxå’Œå½“å‰å±‚çš„å“ªä¸ªanchoråŒ¹é…\n        ai = torch.arange(na, device=targets.device).float().view(na, 1).repeat(1, nt)  # same as .repeat_interleave(nt)\n        #å…ˆrespeat targets å½“å‰å±‚anchorä¸ªæ•°ä¸€æ ·ï¼Œç›¸å½“äºæ¯ä¸ªbboxå˜æˆäº†ä¸‰ä¸ªï¼Œç„¶åå’Œ3ä¸ªanchorå•ç‹¬åŒ¹é…\n        targets = torch.cat((targets.repeat(na, 1, 1), ai[:, :, None]), 2)  # append anchor indices\n\n        #è®¾ç½®ç½‘ç»œä¸­å¿ƒåç§»é‡\n        g = 0.5  # bias\n        #é™„è¿‘4ä¸ªç½‘ç»œ\n        off = torch.tensor([[0, 0],\n                            [1, 0], [0, 1], [-1, 0], [0, -1],  # j,k,l,m\n                            # [1, 1], [1, -1], [-1, 1], [-1, -1],  # jk,jm,lk,lm\n                            ], device=targets.device).float() * g  # offsets\n\n        #å¯¹æ¯ä¸ªæ£€æŸ¥å±‚è¿›è¡Œå¤„ç†\n        for i in range(self.nl):\n            anchors = self.anchors[i]\n            #p æ˜¯ç½‘ç»œè¾“å‡ºå€¼\n            gain[2:6] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain\n\n            # Match targets to anchors\n            #å°†æ ‡ç­¾æ¡†çš„xywhä»åŸºäº0-1æ˜ å°„åˆ°åŸºäºç‰¹å¾å›¾ï¼štargetçš„xywhæœ¬èº«æ˜¯å½’ä¸€åŒ–å°ºåº¦ï¼Œæ•…éœ€è¦å˜æˆç‰¹å¾å›¾å°ºåº¦\n            t = targets * gain\n            #å¯¹æ¯ä¸ªè¾“å‡ºå±‚å•ç‹¬åŒ¹é…\n            #é¦–å…ˆå°†targetså˜æˆanchorå°ºåº¦ï¼Œæ–¹ä¾¿è®¡ç®—\n            if nt:\n                # Matches\n                #é¢„æµ‹çš„whä¸anchorçš„whåšåŒ¹é…ï¼Œç­›é€‰æ‰æ¯”å€¼å¤§äºhyp['anchor_t']çš„ï¼Œä»è€Œæ›´å¥½çš„å›å½’\n                #ä½œè€…é‡‡ç”¨æ›´å¥½çš„å›å½’æ–¹å¼ï¼šï¼ˆwh.sigmoid() * 2ï¼‰** 2 * anchors[i]\n                #åŸæ¥çš„ä¸º anchors[i] *expï¼ˆwhï¼‰\n                #å°†æ ‡ç­¾æ¡†ä¸anchorçš„å€æ•°æ§åˆ¶åœ¨0-4ä¹‹é—´ï¼Œhyp.scratch.yamlä¸­çš„è¶…å‚æ•°anchors_t =4 ,ç”¨äºåˆ¤å®šanchorSä¸æ ‡ç­¾æ¡†çš„å¥‘åˆåº¦\n\n                #è®¡ç®—å½“å‰çš„targetçš„wh å’Œanchorçš„whæ¯”ä¾‹å€¼\n                #å¦‚æœæœ€å¤§æ¯”ä¾‹å¤§äºé˜ˆå€¼model.hyp['anchor_t']=4,åˆ™å½“å‰targetå’Œanchorçš„åŒ¹é…åº¦ä¸å¤Ÿé«˜ï¼Œä¸å¼ºåˆ¶å›å½’ï¼Œè€Œå°†targetä¸¢å¼ƒ\n                #è®¡ç®—æ¯”å€¼ratio\n                r = t[:, :, 4:6] / anchors[:, None]  # wh ratio  ï¼Œä¸è€ƒè™‘xyåæ ‡\n                #ç­›é€‰æ»¡è¶³ 1/ hyp['anchor_t'] &lt; targets_wh / anchor_wh &lt;hyp['anchor_t']çš„æ¡†\n                j = torch.max(r, 1. / r).max(2)[0] &lt; self.hyp['anchor_t']  # compare\n                # j = wh_iou(anchors, t[:, 4:6]) &gt; model.hyp['iou_t']  # iou(3,n)=wh_iou(anchors(3,2), gwh(n,2))\n                #ç­›é€‰è¿‡åçš„t.shape = (M,7),Mï¼Œä¸ºè¿‡æ»¤åçš„æ•°é‡\n                t = t[j]  # filter\n\n                # Offsets\n                gxy = t[:, 2:4]  # grid xy  labelçš„ä¸­å¿ƒåæ ‡\n                #å¾—åˆ°ä¸­å¿ƒåæ ‡ç›¸å¯¹äºå½“å‰ç‰¹å¾å›¾çš„åæ ‡ï¼Œ ï¼ˆM,2ï¼‰\n                gxi = gain[[2, 3]] - gxy  # inverse\n                #å¯¹äºç­›é€‰åçš„bbox,è®¡ç®—å…¶è½åœ¨å“ªä¸ªç½‘æ ¼å†…ï¼ŒåŒæ—¶æ‰¾å‡ºç´§é‚»çš„ç½‘æ ¼ï¼Œå°†è¿™äº›ç½‘æ ¼éƒ½è®¤ä¸ºæ˜¯è´Ÿè´£é¢„æµ‹è¯¥bboxçš„ç½‘æ ¼\n                #æµ®ç‚¹æ•°å–æ¨¡çš„æ•°å­¦å®šä¹‰ï¼Œå¯¹äºä¸¤ä¸ªæµ®ç‚¹æ•°aå’Œb,a%b = a - n*b,å…¶ä¸­nä¸ºä¸è¶…è¿‡a/b çš„æœ€å¤§æ•´æ•°\n                j, k = ((gxy % 1. &lt; g) &amp; (gxy &gt; 1.)).T\n                l, m = ((gxi % 1. &lt; g) &amp; (gxi &gt; 1.)).T\n                #j.shape = (5ï¼ŒM)\n                j = torch.stack((torch.ones_like(j), j, k, l, m))\n                #é¢„è®¾çš„off = 5\n                t = t.repeat((5, 1, 1))[j]\n                #æ·»åŠ åç§»é‡\n                offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]#é€‰å‡º5ä¸ªæœ€è¿‘çš„ï¼ŒåŒ…æ‹¬è‡ªå·±\n            else:\n                t = targets[0]\n                offsets = 0\n\n            # Define\n            b, c = t[:, :2].long().T  # image, class\n            gxy = t[:, 2:4]  # grid xy  labelä¸­å¿ƒç‚¹åæ ‡\n            #å®½é«˜å›å½’æ ‡ç­¾\n            gwh = t[:, 4:6]  # grid wh\n            #å½“å‰labelè½åœ¨å“ªä¸€ä¸ªæ ‡ç­¾ä¸Š\n            gij = (gxy - offsets).long()\n            gi, gj = gij.T  # grid xy indicesï¼ˆç´¢å¼•å€¼ï¼‰\n\n            # Append\n            a = t[:, 6].long()  # anchor indicesï¼Œanchorçš„ç´¢å¼•\n            #æ·»åŠ ç´¢å¼•æ–¹ä¾¿ï¼Œè®¡ç®—æŸå¤±çš„æ—¶å€™å–å‡ºå¯¹äºä½ç½®çš„åæ ‡\n            indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))  # image, anchor, grid indices\n            tbox.append(torch.cat((gxy - gij, gwh), 1))  # box åæ ‡å€¼\n            anch.append(anchors[a])  # anchors å°ºå¯¸\n            tcls.append(c)  # class\n\n        return tcls, tbox, indices, anch\n\n\n</code></pre>\n<h3><a id=\"metricspy___3740\"></a>metrics.py è®¡ç®—æ€§èƒ½æŒ‡æ ‡çš„ç›¸å…³å‡½æ•°</h3>\n<p><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"..\\..\\static\\image\\a3185d5894b64cb1b4dbe1feb0ad51eb.png\"/></p>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nModel validation metrics\n\"\"\"\n\nimport math\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n\ndef fitness(x):\n    # Model fitness as a weighted combination of metrics\n    w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\n    return (x[:, :4] * w).sum(1)\n\n#è®¡ç®—ç±»åˆ«çš„ap(p,r,f1)\ndef ap_per_class(tp, conf, pred_cls, target_cls, plot=False, save_dir='.', names=()):\n    \"\"\" Compute the average precision, given the recall and precision curves.\n    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n    # Arguments\n        tp:  True positives (nparray, nx1 or nx10).\n        conf:  Objectness value from 0-1 (nparray).\n        pred_cls:  Predicted object classes (nparray).\n        target_cls:  True object classes (nparray).\n        plot:  Plot precision-recall curve at mAP@0.5\n        save_dir:  Plot save directory\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    \"\"\"\n\n    # Sort by objectness\n    i = np.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n    # Find unique classes\n    unique_classes = np.unique(target_cls)\n    nc = unique_classes.shape[0]  # number of classes, number of detections\n\n    # Create Precision-Recall curve and compute AP for each class\n    px, py = np.linspace(0, 1, 1000), []  # for plotting\n    ap, p, r = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n    for ci, c in enumerate(unique_classes):\n        i = pred_cls == c\n        n_l = (target_cls == c).sum()  # number of labels\n        n_p = i.sum()  # number of predictions\n\n        if n_p == 0 or n_l == 0:\n            continue\n        else:\n            # Accumulate FPs and TPs\n            fpc = (1 - tp[i]).cumsum(0)\n            tpc = tp[i].cumsum(0)\n\n            # Recall\n            recall = tpc / (n_l + 1e-16)  # recall curve\n            r[ci] = np.interp(-px, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n\n            # Precision\n            precision = tpc / (tpc + fpc)  # precision curve\n            p[ci] = np.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n\n            # AP from recall-precision curve\n            for j in range(tp.shape[1]):\n                ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n                if plot and j == 0:\n                    py.append(np.interp(px, mrec, mpre))  # precision at mAP@0.5\n\n    # Compute F1 (harmonic mean of precision and recall)\n    f1 = 2 * p * r / (p + r + 1e-16)\n    names = [v for k, v in names.items() if k in unique_classes]  # list: only classes that have data\n    names = {i: v for i, v in enumerate(names)}  # to dict\n    if plot:#ç»˜åˆ¶p-ræ›²çº¿\n        plot_pr_curve(px, py, ap, Path(save_dir) / 'PR_curve.png', names)\n        plot_mc_curve(px, f1, Path(save_dir) / 'F1_curve.png', names, ylabel='F1')\n        plot_mc_curve(px, p, Path(save_dir) / 'P_curve.png', names, ylabel='Precision')\n        plot_mc_curve(px, r, Path(save_dir) / 'R_curve.png', names, ylabel='Recall')\n\n    i = f1.mean(0).argmax()  # max F1 index\n    return p[:, i], r[:, i], ap, f1[:, i], unique_classes.astype('int32')\n\n#ä¸Šé¢å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œæ ¹æ®PRæ›²çº¿è®¡ç®—ap\ndef compute_ap(recall, precision):\n    \"\"\" Compute the average precision, given the recall and precision curves\n    # Arguments\n        recall:    The recall curve (list)\n        precision: The precision curve (list)\n    # Returns\n        Average precision, precision curve, recall curve\n    \"\"\"\n\n    # Append sentinel values to beginning and end\n    #æŠŠrecallå¼€æ”¾çš„éƒ¨åˆ†è¡¥å……æˆé—­åˆåŒºé—´\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    #mpreåšç›¸åº”çš„è¡¥å……\n    mpre = np.concatenate(([1.0], precision, [0.0]))\n\n    # Compute the precision envelope\n    #äººä¸ºçš„æŠŠpre-recæ›²çº¿å˜å¾—å•è°ƒé€’å‡ï¼ŒæŠŠæ•´ä¸ªæ›²çº¿ç»™å¡«é¼“èµ·æ¥\n    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n\n    # Integrate area under curve\n    method = 'interp'  # methods: 'continuous', 'interp'\n    if method == 'interp':\n        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n        ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n    else:  # 'continuous'\n        i = np.where(mrec[1:] != mrec[:-1])[0]  # points where x axis (recall) changes\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\n\n    return ap, mpre, mrec\n\n#å®šä¹‰æ··æ·†çŸ©é˜µ\nclass ConfusionMatrix:\n    # Updated version of https://github.com/kaanakan/object_detection_confusion_matrix\n    def __init__(self, nc, conf=0.25, iou_thres=0.45):\n        self.matrix = np.zeros((nc + 1, nc + 1))\n        self.nc = nc  # number of classes\n        self.conf = conf\n        self.iou_thres = iou_thres\n\n    def process_batch(self, detections, labels):\n        \"\"\"\n        Return intersection-over-union (Jaccard index) of boxes.\n        Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n        Arguments:\n            detections (Array[N, 6]), x1, y1, x2, y2, conf, class\n            labels (Array[M, 5]), class, x1, y1, x2, y2\n        Returns:\n            None, updates confusion matrix accordingly\n        \"\"\"\n        detections = detections[detections[:, 4] &gt; self.conf]\n        gt_classes = labels[:, 0].int()\n        detection_classes = detections[:, 5].int()\n        iou = box_iou(labels[:, 1:], detections[:, :4])\n\n        x = torch.where(iou &gt; self.iou_thres)\n        if x[0].shape[0]:\n            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\n            if x[0].shape[0] &gt; 1:\n                matches = matches[matches[:, 2].argsort()[::-1]]\n                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n                matches = matches[matches[:, 2].argsort()[::-1]]\n                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n        else:\n            matches = np.zeros((0, 3))\n\n        n = matches.shape[0] &gt; 0\n        m0, m1, _ = matches.transpose().astype(np.int16)\n        for i, gc in enumerate(gt_classes):\n            j = m0 == i\n            if n and sum(j) == 1:\n                self.matrix[detection_classes[m1[j]], gc] += 1  # correct\n            else:\n                self.matrix[self.nc, gc] += 1  # background FP\n\n        if n:\n            for i, dc in enumerate(detection_classes):\n                if not any(m1 == i):\n                    self.matrix[dc, self.nc] += 1  # background FN\n\n    def matrix(self):\n        return self.matrix\n\n    def plot(self, normalize=True, save_dir='', names=()):\n        try:\n            import seaborn as sn\n\n            array = self.matrix / ((self.matrix.sum(0).reshape(1, -1) + 1E-6) if normalize else 1)  # normalize columns\n            array[array &lt; 0.005] = np.nan  # don't annotate (would appear as 0.00)\n\n            fig = plt.figure(figsize=(12, 9), tight_layout=True)\n            sn.set(font_scale=1.0 if self.nc &lt; 50 else 0.8)  # for label size\n            labels = (0 &lt; len(names) &lt; 99) and len(names) == self.nc  # apply names to ticklabels\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore')  # suppress empty matrix RuntimeWarning: All-NaN slice encountered\n                sn.heatmap(array, annot=self.nc &lt; 30, annot_kws={\"size\": 8}, cmap='Blues', fmt='.2f', square=True,\n                           xticklabels=names + ['background FP'] if labels else \"auto\",\n                           yticklabels=names + ['background FN'] if labels else \"auto\").set_facecolor((1, 1, 1))\n            fig.axes[0].set_xlabel('True')\n            fig.axes[0].set_ylabel('Predicted')\n            fig.savefig(Path(save_dir) / 'confusion_matrix.png', dpi=250)\n            plt.close()\n        except Exception as e:\n            print(f'WARNING: ConfusionMatrix plot failure: {e}')\n\n    def print(self):\n        for i in range(self.nc + 1):\n            print(' '.join(map(str, self.matrix[i])))\n\n#è®¡ç®—ä¸¤ä¸ªæ¡†çš„ç‰¹å®šiouï¼ˆDIOU,CIOU,GIOUï¼‰\ndef bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n    box2 = box2.T\n\n    # Get the coordinates of bounding boxes\n    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n    else:  # transform from xywh to xyxy\n        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n\n    # Intersection area\n    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n\n    # Union Area\n    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n    union = w1 * h1 + w2 * h2 - inter + eps\n\n    iou = inter / union\n    if GIoU or DIoU or CIoU:\n        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n            if DIoU:\n                return iou - rho2 / c2  # DIoU\n            elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n                with torch.no_grad():\n                    alpha = v / (v - iou + (1 + eps))\n                return iou - (rho2 / c2 + v * alpha)  # CIoU\n        else:  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n            c_area = cw * ch + eps  # convex area\n            return iou - (c_area - union) / c_area  # GIoU\n    else:\n        return iou  # IoU\n\n\ndef box_iou(box1, box2):\n    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n    \"\"\"\n    Return intersection-over-union (Jaccard index) of boxes.\n    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Arguments:\n        box1 (Tensor[N, 4])\n        box2 (Tensor[M, 4])\n    Returns:\n        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n            IoU values for every element in boxes1 and boxes2\n    \"\"\"\n\n    def box_area(box):\n        # box = 4xn\n        return (box[2] - box[0]) * (box[3] - box[1])\n\n    area1 = box_area(box1.T)\n    area2 = box_area(box2.T)\n\n    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n\n\ndef bbox_ioa(box1, box2, eps=1E-7):\n    \"\"\" Returns the intersection over box2 area given box1, box2. Boxes are x1y1x2y2\n    box1:       np.array of shape(4)\n    box2:       np.array of shape(nx4)\n    returns:    np.array of shape(n)\n    \"\"\"\n\n    box2 = box2.transpose()\n\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n\n    # Intersection area\n    inter_area = (np.minimum(b1_x2, b2_x2) - np.maximum(b1_x1, b2_x1)).clip(0) * \\\n                 (np.minimum(b1_y2, b2_y2) - np.maximum(b1_y1, b2_y1)).clip(0)\n\n    # box2 area\n    box2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1) + eps\n\n    # Intersection over box2 area\n    return inter_area / box2_area\n\n#è®¡ç®—iouçŸ©é˜µ\ndef wh_iou(wh1, wh2):\n    # Returns the nxm IoU matrix. wh1 is nx2, wh2 is mx2\n    wh1 = wh1[:, None]  # [N,1,2]\n    wh2 = wh2[None]  # [1,M,2]\n    inter = torch.min(wh1, wh2).prod(2)  # [N,M]\n    return inter / (wh1.prod(2) + wh2.prod(2) - inter)  # iou = inter / (area1 + area2 - inter)\n\n\n# Plots ----------------------------------------------------------------------------------------------------------------\n\ndef plot_pr_curve(px, py, ap, save_dir='pr_curve.png', names=()):\n    # Precision-recall curve\n    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n    py = np.stack(py, axis=1)\n\n    if 0 &lt; len(names) &lt; 21:  # display per-class legend if &lt; 21 classes\n        for i, y in enumerate(py.T):\n            ax.plot(px, y, linewidth=1, label=f'{names[i]} {ap[i, 0]:.3f}')  # plot(recall, precision)\n    else:\n        ax.plot(px, py, linewidth=1, color='grey')  # plot(recall, precision)\n\n    ax.plot(px, py.mean(1), linewidth=3, color='blue', label='all classes %.3f mAP@0.5' % ap[:, 0].mean())\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Precision')\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n    fig.savefig(Path(save_dir), dpi=250)\n    plt.close()\n\n\ndef plot_mc_curve(px, py, save_dir='mc_curve.png', names=(), xlabel='Confidence', ylabel='Metric'):\n    # Metric-confidence curve\n    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n\n    if 0 &lt; len(names) &lt; 21:  # display per-class legend if &lt; 21 classes\n        for i, y in enumerate(py):\n            ax.plot(px, y, linewidth=1, label=f'{names[i]}')  # plot(confidence, metric)\n    else:\n        ax.plot(px, py.T, linewidth=1, color='grey')  # plot(confidence, metric)\n\n    y = py.mean(0)\n    ax.plot(px, y, linewidth=3, color='blue', label=f'all classes {y.max():.2f} at {px[y.argmax()]:.3f}')\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n    fig.savefig(Path(save_dir), dpi=250)\n    plt.close()\n\n</code></pre>\n<h3><a id=\"plotspy_4085\"></a>plots.py</h3>\n<h3><a id=\"generalpy_4086\"></a>general.py</h3>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nGeneral utils\n\"\"\"\n\nimport contextlib\nimport glob\nimport logging\nimport math\nimport os\nimport platform\nimport random\nimport re\nimport signal\nimport time\nimport urllib\nfrom itertools import repeat\nfrom multiprocessing.pool import ThreadPool\nfrom pathlib import Path\nfrom subprocess import check_output\nfrom zipfile import ZipFile\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport pkg_resources as pkg\nimport torch\nimport torchvision\nimport yaml\n\nfrom utils.downloads import gsutil_getsize\nfrom utils.metrics import box_iou, fitness\n\n# Settings\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\n#ç¦æ­¢OpenCVçš„å¤šçº¿ç¨‹ï¼Œä½¿ç”¨torchçš„æ‰€çº¿ç¨‹\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(min(os.cpu_count(), 8))  # NumExpr max threads\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\n\n\nclass Profile(contextlib.ContextDecorator):\n    # Usage: @Profile() decorator or 'with Profile():' context manager\n    def __enter__(self):\n        self.start = time.time()\n\n    def __exit__(self, type, value, traceback):\n        print(f'Profile results: {time.time() - self.start:.5f}s')\n\n\nclass Timeout(contextlib.ContextDecorator):\n    # Usage: @Timeout(seconds) decorator or 'with Timeout(seconds):' context manager\n    def __init__(self, seconds, *, timeout_msg='', suppress_timeout_errors=True):\n        self.seconds = int(seconds)\n        self.timeout_message = timeout_msg\n        self.suppress = bool(suppress_timeout_errors)\n\n    def _timeout_handler(self, signum, frame):\n        raise TimeoutError(self.timeout_message)\n\n    def __enter__(self):\n        signal.signal(signal.SIGALRM, self._timeout_handler)  # Set handler for SIGALRM\n        signal.alarm(self.seconds)  # start countdown for SIGALRM to be raised\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        signal.alarm(0)  # Cancel SIGALRM if it's scheduled\n        if self.suppress and exc_type is TimeoutError:  # Suppress TimeoutError\n            return True\n\n\ndef try_except(func):\n    # try-except function. Usage: @try_except decorator\n    def handler(*args, **kwargs):\n        try:\n            func(*args, **kwargs)\n        except Exception as e:\n            print(e)\n\n    return handler\n\n\ndef methods(instance):\n    # Get class/instance methods\n    return [f for f in dir(instance) if callable(getattr(instance, f)) and not f.startswith(\"__\")]\n\n#è®¾ç½®æ—¥å¿—çš„ä¿å­˜çº§åˆ«\ndef set_logging(rank=-1, verbose=True):\n    logging.basicConfig(\n        format=\"%(message)s\",\n        level=logging.INFO if (verbose and rank in [-1, 0]) else logging.WARN)\n\n\ndef print_args(name, opt):\n    # Print argparser arguments\n    print(colorstr(f'{name}: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))\n\n#åˆå§‹åŒ–éšæœºæ•°ç§å­\ndef init_seeds(seed=0):\n    # Initialize random number generator (RNG) seeds https://pytorch.org/docs/stable/notes/randomness.html\n    # cudnn seed 0 settings are slower and more reproducible, else faster and less reproducible\n    import torch.backends.cudnn as cudnn\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    cudnn.benchmark, cudnn.deterministic = (False, True) if seed == 0 else (True, False)\n\n# è·å–æœ€è¿‘è®­ç»ƒçš„æƒé‡æ–‡ä»¶ï¼Œlast.pt\ndef get_latest_run(search_dir='.'):\n    # Return path to most recent 'last.pt' in /runs (i.e. to --resume from)\n    last_list = glob.glob(f'{search_dir}/**/last*.pt', recursive=True)\n    return max(last_list, key=os.path.getctime) if last_list else ''\n\n\ndef user_config_dir(dir='Ultralytics', env_var='YOLOV5_CONFIG_DIR'):\n    # Return path of user configuration directory. Prefer environment variable if exists. Make dir if required.\n    env = os.getenv(env_var)\n    if env:\n        path = Path(env)  # use environment variable\n    else:\n        cfg = {'Windows': 'AppData/Roaming', 'Linux': '.config', 'Darwin': 'Library/Application Support'}  # 3 OS dirs\n        path = Path.home() / cfg.get(platform.system(), '')  # OS-specific config dir\n        path = (path if is_writeable(path) else Path('/tmp')) / dir  # GCP and AWS lambda fix, only /tmp is writeable\n    path.mkdir(exist_ok=True)  # make if required\n    return path\n\n\ndef is_writeable(dir, test=False):\n    # Return True if directory has write permissions, test opening a file with write permissions if test=True\n    if test:  # method 1\n        file = Path(dir) / 'tmp.txt'\n        try:\n            with open(file, 'w'):  # open file with write permissions\n                pass\n            file.unlink()  # remove file\n            return True\n        except IOError:\n            return False\n    else:  # method 2\n        return os.access(dir, os.R_OK)  # possible issues on Windows\n\n\ndef is_docker():\n    # Is environment a Docker container?\n    return Path('/workspace').exists()  # or Path('/.dockerenv').exists()\n\n\ndef is_colab():\n    # Is environment a Google Colab instance?\n    try:\n        import google.colab\n        return True\n    except ImportError:\n        return False\n\n\ndef is_pip():\n    # Is file in a pip package?\n    return 'site-packages' in Path(__file__).resolve().parts\n\n\ndef is_ascii(s=''):\n    # Is string composed of all ASCII (no UTF) characters? (note str().isascii() introduced in python 3.7)\n    s = str(s)  # convert list, tuple, None, etc. to str\n    return len(s.encode().decode('ascii', 'ignore')) == len(s)\n\n\ndef is_chinese(s='äººå·¥æ™ºèƒ½'):\n    # Is string composed of any Chinese characters?\n    return re.search('[\\u4e00-\\u9fff]', s)\n\n\ndef emojis(str=''):\n    # Return platform-dependent emoji-safe version of string\n    return str.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else str\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / 1E6\n    else:\n        return 0.0\n\n\ndef check_online():\n    # Check internet connectivity\n    import socket\n    try:\n        socket.create_connection((\"1.1.1.1\", 443), 5)  # check host accessibility\n        return True\n    except OSError:\n        return False\n\n\n@try_except\n#æ£€æŸ¥å½“å‰çš„åˆ†æ”¯å’ŒGitä¸Šçš„ç‰ˆæœ¬æ˜¯å¦ä¸€è‡´ï¼Œå¦åˆ™æé†’ç”¨æˆ·\ndef check_git_status():\n    # Recommend 'git pull' if code is out of date\n    msg = ', for updates see https://github.com/ultralytics/yolov5'\n    print(colorstr('github: '), end='')\n    assert Path('.git').exists(), 'skipping check (not a git repository)' + msg\n    assert not is_docker(), 'skipping check (Docker image)' + msg\n    assert check_online(), 'skipping check (offline)' + msg\n\n    cmd = 'git fetch &amp;&amp; git config --get remote.origin.url'\n    url = check_output(cmd, shell=True, timeout=5).decode().strip().rstrip('.git')  # git fetch\n    branch = check_output('git rev-parse --abbrev-ref HEAD', shell=True).decode().strip()  # checked out\n    n = int(check_output(f'git rev-list {branch}..origin/master --count', shell=True))  # commits behind\n    if n &gt; 0:\n        s = f\"âš ï¸ YOLOv5 is out of date by {n} commit{'s' * (n &gt; 1)}. Use `git pull` or `git clone {url}` to update.\"\n    else:\n        s = f'up to date with {url} âœ…'\n    print(emojis(s))  # emoji-safe\n\n\ndef check_python(minimum='3.6.2'):\n    # Check current python version vs. required python version\n    check_version(platform.python_version(), minimum, name='Python ')\n\n\ndef check_version(current='0.0.0', minimum='0.0.0', name='version ', pinned=False):\n    # Check version vs. required version\n    current, minimum = (pkg.parse_version(x) for x in (current, minimum))\n    result = (current == minimum) if pinned else (current &gt;= minimum)\n    assert result, f'{name}{minimum} required by YOLOv5, but {name}{current} is currently installed'\n\n\n@try_except\ndef check_requirements(requirements=ROOT / 'requirements.txt', exclude=(), install=True):\n    # Check installed dependencies meet requirements (pass *.txt file or list of packages)\n    prefix = colorstr('red', 'bold', 'requirements:')\n    check_python()  # check python version\n    if isinstance(requirements, (str, Path)):  # requirements.txt file\n        file = Path(requirements)\n        assert file.exists(), f\"{prefix} {file.resolve()} not found, check failed.\"\n        requirements = [f'{x.name}{x.specifier}' for x in pkg.parse_requirements(file.open()) if x.name not in exclude]\n    else:  # list or tuple of packages\n        requirements = [x for x in requirements if x not in exclude]\n\n    n = 0  # number of packages updates\n    for r in requirements:\n        try:\n            pkg.require(r)\n        except Exception as e:  # DistributionNotFound or VersionConflict if requirements not met\n            s = f\"{prefix} {r} not found and is required by YOLOv5\"\n            if install:\n                print(f\"{s}, attempting auto-update...\")\n                try:\n                    assert check_online(), f\"'pip install {r}' skipped (offline)\"\n                    print(check_output(f\"pip install '{r}'\", shell=True).decode())\n                    n += 1\n                except Exception as e:\n                    print(f'{prefix} {e}')\n            else:\n                print(f'{s}. Please install and rerun your command.')\n\n    if n:  # if packages updated\n        source = file.resolve() if 'file' in locals() else requirements\n        s = f\"{prefix} {n} package{'s' * (n &gt; 1)} updated per {source}\\n\" \\\n            f\"{prefix} âš ï¸ {colorstr('bold', 'Restart runtime or rerun command for updates to take effect')}\\n\"\n        print(emojis(s))\n\n#æ£€æŸ¥å›¾åƒçš„å°ºå¯¸æ˜¯å¦æ˜¯32çš„æ•´æ•°å€ã€‚å¦åˆ™è°ƒæ•´\ndef check_img_size(imgsz, s=32, floor=0):\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)\n    else:  # list i.e. img_size=[640, 480]\n        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]\n    if new_size != imgsz:\n        print(f'WARNING: --img-size {imgsz} must be multiple of max stride {s}, updating to {new_size}')\n    return new_size\n\n\ndef check_imshow():\n    # Check if environment supports image displays\n    try:\n        assert not is_docker(), 'cv2.imshow() is disabled in Docker environments'\n        assert not is_colab(), 'cv2.imshow() is disabled in Google Colab environments'\n        cv2.imshow('test', np.zeros((1, 1, 3)))\n        cv2.waitKey(1)\n        cv2.destroyAllWindows()\n        cv2.waitKey(1)\n        return True\n    except Exception as e:\n        print(f'WARNING: Environment does not support cv2.imshow() or PIL Image.show() image displays\\n{e}')\n        return False\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f\"{msg}{f} acceptable suffix is {suffix}\"\n\n\ndef check_yaml(file, suffix=('.yaml', '.yml')):\n    # Search/download YAML file (if necessary) and return path, checking suffix\n    return check_file(file, suffix)\n\n\ndef check_file(file, suffix=''):\n    # Search/download file (if necessary) and return path\n    check_suffix(file, suffix)  # optional\n    file = str(file)  # convert to str()\n    if Path(file).is_file() or file == '':  # exists\n        return file\n    elif file.startswith(('http:/', 'https:/')):  # download\n        url = str(Path(file)).replace(':/', '://')  # Pathlib turns :// -&gt; :/\n        file = Path(urllib.parse.unquote(file).split('?')[0]).name  # '%2F' to '/', split https://url.com/file.txt?auth\n        print(f'Downloading {url} to {file}...')\n        torch.hub.download_url_to_file(url, file)\n        assert Path(file).exists() and Path(file).stat().st_size &gt; 0, f'File download failed: {url}'  # check\n        return file\n    else:  # search\n        files = []\n        for d in 'data', 'models', 'utils':  # search directories\n            files.extend(glob.glob(str(ROOT / d / '**' / file), recursive=True))  # find file\n        assert len(files), f'File not found: {file}'  # assert file was found\n        assert len(files) == 1, f\"Multiple files match '{file}', specify exact path: {files}\"  # assert unique\n        return files[0]  # return file\n\n\ndef check_dataset(data, autodownload=True):\n    # Download and/or unzip dataset if not found locally\n    # Usage: https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128_with_yaml.zip\n\n    # Download (optional)\n    extract_dir = ''\n    if isinstance(data, (str, Path)) and str(data).endswith('.zip'):  # i.e. gs://bucket/dir/coco128.zip\n        download(data, dir='../datasets', unzip=True, delete=False, curl=False, threads=1)\n        data = next((Path('../datasets') / Path(data).stem).rglob('*.yaml'))\n        extract_dir, autodownload = data.parent, False\n\n    # Read yaml (optional)\n    if isinstance(data, (str, Path)):\n        with open(data, errors='ignore') as f:\n            data = yaml.safe_load(f)  # dictionary\n\n    # Parse yaml\n    path = extract_dir or Path(data.get('path') or '')  # optional 'path' default to '.'\n    for k in 'train', 'val', 'test':\n        if data.get(k):  # prepend path\n            data[k] = str(path / data[k]) if isinstance(data[k], str) else [str(path / x) for x in data[k]]\n\n    assert 'nc' in data, \"Dataset 'nc' key missing.\"\n    if 'names' not in data:\n        data['names'] = [f'class{i}' for i in range(data['nc'])]  # assign class names if missing\n    train, val, test, s = [data.get(x) for x in ('train', 'val', 'test', 'download')]\n    if val:\n        val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val path\n        if not all(x.exists() for x in val):\n            print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()])\n            if s and autodownload:  # download script\n                root = path.parent if 'path' in data else '..'  # unzip directory i.e. '../'\n                if s.startswith('http') and s.endswith('.zip'):  # URL\n                    f = Path(s).name  # filename\n                    print(f'Downloading {s} to {f}...')\n                    torch.hub.download_url_to_file(s, f)\n                    Path(root).mkdir(parents=True, exist_ok=True)  # create root\n                    ZipFile(f).extractall(path=root)  # unzip\n                    Path(f).unlink()  # remove zip\n                    r = None  # success\n                elif s.startswith('bash '):  # bash script\n                    print(f'Running {s} ...')\n                    r = os.system(s)\n                else:  # python script\n                    r = exec(s, {'yaml': data})  # return None\n                print(f\"Dataset autodownload {f'success, saved to {root}' if r in (0, None) else 'failure'}\\n\")\n            else:\n                raise Exception('Dataset not found.')\n\n    return data  # dictionary\n\n\ndef url2file(url):\n    # Convert URL to filename, i.e. https://url.com/file.txt?auth -&gt; file.txt\n    url = str(Path(url)).replace(':/', '://')  # Pathlib turns :// -&gt; :/\n    file = Path(urllib.parse.unquote(url)).name.split('?')[0]  # '%2F' to '/', split https://url.com/file.txt?auth\n    return file\n\n\ndef download(url, dir='.', unzip=True, delete=True, curl=False, threads=1):\n    # Multi-threaded file download and unzip function, used in data.yaml for autodownload\n    def download_one(url, dir):\n        # Download 1 file\n        f = dir / Path(url).name  # filename\n        if Path(url).is_file():  # exists in current path\n            Path(url).rename(f)  # move to dir\n        elif not f.exists():\n            print(f'Downloading {url} to {f}...')\n            if curl:\n                os.system(f\"curl -L '{url}' -o '{f}' --retry 9 -C -\")  # curl download, retry and resume on fail\n            else:\n                torch.hub.download_url_to_file(url, f, progress=True)  # torch download\n        if unzip and f.suffix in ('.zip', '.gz'):\n            print(f'Unzipping {f}...')\n            if f.suffix == '.zip':\n                ZipFile(f).extractall(path=dir)  # unzip\n            elif f.suffix == '.gz':\n                os.system(f'tar xfz {f} --directory {f.parent}')  # unzip\n            if delete:\n                f.unlink()  # remove zip\n\n    dir = Path(dir)\n    dir.mkdir(parents=True, exist_ok=True)  # make directory\n    if threads &gt; 1:\n        pool = ThreadPool(threads)\n        pool.imap(lambda x: download_one(*x), zip(url, repeat(dir)))  # multi-threaded\n        pool.close()\n        pool.join()\n    else:\n        for u in [url] if isinstance(url, (str, Path)) else url:\n            download_one(u, dir)\n\n\ndef make_divisible(x, divisor):\n    # Returns x evenly divisible by divisor\n    return math.ceil(x / divisor) * divisor\n\n\ndef clean_str(s):\n    # Cleans a string by replacing special characters with underscore _\n    return re.sub(pattern=\"[|@#!Â¡Â·$â‚¬%&amp;()=?Â¿^*;:,Â¨Â´&gt;&lt;+]\", repl=\"_\", string=s)\n\n\ndef one_cycle(y1=0.0, y2=1.0, steps=100):\n    # lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf\n    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1\n\n\ndef colorstr(*input):\n    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n    *args, string = input if len(input) &gt; 1 else ('blue', 'bold', input[0])  # color arguments, string\n    colors = {'black': '\\033[30m',  # basic colors\n              'red': '\\033[31m',\n              'green': '\\033[32m',\n              'yellow': '\\033[33m',\n              'blue': '\\033[34m',\n              'magenta': '\\033[35m',\n              'cyan': '\\033[36m',\n              'white': '\\033[37m',\n              'bright_black': '\\033[90m',  # bright colors\n              'bright_red': '\\033[91m',\n              'bright_green': '\\033[92m',\n              'bright_yellow': '\\033[93m',\n              'bright_blue': '\\033[94m',\n              'bright_magenta': '\\033[95m',\n              'bright_cyan': '\\033[96m',\n              'bright_white': '\\033[97m',\n              'end': '\\033[0m',  # misc\n              'bold': '\\033[1m',\n              'underline': '\\033[4m'}\n    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']\n\n\ndef labels_to_class_weights(labels, nc=80):\n    # Get class weights (inverse frequency) from training labels\n    if labels[0] is None:  # no labels loaded\n        return torch.Tensor()\n\n    labels = np.concatenate(labels, 0)  # labels.shape = (866643, 5) for COCO\n    classes = labels[:, 0].astype(np.int)  # labels = [class xywh]\n    weights = np.bincount(classes, minlength=nc)  # occurrences per class\n\n    # Prepend gridpoint count (for uCE training)\n    # gpi = ((320 / 32 * np.array([1, 2, 4])) ** 2 * 3).sum()  # gridpoints per image\n    # weights = np.hstack([gpi * len(labels)  - weights.sum() * 9, weights * 9]) ** 0.5  # prepend gridpoints to start\n\n    weights[weights == 0] = 1  # replace empty bins with 1\n    weights = 1 / weights  # number of targets per class\n    weights /= weights.sum()  # normalize\n    return torch.from_numpy(weights)\n\n\ndef labels_to_image_weights(labels, nc=80, class_weights=np.ones(80)):\n    # Produces image weights based on class_weights and image contents\n    class_counts = np.array([np.bincount(x[:, 0].astype(np.int), minlength=nc) for x in labels])\n    image_weights = (class_weights.reshape(1, nc) * class_counts).sum(1)\n    # index = random.choices(range(n), weights=image_weights, k=1)  # weight image sample\n    return image_weights\n\n\ndef coco80_to_coco91_class():  # converts 80-index (val2014) to 91-index (paper)\n    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\\n')\n    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\\n')\n    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco\n    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n    return x\n\n\ndef xyxy2xywh(x):\n    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n    y[:, 2] = x[:, 2] - x[:, 0]  # width\n    y[:, 3] = x[:, 3] - x[:, 1]  # height\n    return y\n\n\ndef xywh2xyxy(x):\n    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n    return y\n\n\ndef xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):\n    # Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + padw  # top left x\n    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + padh  # top left y\n    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + padw  # bottom right x\n    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + padh  # bottom right y\n    return y\n\n\ndef xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n    if clip:\n        clip_coords(x, (h - eps, w - eps))  # warning: inplace clip\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n    return y\n\n\ndef xyn2xy(x, w=640, h=640, padw=0, padh=0):\n    # Convert normalized segments into pixel segments, shape (n,2)\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = w * x[:, 0] + padw  # top left x\n    y[:, 1] = h * x[:, 1] + padh  # top left y\n    return y\n\n\ndef segment2box(segment, width=640, height=640):\n    # Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)\n    x, y = segment.T  # segment xy\n    inside = (x &gt;= 0) &amp; (y &gt;= 0) &amp; (x &lt;= width) &amp; (y &lt;= height)\n    x, y, = x[inside], y[inside]\n    return np.array([x.min(), y.min(), x.max(), y.max()]) if any(x) else np.zeros((1, 4))  # xyxy\n\n\ndef segments2boxes(segments):\n    # Convert segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh)\n    boxes = []\n    for s in segments:\n        x, y = s.T  # segment xy\n        boxes.append([x.min(), y.min(), x.max(), y.max()])  # cls, xyxy\n    return xyxy2xywh(np.array(boxes))  # cls, xywh\n\n\ndef resample_segments(segments, n=1000):\n    # Up-sample an (n,2) segment\n    for i, s in enumerate(segments):\n        x = np.linspace(0, len(s) - 1, n)\n        xp = np.arange(len(s))\n        segments[i] = np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)]).reshape(2, -1).T  # segment xy\n    return segments\n\n\ndef scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n    # Rescale coords (xyxy) from img1_shape to img0_shape\n    if ratio_pad is None:  # calculate from img0_shape\n        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n    else:\n        gain = ratio_pad[0][0]\n        pad = ratio_pad[1]\n\n    coords[:, [0, 2]] -= pad[0]  # x padding\n    coords[:, [1, 3]] -= pad[1]  # y padding\n    coords[:, :4] /= gain\n    clip_coords(coords, img0_shape)\n    return coords\n\n\ndef clip_coords(boxes, shape):\n    # Clip bounding xyxy bounding boxes to image shape (height, width)\n    if isinstance(boxes, torch.Tensor):  # faster individually\n        boxes[:, 0].clamp_(0, shape[1])  # x1\n        boxes[:, 1].clamp_(0, shape[0])  # y1\n        boxes[:, 2].clamp_(0, shape[1])  # x2\n        boxes[:, 3].clamp_(0, shape[0])  # y2\n    else:  # np.array (faster grouped)\n        boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, shape[1])  # x1, x2\n        boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, shape[0])  # y1, y2\n\n#éæå¤§å€¼æŠ‘åˆ¶\ndef non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n                        labels=(), max_det=300):\n    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n\n    Returns:\n         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n    \"\"\"\n\n    nc = prediction.shape[2] - 5  # number of classes\n    xc = prediction[..., 4] &gt; conf_thres  # candidates\n\n    # Checks\n    assert 0 &lt;= conf_thres &lt;= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n    assert 0 &lt;= iou_thres &lt;= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n\n    # Settings\n    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n    time_limit = 10.0  # seconds to quit after\n    redundant = True  # require redundant detections\n    multi_label &amp;= nc &gt; 1  # multiple labels per box (adds 0.5ms/img)\n    merge = False  # use merge-NMS\n\n    t = time.time()\n    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n    for xi, x in enumerate(prediction):  # image index, image inference\n        # Apply constraints\n        # x[((x[..., 2:4] &lt; min_wh) | (x[..., 2:4] &gt; max_wh)).any(1), 4] = 0  # width-height\n        x = x[xc[xi]]  # confidence\n\n        # Cat apriori labels if autolabelling\n        if labels and len(labels[xi]):\n            l = labels[xi]\n            v = torch.zeros((len(l), nc + 5), device=x.device)\n            v[:, :4] = l[:, 1:5]  # box\n            v[:, 4] = 1.0  # conf\n            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n            x = torch.cat((x, v), 0)\n\n        # If none remain process next image\n        if not x.shape[0]:\n            continue\n\n        # Compute conf\n        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n\n        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n        box = xywh2xyxy(x[:, :4])\n\n        # Detections matrix nx6 (xyxy, conf, cls)\n        if multi_label:\n            i, j = (x[:, 5:] &gt; conf_thres).nonzero(as_tuple=False).T\n            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n        else:  # best class only\n            conf, j = x[:, 5:].max(1, keepdim=True)\n            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) &gt; conf_thres]\n\n        # Filter by class\n        if classes is not None:\n            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n\n        # Apply finite constraint\n        # if not torch.isfinite(x).all():\n        #     x = x[torch.isfinite(x).all(1)]\n\n        # Check shape\n        n = x.shape[0]  # number of boxes\n        if not n:  # no boxes\n            continue\n        elif n &gt; max_nms:  # excess boxes\n            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n\n        # Batched NMS\n        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n        if i.shape[0] &gt; max_det:  # limit detections\n            i = i[:max_det]\n        if merge and (1 &lt; n &lt; 3E3):  # Merge NMS (boxes merged using weighted mean)\n            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n            iou = box_iou(boxes[i], boxes) &gt; iou_thres  # iou matrix\n            weights = iou * scores[None]  # box weights\n            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n            if redundant:\n                i = i[iou.sum(1) &gt; 1]  # require redundancy\n\n        output[xi] = x[i]\n        if (time.time() - t) &gt; time_limit:\n            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n            break  # time limit exceeded\n\n    return output\n\n\ndef strip_optimizer(f='best.pt', s=''):  # from utils.general import *; strip_optimizer()\n    # Strip optimizer from 'f' to finalize training, optionally save as 's'\n    x = torch.load(f, map_location=torch.device('cpu'))\n    if x.get('ema'):\n        x['model'] = x['ema']  # replace model with ema\n    for k in 'optimizer', 'training_results', 'wandb_id', 'ema', 'updates':  # keys\n        x[k] = None\n    x['epoch'] = -1\n    x['model'].half()  # to FP16\n    for p in x['model'].parameters():\n        p.requires_grad = False\n    torch.save(x, s or f)\n    mb = os.path.getsize(s or f) / 1E6  # filesize\n    print(f\"Optimizer stripped from {f},{(' saved as %s,' % s) if s else ''} {mb:.1f}MB\")\n\n\ndef print_mutation(results, hyp, save_dir, bucket):\n    evolve_csv, results_csv, evolve_yaml = save_dir / 'evolve.csv', save_dir / 'results.csv', save_dir / 'hyp_evolve.yaml'\n    keys = ('metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',\n            'val/box_loss', 'val/obj_loss', 'val/cls_loss') + tuple(hyp.keys())  # [results + hyps]\n    keys = tuple(x.strip() for x in keys)\n    vals = results + tuple(hyp.values())\n    n = len(keys)\n\n    # Download (optional)\n    if bucket:\n        url = f'gs://{bucket}/evolve.csv'\n        if gsutil_getsize(url) &gt; (os.path.getsize(evolve_csv) if os.path.exists(evolve_csv) else 0):\n            os.system(f'gsutil cp {url} {save_dir}')  # download evolve.csv if larger than local\n\n    # Log to evolve.csv\n    s = '' if evolve_csv.exists() else (('%20s,' * n % keys).rstrip(',') + '\\n')  # add header\n    with open(evolve_csv, 'a') as f:\n        f.write(s + ('%20.5g,' * n % vals).rstrip(',') + '\\n')\n\n    # Print to screen\n    print(colorstr('evolve: ') + ', '.join(f'{x.strip():&gt;20s}' for x in keys))\n    print(colorstr('evolve: ') + ', '.join(f'{x:20.5g}' for x in vals), end='\\n\\n\\n')\n\n    # Save yaml\n    with open(evolve_yaml, 'w') as f:\n        data = pd.read_csv(evolve_csv)\n        data = data.rename(columns=lambda x: x.strip())  # strip keys\n        i = np.argmax(fitness(data.values[:, :7]))  #\n        f.write('# YOLOv5 Hyperparameter Evolution Results\\n' +\n                f'# Best generation: {i}\\n' +\n                f'# Last generation: {len(data)}\\n' +\n                '# ' + ', '.join(f'{x.strip():&gt;20s}' for x in keys[:7]) + '\\n' +\n                '# ' + ', '.join(f'{x:&gt;20.5g}' for x in data.values[i, :7]) + '\\n\\n')\n        yaml.safe_dump(hyp, f, sort_keys=False)\n\n    if bucket:\n        os.system(f'gsutil cp {evolve_csv} {evolve_yaml} gs://{bucket}')  # upload\n\n\ndef apply_classifier(x, model, img, im0):\n    # Apply a second stage classifier to yolo outputs\n    im0 = [im0] if isinstance(im0, np.ndarray) else im0\n    for i, d in enumerate(x):  # per image\n        if d is not None and len(d):\n            d = d.clone()\n\n            # Reshape and pad cutouts\n            b = xyxy2xywh(d[:, :4])  # boxes\n            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square\n            b[:, 2:] = b[:, 2:] * 1.3 + 30  # pad\n            d[:, :4] = xywh2xyxy(b).long()\n\n            # Rescale boxes from img_size to im0 size\n            scale_coords(img.shape[2:], d[:, :4], im0[i].shape)\n\n            # Classes\n            pred_cls1 = d[:, 5].long()\n            ims = []\n            for j, a in enumerate(d):  # per item\n                cutout = im0[i][int(a[1]):int(a[3]), int(a[0]):int(a[2])]\n                im = cv2.resize(cutout, (224, 224))  # BGR\n                # cv2.imwrite('example%i.jpg' % j, cutout)\n\n                im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n                im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 to float32\n                im /= 255.0  # 0 - 255 to 0.0 - 1.0\n                ims.append(im)\n\n            pred_cls2 = model(torch.Tensor(ims).to(d.device)).argmax(1)  # classifier prediction\n            x[i] = x[i][pred_cls1 == pred_cls2]  # retain matching class detections\n\n    return x\n\n\ndef save_one_box(xyxy, im, file='image.jpg', gain=1.02, pad=10, square=False, BGR=False, save=True):\n    # Save image crop as {file} with crop size multiple {gain} and {pad} pixels. Save and/or return crop\n    xyxy = torch.tensor(xyxy).view(-1, 4)\n    b = xyxy2xywh(xyxy)  # boxes\n    if square:\n        b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # attempt rectangle to square\n    b[:, 2:] = b[:, 2:] * gain + pad  # box wh * gain + pad\n    xyxy = xywh2xyxy(b).long()\n    clip_coords(xyxy, im.shape)\n    crop = im[int(xyxy[0, 1]):int(xyxy[0, 3]), int(xyxy[0, 0]):int(xyxy[0, 2]), ::(1 if BGR else -1)]\n    if save:\n        cv2.imwrite(str(increment_path(file, mkdir=True).with_suffix('.jpg')), crop)\n    return crop\n\n\ndef increment_path(path, exist_ok=False, sep='', mkdir=False):\n    # Increment file or directory path, i.e. runs/exp --&gt; runs/exp{sep}2, runs/exp{sep}3, ... etc.\n    path = Path(path)  # os-agnostic\n    if path.exists() and not exist_ok:\n        suffix = path.suffix\n        path = path.with_suffix('')\n        dirs = glob.glob(f\"{path}{sep}*\")  # similar paths\n        matches = [re.search(rf\"%s{sep}(\\d+)\" % path.stem, d) for d in dirs]\n        i = [int(m.groups()[0]) for m in matches if m]  # indices\n        n = max(i) + 1 if i else 2  # increment number\n        path = Path(f\"{path}{sep}{n}{suffix}\")  # update path\n    dir = path if path.suffix == '' else path.parent  # directory\n    if not dir.exists() and mkdir:\n        dir.mkdir(parents=True, exist_ok=True)  # make directory\n    return path\n\n</code></pre>\n<h3><a id=\"torch_utils__4916\"></a>torch_utils è¾…åŠ©ç¨‹åºä»£ç å¹¶è¡Œè®¡ç®—ï¼Œæ—©åœç­–ç•¥ç­‰å‡½æ•°</h3>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nPyTorch utils\n\"\"\"\n\nimport datetime\nimport logging\nimport math\nimport os\nimport platform\nimport subprocess\nimport time\nfrom contextlib import contextmanager\nfrom copy import deepcopy\nfrom pathlib import Path\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n\ntry:\n    import thop  # for FLOPs computation\nexcept ImportError:\n    thop = None\n\nLOGGER = logging.getLogger(__name__)\n\n\n@contextmanager\ndef torch_distributed_zero_first(local_rank: int):\n    \"\"\"\n    Decorator to make all processes in distributed training wait for each local_master to do something.\n    \"\"\"\n    if local_rank not in [-1, 0]:\n        dist.barrier(device_ids=[local_rank])\n    yield\n    if local_rank == 0:\n        dist.barrier(device_ids=[0])\n\n\ndef date_modified(path=__file__):\n    # return human-readable file modification date, i.e. '2021-3-26'\n    t = datetime.datetime.fromtimestamp(Path(path).stat().st_mtime)\n    return f'{t.year}-{t.month}-{t.day}'\n\n\ndef git_describe(path=Path(__file__).parent):  # path must be a directory\n    # return human-readable git description, i.e. v5.0-5-g3e25f1e https://git-scm.com/docs/git-describe\n    s = f'git -C {path} describe --tags --long --always'\n    try:\n        return subprocess.check_output(s, shell=True, stderr=subprocess.STDOUT).decode()[:-1]\n    except subprocess.CalledProcessError as e:\n        return ''  # not a git repository\n\n#é€‰æ‹©è®¾å¤‡\ndef select_device(device='', batch_size=None):\n    # device = 'cpu' or '0' or '0,1,2,3'\n    s = f'YOLOv5 ğŸš€ {git_describe() or date_modified()} torch {torch.__version__} '  # string\n    device = str(device).strip().lower().replace('cuda:', '')  # to string, 'cuda:0' to '0'\n    cpu = device == 'cpu'\n    if cpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False\n    elif device:  # non-cpu device requested\n        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable  è®¾å®šå¯ç”¨çš„gpuæ˜¾å¡\n        assert torch.cuda.is_available(), f'CUDA unavailable, invalid device {device} requested'  # check availability\n\n    cuda = not cpu and torch.cuda.is_available()\n    if cuda:\n        devices = device.split(',') if device else '0'  # range(torch.cuda.device_count())  # i.e. 0,1,6,7\n        n = len(devices)  # device count\n        if n &gt; 1 and batch_size:  # check batch_size is divisible by device_count\n            assert batch_size % n == 0, f'batch-size {batch_size} not multiple of GPU count {n}'\n        space = ' ' * (len(s) + 1)\n        for i, d in enumerate(devices):\n            p = torch.cuda.get_device_properties(i)\n            s += f\"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 2}MB)\\n\"  # bytes to MB\n    else:\n        s += 'CPU\\n'\n\n    LOGGER.info(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emoji-safe\n    return torch.device('cuda:0' if cuda else 'cpu')\n\n#æ—¶é—´åŒæ­¥ï¼ˆç­‰å¾…GPUæ“ä½œå®Œæˆï¼‰\ndef time_sync():\n    # pytorch-accurate time\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\n\ndef profile(input, ops, n=10, device=None):\n    # YOLOv5 speed/memory/FLOPs profiler\n    #\n    # Usage:\n    #     input = torch.randn(16, 3, 640, 640)\n    #     m1 = lambda x: x * torch.sigmoid(x)\n    #     m2 = nn.SiLU()\n    #     profile(input, [m1, m2], n=100)  # profile over 100 iterations\n\n    results = []\n    logging.basicConfig(format=\"%(message)s\", level=logging.INFO)\n    device = device or select_device()\n    print(f\"{'Params':&gt;12s}{'GFLOPs':&gt;12s}{'GPU_mem (GB)':&gt;14s}{'forward (ms)':&gt;14s}{'backward (ms)':&gt;14s}\"\n          f\"{'input':&gt;24s}{'output':&gt;24s}\")\n\n    for x in input if isinstance(input, list) else [input]:\n        x = x.to(device)\n        x.requires_grad = True\n        for m in ops if isinstance(ops, list) else [ops]:\n            m = m.to(device) if hasattr(m, 'to') else m  # device\n            m = m.half() if hasattr(m, 'half') and isinstance(x, torch.Tensor) and x.dtype is torch.float16 else m\n            tf, tb, t = 0., 0., [0., 0., 0.]  # dt forward, backward\n            try:\n                flops = thop.profile(m, inputs=(x,), verbose=False)[0] / 1E9 * 2  # GFLOPs\n            except:\n                flops = 0\n\n            try:\n                for _ in range(n):\n                    t[0] = time_sync()\n                    y = m(x)\n                    t[1] = time_sync()\n                    try:\n                        _ = (sum([yi.sum() for yi in y]) if isinstance(y, list) else y).sum().backward()\n                        t[2] = time_sync()\n                    except Exception as e:  # no backward method\n                        print(e)\n                        t[2] = float('nan')\n                    tf += (t[1] - t[0]) * 1000 / n  # ms per op forward\n                    tb += (t[2] - t[1]) * 1000 / n  # ms per op backward\n                mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0  # (GB)\n                s_in = tuple(x.shape) if isinstance(x, torch.Tensor) else 'list'\n                s_out = tuple(y.shape) if isinstance(y, torch.Tensor) else 'list'\n                p = sum(list(x.numel() for x in m.parameters())) if isinstance(m, nn.Module) else 0  # parameters\n                print(f'{p:12}{flops:12.4g}{mem:&gt;14.3f}{tf:14.4g}{tb:14.4g}{str(s_in):&gt;24s}{str(s_out):&gt;24s}')\n                results.append([p, flops, mem, tf, tb, s_in, s_out])\n            except Exception as e:\n                print(e)\n                results.append(None)\n            torch.cuda.empty_cache()\n    return results\n\n#æ˜¯å¦å¹¶è¡Œè®¡ç®—\ndef is_parallel(model):\n    # Returns True if model is of type DP or DDP\n    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)\n\n\ndef de_parallel(model):\n    # De-parallelize a model: returns single-GPU model if model is of type DP or DDP\n    return model.module if is_parallel(model) else model\n\n#æŸ¥æ‰¾å­—å…¸äº¤é›†\ndef intersect_dicts(da, db, exclude=()):\n    # Dictionary intersection of matching keys and shapes, omitting 'exclude' keys, using da values\n    return {k: v for k, v in da.items() if k in db and not any(x in k for x in exclude) and v.shape == db[k].shape}\n\n#åˆå§‹åŒ–æƒé‡\ndef initialize_weights(model):\n    for m in model.modules():\n        t = type(m)\n        if t is nn.Conv2d:\n            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        elif t is nn.BatchNorm2d:\n            m.eps = 1e-3\n            m.momentum = 0.03\n        elif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6]:\n            m.inplace = True\n\n#æ‰¾åˆ°åŒ¹é…çš„module classåå­—çš„å±‚ç´¢å¼•\ndef find_modules(model, mclass=nn.Conv2d):\n    # Finds layer indices matching module class 'mclass'\n    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]\n\n#è®¡ç®—æ¨¡å‹ç¨€ç–åº¦\ndef sparsity(model):\n    # Return global model sparsity\n    a, b = 0., 0.\n    for p in model.parameters():\n        a += p.numel() #æƒé‡æ€»æ•°\n        b += (p == 0).sum() #ä¸º0 çš„æƒé‡æ•°\n    return b / a\n\n#æ¨¡å‹å‰ªæ\ndef prune(model, amount=0.3):\n    # Prune model to requested global sparsity\n    import torch.nn.utils.prune as prune\n    print('Pruning model... ', end='')\n    for name, m in model.named_modules():\n        if isinstance(m, nn.Conv2d):#å¯¹å·ç§¯å±‚è¿›è¡Œå‰ªæ\n            #å°†æ‰€æœ‰å·ç§¯å±‚çš„æƒé‡å‡å»30%ï¼Œå‡å»lowest L1-normçš„æƒé‡\n            prune.l1_unstructured(m, name='weight', amount=amount)  # prune\n            prune.remove(m, 'weight')  # make permanent\n    print(' %.3g global sparsity' % sparsity(model))\n\n# èåˆconvå’Œbnå±‚\ndef fuse_conv_and_bn(conv, bn):\n    # Fuse convolution and batchnorm layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/\n    fusedconv = nn.Conv2d(conv.in_channels,\n                          conv.out_channels,\n                          kernel_size=conv.kernel_size,\n                          stride=conv.stride,\n                          padding=conv.padding,\n                          groups=conv.groups,\n                          bias=True).requires_grad_(False).to(conv.weight.device)\n\n    # prepare filters\n    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))\n\n    # prepare spatial bias\n    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\n\n    return fusedconv\n\n\ndef model_info(model, verbose=False, img_size=640):\n    # Model information. img_size may be int or list, i.e. img_size=640 or img_size=[640, 320]\n    n_p = sum(x.numel() for x in model.parameters())  # number parameters\n    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients\n    if verbose:\n        print('%5s %40s %9s %12s %20s %10s %10s' % ('layer', 'name', 'gradient', 'parameters', 'shape', 'mu', 'sigma'))\n        for i, (name, p) in enumerate(model.named_parameters()):\n            name = name.replace('module_list.', '')\n            print('%5g %40s %9s %12g %20s %10.3g %10.3g' %\n                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))\n\n    try:  # FLOPs\n        from thop import profile\n        stride = max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32\n        img = torch.zeros((1, model.yaml.get('ch', 3), stride, stride), device=next(model.parameters()).device)  # input\n        flops = profile(deepcopy(model), inputs=(img,), verbose=False)[0] / 1E9 * 2  # stride GFLOPs\n        img_size = img_size if isinstance(img_size, list) else [img_size, img_size]  # expand if int/float\n        fs = ', %.1f GFLOPs' % (flops * img_size[0] / stride * img_size[1] / stride)  # 640x640 GFLOPs\n    except (ImportError, Exception):\n        fs = ''\n\n    LOGGER.info(f\"Model Summary: {len(list(model.modules()))} layers, {n_p} parameters, {n_g} gradients{fs}\")\n\n#åŠ è½½ç¬¬äºŒç±»åˆ†ç±»å™¨\ndef load_classifier(name='resnet101', n=2):\n    # Loads a pretrained model reshaped to n-class output\n    model = torchvision.models.__dict__[name](pretrained=True)\n\n    # ResNet model properties\n    # input_size = [3, 224, 224]\n    # input_space = 'RGB'\n    # input_range = [0, 1]\n    # mean = [0.485, 0.456, 0.406]\n    # std = [0.229, 0.224, 0.225]\n\n    # Reshape output to n classes\n    filters = model.fc.weight.shape[1]\n    model.fc.bias = nn.Parameter(torch.zeros(n), requires_grad=True)\n    model.fc.weight = nn.Parameter(torch.zeros(n, filters), requires_grad=True)\n    model.fc.out_features = n\n    return model\n\n#æ ¹æ®ratioæ”¹å˜æ¨¡å‹å°ºå¯¸\ndef scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)\n    # scales img(bs,3,y,x) by ratio constrained to gs-multiple\n    if ratio == 1.0:\n        return img\n    else:\n        h, w = img.shape[2:]\n        s = (int(h * ratio), int(w * ratio))  # new size\n        img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize\n        if not same_shape:  # pad/crop img\n            h, w = [math.ceil(x * ratio / gs) * gs for x in (h, w)]\n        return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean\n\n#å¤åˆ¶å±æ€§å€¼\ndef copy_attr(a, b, include=(), exclude=()):\n    # Copy attributes from b to a, options to only include [...] and to exclude [...]\n    for k, v in b.__dict__.items():\n        if (len(include) and k not in include) or k.startswith('_') or k in exclude:\n            continue\n        else:\n            setattr(a, k, v)\n\n#æ—©åœç­–ç•¥\nclass EarlyStopping:\n    # YOLOv5 simple early stopper\n    def __init__(self, patience=30):\n        self.best_fitness = 0.0  # i.e. mAP\n        self.best_epoch = 0\n        self.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop\n        self.possible_stop = False  # possible stop may occur next epoch\n\n    def __call__(self, epoch, fitness):\n        if fitness &gt;= self.best_fitness:  # &gt;= 0 to allow for early zero-fitness stage of training\n            self.best_epoch = epoch\n            self.best_fitness = fitness\n        delta = epoch - self.best_epoch  # epochs without improvement\n        self.possible_stop = delta &gt;= (self.patience - 1)  # possible stop may occur next epoch\n        stop = delta &gt;= self.patience  # stop training if patience exceeded\n        if stop:\n            LOGGER.info(f'EarlyStopping patience {self.patience} exceeded, stopping training.')\n        return stop\n\n#æ¨¡å‹EMA\nclass ModelEMA:\n    \"\"\" Model Exponential Moving Average from https://github.com/rwightman/pytorch-image-models\n    Keep a moving average of everything in the model state_dict (parameters and buffers).\n    This is intended to allow functionality like\n    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n    A smoothed version of the weights is necessary for some training schemes to perform well.\n    This class is sensitive where it is initialized in the sequence of model init,\n    GPU assignment and distributed training wrappers.\n    \"\"\"\n\n    def __init__(self, model, decay=0.9999, updates=0):\n        # Create EMA\n        self.ema = deepcopy(model.module if is_parallel(model) else model).eval()  # FP32 EMA\n        # if next(model.parameters()).device.type != 'cpu':\n        #     self.ema.half()  # FP16 EMA\n        self.updates = updates  # number of EMA updates\n        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))  # decay exponential ramp (to help early epochs)\n        for p in self.ema.parameters():\n            p.requires_grad_(False)\n\n    def update(self, model):\n        # Update EMA parameters\n        with torch.no_grad():\n            self.updates += 1\n            d = self.decay(self.updates)\n\n            msd = model.module.state_dict() if is_parallel(model) else model.state_dict()  # model state_dict\n            for k, v in self.ema.state_dict().items():\n                if v.dtype.is_floating_point:\n                    v *= d\n                    v += (1. - d) * msd[k].detach() #detachæˆªæ–­åå‘ä¼ æ’­çš„æ¢¯åº¦æµ\n                    #å°†æŸä¸ªnodeå˜æˆä¸éœ€è¦æ¢¯åº¦çš„varibale,å› æ­¤å½“åå‘ä¼ æ’­ç»è¿‡è¿™ä¸ªnodeæ—¶ï¼Œæ¢¯åº¦ä¸åœ¨å‘å‰ä¼ æ’­\n\n    def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):\n        # Update EMA attributes\n        copy_attr(self.ema, model, include, exclude)\n\n</code></pre>\n<h2><a id=\"weight_5264\"></a>weightæ–‡ä»¶å¤¹</h2>\n<p>ä¸‹è½½çš„æƒé‡æ–‡ä»¶ï¼Œsmlx,å»ºè®®æå‰åˆ°Gitä¸Šä¸‹è½½å¥½ï¼Œæ”¾åœ¨æ­¤å¤„ï¼Œdownload.py ä¸€èˆ¬ä¼šä¸‹è½½å¤±è´¥</p>\n<h2><a id=\"detectpy_5267\"></a>detect.py</h2>\n<pre><code># YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n\"\"\"\nRun inference on images, videos, directories, streams, etc.\n\nUsage:\n    $ python path/to/detect.py --source path/to/img.jpg --weights yolov5s.pt --img 640\n\"\"\"\n\nimport argparse  #å‚æ•°è§£æåŒ…\nimport os\nimport sys\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom models.experimental import attempt_load\nfrom utils.datasets import LoadImages, LoadStreams,LoadWebcam\nfrom utils.general import apply_classifier, check_img_size, check_imshow, check_requirements, check_suffix, colorstr, \\\n    increment_path, non_max_suppression, print_args, save_one_box, scale_coords, set_logging, \\\n    strip_optimizer, xyxy2xywh\nfrom utils.plots import Annotator, colors\nfrom utils.torch_utils import load_classifier, select_device, time_sync\n\n\n@torch.no_grad()\ndef run(weights=ROOT / 'yolov5s.pt',  # model.pt path(s)\n        source=ROOT / 'data/images',  # file/dir/URL/glob, 0 for webcam\n        imgsz=640,  # inference size (pixels)\n        conf_thres=0.25,  # confidence threshold\n        iou_thres=0.45,  # NMS IOU threshold\n        max_det=1000,  # maximum detections per image\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        view_img=False,  # show results\n        save_txt=False,  # save results to *.txt\n        save_conf=False,  # save confidences in --save-txt labels\n        save_crop=False,  # save cropped prediction boxes\n        nosave=False,  # do not save images/videos\n        classes=None,  # filter by class: --class 0, or --class 0 2 3\n        agnostic_nms=False,  # class-agnostic NMS\n        augment=False,  # augmented inference\n        visualize=False,  # visualize features\n        update=False,  # update all models\n        project=ROOT / 'runs/detect',  # save results to project/name\n        name='exp',  # save results to project/name\n        exist_ok=False,  # existing project/name ok, do not increment\n        line_thickness=3,  # bounding box thickness (pixels)\n        hide_labels=False,  # hide labels\n        hide_conf=False,  # hide confidences\n        half=False,  # use FP16 half-precision inference\n        dnn=False,  # use OpenCV DNN for ONNX inference\n        ):\n    source = str(source)\n    save_img = not nosave #and not source.endswith('.txt')  # save inference imagesï¼Œä¸ä»¥ä¸‹txtç»“å°¾\n    webcam = source.isnumeric() or source.lower.startswith(('rtsp://','rtmp://','http://')) or source.endswith('.txt')\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Initialize\n    set_logging()\n    device = select_device(device)\n    half &amp;= device.type != 'cpu'  # half precision only supported on CUDA\n\n    # Load model\n    w = str(weights[0] if isinstance(weights, list) else weights)\n    classify, suffix, suffixes = False, Path(w).suffix.lower(), ['.pt', '.onnx', '.tflite', '.pb', '']\n    check_suffix(w, suffixes)  # check weights have acceptable suffix\n    pt, onnx, tflite, pb, saved_model = (suffix == x for x in suffixes)  # backend booleans\n    stride, names = 64, [f'class{i}' for i in range(1000)]  # assign defaults\n    #classify è®¾ç½®äºŒçº§åˆ†ç±»ï¼Œé»˜è®¤ä¸é€‚ç”¨\n    if pt:\n        model = torch.jit.load(w) if 'torchscript' in w else attempt_load(weights, map_location=device)\n        stride = int(model.stride.max())  # model stride\n        names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n        if half:\n            model.half()  # to FP16\n        if classify:  # second-stage classifier\n            modelc = load_classifier(name='resnet50', n=2)  # initialize\n            modelc.load_state_dict(torch.load('resnet50.pt', map_location=device)['model']).to(device).eval()\n    elif onnx:\n        if dnn:\n            # check_requirements(('opencv-python&gt;=4.5.4',))\n            net = cv2.dnn.readNetFromONNX(w)\n        else:\n            check_requirements(('onnx', 'onnxruntime-gpu' if torch.has_cuda else 'onnxruntime'))\n            import onnxruntime\n            session = onnxruntime.InferenceSession(w, None)\n    else:  # TensorFlow models\n        check_requirements(('tensorflow&gt;=2.4.1',))\n        import tensorflow as tf\n        if pb:  # https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt\n            def wrap_frozen_graph(gd, inputs, outputs):\n                x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=\"\"), [])  # wrapped import\n                return x.prune(tf.nest.map_structure(x.graph.as_graph_element, inputs),\n                               tf.nest.map_structure(x.graph.as_graph_element, outputs))\n\n            graph_def = tf.Graph().as_graph_def()\n            graph_def.ParseFromString(open(w, 'rb').read())\n            frozen_func = wrap_frozen_graph(gd=graph_def, inputs=\"x:0\", outputs=\"Identity:0\")\n        elif saved_model:\n            model = tf.keras.models.load_model(w)\n        elif tflite:\n            interpreter = tf.lite.Interpreter(model_path=w)  # load TFLite model\n            interpreter.allocate_tensors()  # allocate\n            input_details = interpreter.get_input_details()  # inputs\n            output_details = interpreter.get_output_details()  # outputs\n            int8 = input_details[0]['dtype'] == np.uint8  # is TFLite quantized uint8 model\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloaderï¼Œå¯¼å…¥æ•°æ®\n    if webcam:\n        view_img = check_imshow()\n        cudnn.benchmark = True  # set True to speed up constant image size inference\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n        bs = len(dataset)  # batch_size\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n        bs = 1  # batch_size\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inferenceï¼Œå‰å‘ä¼ æ’­\n    if pt and device.type != 'cpu':\n        #è¿›è¡Œä¸€æ¬¡å‰å‘æ¨ç†ï¼Œæµ‹è¯•ç¨‹åºæ˜¯å¦æ­£å¸¸\n        model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(model.parameters())))  # run once\n    dt, seen = [0.0, 0.0, 0.0], 0\n    for path, img, im0s, vid_cap in dataset:\n        print(vid_cap)\n        t1 = time_sync()\n        if onnx:\n            img = img.astype('float32')\n        else:\n            #è½¬æ¢è§†é¢‘ï¼Œå›¾ç‰‡æ ¼å¼\n            img = torch.from_numpy(img).to(device)\n            img = img.half() if half else img.float()  # uint8 to fp16/32\n        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n        if len(img.shape) == 3:\n            img = img[None]  # expand for batch dim\n        t2 = time_sync()\n        dt[0] += t2 - t1\n\n        # Inference\n        if pt:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            pred = model(img, augment=augment, visualize=visualize)[0]\n        elif onnx:\n            if dnn:\n                net.setInput(img)\n                pred = torch.tensor(net.forward())\n            else:\n                pred = torch.tensor(session.run([session.get_outputs()[0].name], {session.get_inputs()[0].name: img}))\n        else:  # tensorflow model (tflite, pb, saved_model)\n            imn = img.permute(0, 2, 3, 1).cpu().numpy()  # image in numpy\n            if pb:\n                pred = frozen_func(x=tf.constant(imn)).numpy()\n            elif saved_model:\n                pred = model(imn, training=False).numpy()\n            elif tflite:\n                if int8:\n                    scale, zero_point = input_details[0]['quantization']\n                    imn = (imn / scale + zero_point).astype(np.uint8)  # de-scale\n                interpreter.set_tensor(input_details[0]['index'], imn)\n                interpreter.invoke()\n                pred = interpreter.get_tensor(output_details[0]['index'])\n                if int8:\n                    scale, zero_point = output_details[0]['quantization']\n                    pred = (pred.astype(np.float32) - zero_point) * scale  # re-scale\n            #çŸ©å½¢æ¡†\n            pred[..., 0] *= imgsz[1]  # x\n            pred[..., 1] *= imgsz[0]  # y\n            pred[..., 2] *= imgsz[1]  # w\n            pred[..., 3] *= imgsz[0]  # h\n            #pred[...,,4]  ä¸ºobjectnessçš„ç½®ä¿¡åº¦\n            #pred[....,5:-1] ä¸ºåˆ†ç±»ç»“æœ\n            pred = torch.tensor(pred)\n        t3 = time_sync()\n        dt[1] += t3 - t2\n\n        # NMS\n        #éæå¤§å€¼æŠ‘åˆ¶å¤„ç†ï¼Œpredå‰å‘ä¼ æ’­ç»“æœ\n        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n        dt[2] += time_sync() - t3\n\n        # Second-stage classifier (optional)\n        #åˆ¤æ–­æ˜¯å¦è¿›è¡ŒäºŒçº§åˆ†ç±»\n        if classify:\n            pred = apply_classifier(pred, modelc, img, im0s)\n\n        # Process predictions\n        #å¯¹æ¯ä¸€å¼ å›¾ç‰‡è¿›è¡Œå¤„ç†\n        for i, det in enumerate(pred):  # per image\n            seen += 1\n            #å¦‚æœè¾“å…¥åŸï¼Œæ¥è‡ªè§†é¢‘\n            if bs&gt;=1:  # batch_size &gt;= 1\n                p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\n            else:\n                p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n\n            p = Path(p)  # to Pathæ˜¯ï¼ŒåŸè§†é¢‘æˆ–å›¾ç‰‡çš„è·¯å¾„ï¼ŒåŒ…å«æ–‡ä»¶å\n            #è®¾ç½®ä¿å­˜è·¯åŠ²\n            save_path = str(save_dir / p.name)  # img.jpg\n            # è®¾ç½®ä¿å­˜æ¡†åæ ‡çš„TXTæ–‡ä»¶è·¯å¾„\n            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n            #è®¾ç½®æ‰“å°ä¿¡æ¯çš„ï¼ˆå›¾ç‰‡çš„å®½é«˜ï¼‰\n            s += '%gx%g ' % img.shape[2:]  # print string\n            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n            imc = im0.copy() if save_crop else im0  # for save_crop\n            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n            if len(det):\n                # Rescale boxes from img_size to im0 size\n                #è°ƒæ•´é¢„æµ‹æ¡†åæ ‡ï¼ŒåŸºäºresize+paidçš„å›¾ç‰‡åæ ‡ï¼Œæ­¤æ—¶çš„åæ ‡æ ¼å¼ä¸ºxyxy\n                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n\n                # Print results\n                #æ‰“å°æ£€æµ‹åˆ°çš„ç±»åˆ«æ•°ç›®\n                for c in det[:, -1].unique():\n                    n = (det[:, -1] == c).sum()  # detections per class\n                    s += f\"{n} {names[int(c)]}{'s' * (n &gt; 1)}, \"  # add to string\n\n                # Write results\n                # ä¿å­˜é¢„æµ‹çš„ç»“æœ\n                for *xyxy, conf, cls in reversed(det):\n                    if save_txt:  # Write to file\n                        #è½¬æ¢åæ ‡ä¿¡æ¯ï¼Œå°†xyxyè½¬æ¢ä¸ºxywhæ ¼å¼ï¼Œå¹¶åˆä¸Šwï¼Œhåšå½’ä¸€åŒ–å¤„ç†\n                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n                        with open(txt_path + '.txt', 'a') as f:\n                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n                    #åœ¨åŸå›¾ä¸Šç”»æ¡†\n                    if save_img or save_crop or view_img:  # Add bbox to image\n                        c = int(cls)  # integer class\n                        #æ ‡ç­¾å€¼\n                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n                        annotator.box_label(xyxy, label, color=colors(c, True))\n                        if save_crop:\n                            save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n\n            # Print time (inference-only)\n            #æ‰“å°å‰å‘ä¼ æ’­+nmsçš„æ—¶é—´\n            print(f'{s}Done. ({t3 - t2:.3f}s)')\n\n            # Stream results\n            #å¦‚æœè®¾ç½®å±•ç¤ºï¼Œåˆ™ç”»å‡ºå›¾ç‰‡ï¼Œæˆ–è§†é¢‘\n            im0 = annotator.result()\n            if view_img:\n                # fps = vid_cap.get(cv2.CAP_PROP_FPS)\n                # cv2.putText(im0, 30, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n                print(str(p))\n                cv2.imshow(str(p), im0)\n\n                key = cv2.waitKey(1)&amp; 0xFF  # 1 millisecond\n                if key == ord('q'):\n                    print(9)\n                    break\n\n\n            # Save results (image with detections)\n            #ä¿å­˜å›¾ç‰‡\n            if save_img:\n                print('save')\n                if dataset.mode == 'image':\n                    cv2.imwrite(save_path, im0)\n                    print(1)\n                else:  # 'video' or 'stream'\n                    print(1111)\n                    if vid_path[i] != save_path:  # new video\n                        vid_path[i] = save_path\n                        if isinstance(vid_writer[i], cv2.VideoWriter):\n                            vid_writer[i].release()  # release previous video writer\n                        if vid_cap:  # video\n                            print(2)\n                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n                            # w,h = im0.shape[1], im0.shape[0]\n                        else:  # stream\n                            print(3)\n                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n                            save_path += '.avi'\n                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (w,h),0)\n                    vid_writer[i].write(im0)\n        if key == ord('q'):\n            cv2.destroyAllWindows()\n            break\n    # Print results\n    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image\n    print(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)\n    if save_txt or save_img:\n        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n        print(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n    if update:\n        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n\n    # vid_writer[i]\n\n\n\n# E:/project/yolov5/yolov5-master/dataset/images/val/IMG_20211019_173105.jpg\ndef parse_opt():#æ ¹æ®éœ€è¦ä¿®æ”¹runä¸­çš„å‚æ•°\n    parser = argparse.ArgumentParser()\n    '''\n    weight:è®­ç»ƒæƒé‡\n    source:æµ‹è¯•æ•°æ®ï¼Œç…§ç‰‡å’Œè§†é¢‘ï¼Œè§†é¢‘æ‘„åƒå¤´ï¼Œè§†é¢‘æµrtsp\n    img-sizeï¼šè¾“å…¥å›¾ç‰‡å¤§å°\n    conf-thres:ç½®ä¿¡åº¦é˜ˆå€¼\n    iou-thres:åšnmsçš„ ioué˜ˆå€¼\n    deviceï¼šè®¾ç½®è®¾å¤‡\n    view-img æ˜¯å¦å±•ç¤ºé¢„æµ‹åçš„ç…§ç‰‡å’Œè§†é¢‘\n    save-TXT æ˜¯å¦å°†åæ ‡æ¡†ä»¥txtæ–‡æœ¬çš„æ ¼å¼ä¿å­˜\n    save-dir è§†é¢‘å›¾ç‰‡é¢„æµ‹åçš„ä¿å­˜è·¯å¾„\n    class è®¾ç½®åªä¿ç•™ä¸€éƒ¨åˆ†ç±»åˆ«\n    agnostic-nms è¿›è¡Œnmsæ˜¯å¦å»é™¤ä¸åŒç±»åˆ«çš„æ¡†\n    augment æ¨ç†çš„æ—¶å€™è¿›è¡Œå¤šå°ºåº¦ï¼Œç¿»è½¬ç­‰æ“ä½œ\n    update å¦‚æœä¸ºtrueï¼Œå¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œstripâ€”optimizeræ“ä½œï¼Œå»é™¤ptæ–‡ä»¶çš„ä¼˜åŒ–å™¨ç­‰ä¿¡æ¯\n    \n    nargs  num of args \n    action å‘½ä»¤è¡Œé‡åˆ°å‚æ•°æ—¶çš„åŠ¨ä½œï¼Œaction = store-trueï¼Œè¡¨ç¤ºæœ‰å‚æ•°ä¼ å…¥æ—¶å°±è®¾ç½®ä¸ºTrue\n    '''\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / './runs/train/exp13/weights/best.pt', help='model path(s)')#æƒé‡æ–‡ä»¶\n    parser.add_argument('--source', type=str, default=ROOT / '0', help='file/dir/URL/glob, 0 for webcam')#ä¿®æ”¹è¦æ£€æµ‹çš„æ–‡ä»¶ç±»å‹\n    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--view-img', action='store_true', help='show results')\n    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n    parser.add_argument('--augment', action='store_true', help='augmented inference')\n    parser.add_argument('--visualize', action='store_true', help='visualize features')\n    parser.add_argument('--update', action='store_true', help='update all models')\n    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')\n    parser.add_argument('--name', default='exp', help='save results to project/name')\n    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n    opt = parser.parse_args()\n    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n    print_args(FILE.stem, opt)\n    return opt\n\n\ndef main(opt):\n    check_requirements(exclude=('tensorboard', 'thop'))\n    run(**vars(opt))\n\n\nif __name__ == \"__main__\":\n    opt = parse_opt()\n    main(opt)\n\n</code></pre>\n<h2><a id=\"trainpy_5638\"></a>train.py</h2>\n<p><a href=\"https://blog.csdn.net/BGMcat/article/details/121035016\">train.py</a></p>\n<h2><a id=\"_5641\"></a>å…¶ä½™</h2>\n<p>è¯¦è§ï¼šhttps://www.bilibili.com/video/BV19K4y197u8?p=36</p>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>", "first_tag": "PHP", "cpp": 0, "csharp": 0, "python": 1, "javascript": 0, "java": 0, "sql": 0, "php": 1, "time": "2021-10-24 09:38:23", "summary": "ç›®å½•ç»“æ„ï¼šä¸»è¦æ˜¯å­˜æ”¾ä¸€äº›è¶…å‚æ•°çš„é…ç½®æ–‡ä»¶è¿™äº›æ–‡ä»¶æ–‡ä»¶æ˜¯ç”¨æ¥é…ç½®è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿˜æœ‰éªŒè¯é›†çš„è·¯å¾„çš„ï¼Œå…¶ä¸­è¿˜åŒ…æ‹¬ç›®æ ‡æ£€æµ‹çš„ç§ç±»æ•°å’Œç§ç±»çš„åç§°ï¼›è¿˜æœ‰ä¸€äº›å®˜æ–¹æä¾›æµ‹è¯•çš„å›¾ç‰‡ã€‚å¦‚æœæ˜¯è®­ç»ƒè‡ªå·±çš„æ•°æ®é›†çš„è¯ï¼Œé‚£ä¹ˆå°±éœ€"}