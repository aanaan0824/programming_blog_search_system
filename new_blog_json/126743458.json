{"blogid": "126743458", "writerAge": "码龄39天", "writerBlogNum": "163", "writerCollect": "13", "writerComment": "7", "writerFan": "175", "writerGrade": "5级", "writerIntegral": "1807", "writerName": "幸福的小浣熊", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_126743458.jpg", "writerRankTotal": "13246", "writerRankWeekly": "4247", "writerThumb": "9", "writerVisitNum": "12889", "blog_read_count": "11", "blog_time": "于 2022-09-07 12:58:58 发布", "blog_title": "使用神经网络实现进制转换,神经网络的求解方式", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<h2>神经网络的准确率是怎么计算的？</h2>\n<p>其实神经网络的准确率的标准是自己定义的。我把你的例子赋予某种意义讲解：1，期望输出[1001]，每个元素代表一个属性是否存在。</p>\n<p>像着4个元素分别表示：是否肺炎，是否肝炎，是否肾炎，是否胆炎，1表示是，0表示不是。2，你的神经网络输出必定不可能全部都是输出只有0,1的输出。</p>\n<p>绝大部分是像[0.99680.00000.00010.9970]这样的输出，所以只要输出中的某个元素大于一定的值，例如0.7，我们就认为这个元素是1，即是有某种炎。</p>\n<p>否则为0，所以你的[0.99680.00000.00010.9970]可以看成是[1,0,0,1],。</p>\n<p>3，所以一般神经网络的输出要按一定的标准定义成另一种输出（像上面说的），看调整后的输出和期望输出是否一致，一致的话算正确，不一致算错误。</p>\n<p>4，用总量为n的检验样本对网络进行评价，输出调整后的输出，统计错误的个数，记为m。所以检验正确率可以定义为n/m。</p>\n<p><strong>谷歌人工智能写作项目：神经网络伪原创</strong></p>\n<p><img alt=\"\" src=\"..\\..\\static\\image\\1343ebe46ee142b89e7d10fa2aee3b80.png\"/></p>\n<h2>神经网络算法原理</h2>\n<p>一共有四种算法及原理，如下所示：1、自适应谐振理论（ART）网络自适应谐振理论（ART）网络具有不同的方案<strong><a href=\"http://www.xiezuomaoai.com/\" title=\"写作猫\">写作猫</a></strong>。一个ART-1网络含有两层一个输入层和一个输出层。</p>\n<p>这两层完全互连，该连接沿着正向（自底向上）和反馈（自顶向下）两个方向进行。2、学习矢量量化（LVQ）网络学习矢量量化（LVQ）网络，它由三层神经元组成，即输入转换层、隐含层和输出层。</p>\n<p>该网络在输入层与隐含层之间为完全连接，而在隐含层与输出层之间为部分连接，每个输出神经元与隐含神经元的不同组相连接。</p>\n<p>3、Kohonen网络Kohonen网络或自组织特征映射网络含有两层，一个输入缓冲层用于接收输入模式，另一个为输出层，输出层的神经元一般按正则二维阵列排列，每个输出神经元连接至所有输入神经元。</p>\n<p>连接权值形成与已知输出神经元相连的参考矢量的分量。4、Hopfield网络Hopfield网络是一种典型的递归网络，这种网络通常只接受二进制输入（0或1）以及双极输入（+1或-1）。</p>\n<p>它含有一个单层神经元，每个神经元与所有其他神经元连接，形成递归结构。扩展资料：人工神经网络算法的历史背景：该算法系统是20世纪40年代后出现的。</p>\n<p>它是由众多的神经元可调的连接权值连接而成，具有大规模并行处理、分布式信息存储、良好的自组织自学习能力等特点。BP算法又称为误差反向传播算法，是人工神经网络中的一种监督式的学习算法。</p>\n<p>BP神经网络算法在理论上可以逼近任意函数，基本的结构由非线性变化单元组成，具有很强的非线性映射能力。</p>\n<p>而且网络的中间层数、各层的处理单元数及网络的学习系数等参数可根据具体情况设定，灵活性很大，在优化、信号处理与模式识别、智能控制、故障诊断等许多领域都有着广泛的应用前景。</p>\n<p>参考资料来源：百度百科——神经网络算法。</p>\n<h2>神经网络里面的代价函数是什么意思?</h2>\n<p>下面是就是神经网络中代价函数J(Θ)J(Θ)的表达式，看起来还是稍微有点复杂。这个表达式到底在计算什么？下面我们先用一个简单的例子来分开一步步计算一下。</p>\n<p>J(Θ)=−1m∑i=1m∑k=1K[y(i)klog((hΘ(x(i)))k)+(1−y(i)k)log(1−(hΘ(x(i)))k)]+λ2m∑l=1L−1∑i=1sl∑j=1sl+1(Θ(l)j,i)2J(Θ)=−1m∑i=1m∑k=1K[yk(i)log⁡((hΘ(x(i)))k)+(1−yk(i))log⁡(1−(hΘ(x(i)))k)]+λ2m∑l=1L−1∑i=1sl∑j=1sl+1(Θj,i(l))2有如下神经网络：其中：LslK=神经网络总共包含的层数=第l层的神经元数目=输出层的神经元数，亦即分类的数目L=神经网络总共包含的层数sl=第l层的神经元数目K=输出层的神经元数，亦即分类的数目假设s1=3,s2=2,s3=3s1=3,s2=2,s3=3，则Θ1Θ1的维度为2×42×4，Θ2Θ2的维度为3×33×3。</p>\n<p>则有：XT=⎡⎣⎢⎢⎢1x1x2x3⎤⎦⎥⎥⎥,Θ1=[θ110θ120θ111θ121θ112θ122θ113θ123]2×4,Θ2=⎡⎣⎢⎢θ210θ220θ230θ211θ221θ231θ212θ222θ232⎤⎦⎥⎥3×3XT=[1x1x2x3],Θ1=[θ101θ111θ121θ131θ201θ211θ221θ231]2×4,Θ2=[θ102θ112θ122θ202θ212θ222θ302θ312θ322]3×3先回忆一下正向传播的计算公式： z(j)=Θ(j−1)a(j−1)……(1)a(j)=g(z(j)),setting a(j)0=1……(2)hΘ(x)=a(j)=g(z(j))……(3)z(j)=Θ(j−1)a(j−1)……(1)a(j)=g(z(j)),setting a0(j)=1……(2)hΘ(x)=a(j)=g(z(j))……(3)详解戳此处 此时我们先忽略regularizedterm ①当m=1时； J(Θ)=−1m∑k=1K[y(i)klog((hΘ(x(i)))k)+(1−y(i)k)log(1−(hΘ(x(i)))k)]J(Θ)=−1m∑k=1K[yk(i)log⁡((hΘ(x(i)))k)+(1−yk(i))log⁡(1−(hΘ(x(i)))k)]1.令a1=XT;⟹z2=Θ1∗a1=[θ110θ120θ111θ121θ112θ122θ113θ123]2×4×⎡⎣⎢⎢⎢1x1x2x3⎤⎦⎥⎥⎥=[θ110+θ111⋅x1+θ112⋅x2+θ113⋅x3θ120+θ121⋅x1+θ122⋅x2+θ123⋅x3]2×11.令a1=XT;⟹z2=Θ1∗a1=[θ101θ111θ121θ131θ201θ211θ221θ231]2×4×[1x1x2x3]=[θ101+θ111⋅x1+θ121⋅x2+θ131⋅x3θ201+θ211⋅x1+θ221⋅x2+θ231⋅x3]2×1=[z21z22],⟹a2=g(z2);=[z12z22],⟹a2=g(z2);2.给a2添加偏置项，并计算a3即hθ(x) 2.给a2添加偏置项，并计算a3即hθ(x); a2=⎡⎣⎢1a21a22⎤⎦⎥;⟹z3=Θ2∗a2=⎡⎣⎢⎢θ210θ220θ230θ211θ221θ231θ212θ222θ232⎤⎦⎥⎥3×3×⎡⎣⎢1a21a22⎤⎦⎥=⎡⎣⎢⎢z31z32z33⎤⎦⎥⎥;a2=[1a12a22];⟹z3=Θ2∗a2=[θ102θ112θ122θ202θ212θ222θ302θ312θ322]3×3×[1a12a22]=[z13z23z33];⟹hθ(x)=a3=g(z3)=⎡⎣⎢⎢g(z31)g(z32)g(z33)⎤⎦⎥⎥=⎡⎣⎢h(x)1h(x)2h(x)3)⎤⎦⎥⟹hθ(x)=a3=g(z3)=[g(z13)g(z23)g(z33)]=[h(x)1h(x)2h(x)3)]此时我们知道，对于每一个example，最终都会输出3个结果，那么这时代价函数所做的就是将这3个输出取对数然后乘以对应的预期期望值y之后，再累加起来。</p>\n<p>具体如下：假设 input:XT=⎡⎣⎢⎢⎢1x1x2x3⎤⎦⎥⎥⎥;output:y=⎡⎣⎢100⎤⎦⎥=⎡⎣⎢y1y2y3⎤⎦⎥input:XT=[1x1x2x3];output:y=[100]=[y1y2y3]则有： J(Θ)∗m=[−y1×log(h(x)1)−(1−y1)×log(1−h(x)1)]+[−y2×log(h(x)2)−(1−y2)×log(1−h(x)2)]+[−y3×log(h(x)3)−(1−y3)×log(1−h(x)3)]=[−1×log(h(x)1)−(1−1)×log(1−h(x)1)]+[−0×log(h(x)2)−(1−0)×log(1−h(x)2)]+[−0×log(h(x)3)−(1−0)×log(1−h(x)3)]=−log(h(x)1)−log(1−h(x)2)−log(1−h(x)3)J(Θ)∗m=[−y1×log(h(x)1)−(1−y1)×log(1−h(x)1)]+[−y2×log(h(x)2)−(1−y2)×log(1−h(x)2)]+[−y3×log(h(x)3)−(1−y3)×log(1−h(x)3)]=[−1×log(h(x)1)−(1−1)×log(1−h(x)1)]+[−0×log(h(x)2)−(1−0)×log(1−h(x)2)]+[−0×log(h(x)3)−(1−0)×log(1−h(x)3)]=−log(h(x)1)−log(1−h(x)2)−log(1−h(x)3)在matlab中，矢量化之后的代价函数为： J(Θ)=(1/m)∗(sum(−labelY.∗log(Hθ)−(1−labelY).∗log(1−Hθ)));J(Θ)=(1/m)∗(sum(−labelY.∗log(Hθ)−(1−labelY).∗log(1−Hθ)));②当m&gt;1时；J(Θ)=−1m∑i=1m∑k=1K[y(i)klog((hΘ(x(i)))k)+(1−y(i)k)log(1−(hΘ(x(i)))k)]J(Θ)=−1m∑i=1m∑k=1K[yk(i)log⁡((hΘ(x(i)))k)+(1−yk(i))log⁡(1−(hΘ(x(i)))k)]此时，对于每一个example都会产生一个上面的代价，所以只需要把所有的对于每一个example产生的代价累加起来即可。</p>\n<p>再来分解一下：假设，X=⎡⎣⎢⎢111x11x21x31x12x22x32x13x23x33⎤⎦⎥⎥,假设，X=[1x11x21x311x12x22x321x13x23x33],1.令a1=XT;⟹z2=Θ1∗a1=[θ110θ120θ111θ121θ112θ122θ113θ123]2×4×⎡⎣⎢⎢⎢⎢1x11x12x131x21x22x231x31x32x33⎤⎦⎥⎥⎥⎥4×3=1.令a1=XT;⟹z2=Θ1∗a1=[θ101θ111θ121θ131θ201θ211θ221θ231]2×4×[111x11x12x13x21x22x23x31x32x33]4×3=[θ110+θ111⋅x11+θ112⋅x12+θ113⋅x13θ120+θ121⋅x11+θ122⋅x12+θ123⋅x13θ110+θ111⋅x21+θ112⋅x22+θ113⋅x23θ120+θ121⋅x21+θ122⋅x22+θ123⋅x23θ110+θ111⋅x31+θ112⋅x32+θ113⋅x33θ120+θ121⋅x31+θ122⋅x32+θ123⋅x33]2×3[θ101+θ111⋅x11+θ121⋅x21+θ131⋅x31θ101+θ111⋅x12+θ121⋅x22+θ131⋅x32θ101+θ111⋅x13+θ121⋅x23+θ131⋅x33θ201+θ211⋅x11+θ221⋅x21+θ231⋅x31θ201+θ211⋅x12+θ221⋅x22+θ231⋅x32θ201+θ211⋅x13+θ221⋅x23+θ231⋅x33]2×3=[z211z221z212z222z213z223]2×3,⟹a2=g(z2);=[z112z122z132z212z222z232]2×3,⟹a2=g(z2);2.给a2添加偏置项，并计算a3即hθ(x) 2.给a2添加偏置项，并计算a3即hθ(x);a2=⎡⎣⎢1a211a2211a212a2221a213a223⎤⎦⎥3×3;⟹z3=Θ2∗a2=⎡⎣⎢⎢θ210θ220θ230θ211θ221θ231θ212θ222θ232⎤⎦⎥⎥3×3×⎡⎣⎢1a211a2211a212a2221a213a223⎤⎦⎥3×3a2=[111a112a122a132a212a222a232]3×3;⟹z3=Θ2∗a2=[θ102θ112θ122θ202θ212θ222θ302θ312θ322]3×3×[111a112a122a132a212a222a232]3×3⟹hθ(x)=a3=g(z3)=⎡⎣⎢⎢g(z311)g(z321)g(z331)g(z312g(z322g(z332)g(z313))g(z323))g(z333)⎤⎦⎥⎥⟹hθ(x)=a3=g(z3)=[g(z113)g(z123g(z133))g(z213)g(z223g(z233))g(z313)g(z323)g(z333)]=⎡⎣⎢⎢⎢⎢m=1时每个example对应的所有输出；h(x1)1h(x1)2h(x1)3m=2时h(x2)1h(x2)2h(x2)3m=3时；h(x3)1h(x3)2h(x3)3⎤⎦⎥⎥⎥⎥=[m=1时每个example对应的所有输出；m=2时m=3时；h(x1)1h(x2)1h(x3)1h(x1)2h(x2)2h(x3)2h(x1)3h(x2)3h(x3)3]假设 input:X=⎡⎣⎢⎢111x11x21x31x12x22x32x13x23x33⎤⎦⎥⎥；output:y=⎡⎣⎢122⎤⎦⎥=⎡⎣⎢y1y2y3⎤⎦⎥input:X=[1x11x21x311x12x22x321x13x23x33]；output:y=[122]=[y1y2y3]该例子的背景为用神经网络识别手写体，即y1=1表示期望输出为1,y2=y3=2,表示其期望输出为2。</p>\n<p>在计算代价函数的时候要将其每一个对应的输出转换为只含有0，1的向量y1=1表示期望输出为1,y2=y3=2,表示其期望输出为2。</p>\n<p>在计算代价函数的时候要将其每一个对应的输出转换为只含有0，1的向量则有: y1=⎡⎣⎢100⎤⎦⎥;y2=⎡⎣⎢010⎤⎦⎥;y3=⎡⎣⎢010⎤⎦⎥⟹labelY=⎡⎣⎢⎢⎢m=1100m=2010m=3010⎤⎦⎥⎥⎥y1=[100];y2=[010];y3=[010]⟹labelY=[m=1m=2m=3100011000]对于如何将普通的输出值转换成只含有0,1的向量，戳此处则有（Malab中的矢量化形式）： J(Θ)=(1/m)∗(sum(sum[−labelY.∗log(Hθ)−(1−labelY).∗log(1−Hθ)]));J(Θ)=(1/m)∗(sum(sum[−labelY.∗log(Hθ)−(1−labelY).∗log(1−Hθ)]));加上regularizedterm regular=λ2m∑l=1L−1∑i=1sl∑j=1sl+1(Θ(l)j,i)2;regular=λ2m∑l=1L−1∑i=1sl∑j=1sl+1(Θj,i(l))2;其实regularizedterm就是所有每一层的参数(Θlj,i,j≠0,即除了每一层的第一列偏置项所对应的参数)(Θj,il,j≠0,即除了每一层的第一列偏置项所对应的参数)的平方和相加即可。</p>\n<p>具体到本文的例子就是：Θ1=[θ110θ120θ111θ121θ112θ122θ113θ123]2×4,Θ2=⎡⎣⎢⎢θ210θ220θ230θ211θ221θ231θ212θ222θ232⎤⎦⎥⎥3×3Θ1=[θ101θ111θ121θ131θ201θ211θ221θ231]2×4,Θ2=[θ102θ112θ122θ202θ212θ222θ302θ312θ322]3×3regular=(θ111)2+(θ112)2+(θ113)2+(θ121)2+(θ122)2+(θ123)2+(θ211)2+(θ212)2+(θ221)2+(θ222)2+(θ231)2+(θ232)2regular=(θ111)2+(θ121)2+(θ131)2+(θ211)2+(θ221)2+(θ231)2+(θ112)2+(θ122)2+(θ212)2+(θ222)2+(θ312)2+(θ322)2Matlab中矢量化为：s_Theta1=sum(Theta1.^2);%先求所有元素的平方，然后再每一列相加r_Theta1=sum(s_Theta1)-s_Theta1(1,1);%减去第一列的和s_Theta2=sum(Theta2.^2);r_Theta2=sum(s_Theta2)-s_Theta2(1,1);regular=(lambda/(2*m))*(r_Theta1+r_Theta2);。</p>\n<h2>对如下BP神经网络，写出它的计算公式（含学习公式），并对其初始权值以及样本x1=1，x</h2>\n<h2>什么是BP神经网络？</h2>\n<p>。</p>\n<p>BP算法的基本思想是：学习过程由信号正向传播与误差的反向回传两个部分组成；正向传播时，输入样本从输入层传入，经各隐层依次逐层处理，传向输出层，若输出层输出与期望不符，则将误差作为调整信号逐层反向回传，对神经元之间的连接权矩阵做出处理，使误差减小。</p>\n<p>经反复学习，最终使误差减小到可接受的范围。具体步骤如下：1、从训练集中取出某一样本，把信息输入网络中。2、通过各节点间的连接情况正向逐层处理后，得到神经网络的实际输出。</p>\n<p>3、计算网络实际输出与期望输出的误差。4、将误差逐层反向回传至之前各层，并按一定原则将误差信号加载到连接权值上，使整个神经网络的连接权值向误差减小的方向转化。</p>\n<p>5、対训练集中每一个输入—输出样本对重复以上步骤，直到整个训练样本集的误差减小到符合要求为止。</p>\n<h2>用BP神经网络建立数学模型，MATLAB实现，怎样得到输入到输出的计算公式</h2>\n<p>% 计算S1与S2层的输出A1=tansig(W1*p,B1);t=purelin(W2*A1,B2);这就是p到t的映射关系。</p>\n<p>BP（BackPropagation）神经网络是1986年由Rumelhart和McCelland为首的科学家小组提出，是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。</p>\n<p>BP网络能学习和存贮大量的输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断调整网络的权值和阈值，使网络的误差平方和最小。</p>\n<p>BP神经网络模型拓扑结构包括输入层（input）、隐层(hiddenlayer)和输出层(outputlayer)。</p>\n<h2>BP神经网络的原理的BP什么意思</h2>\n<p>人工神经网络有很多模型，但是日前应用最广、基本思想最直观、最容易被理解的是多层前馈神经网络及误差逆传播学习算法（ErrorBack-Prooaeation），简称为BP网络。</p>\n<p>在1986年以Rumelhart和McCelland为首的科学家出版的《ParallelDistributedProcessing》一书中，完整地提出了误差逆传播学习算法，并被广泛接受。</p>\n<p>多层感知网络是一种具有三层或三层以上的阶层型神经网络。</p>\n<p>典型的多层感知网络是三层、前馈的阶层网络（图4.1），即：输入层、隐含层（也称中间层）、输出层，具体如下：图4.1三层BP网络结构（1）输入层输入层是网络与外部交互的接口。</p>\n<p>一般输入层只是输入矢量的存储层，它并不对输入矢量作任何加工和处理。输入层的神经元数目可以根据需要求解的问题和数据表示的方式来确定。</p>\n<p>一般而言，如果输入矢量为图像，则输入层的神经元数目可以为图像的像素数，也可以是经过处理后的图像特征数。</p>\n<p>（2）隐含层1989年，RobertHechtNielsno证明了对于任何在闭区间内的一个连续函数都可以用一个隐层的BP网络来逼近，因而一个三层的BP网络可以完成任意的n维到m维的映射。</p>\n<p>增加隐含层数虽然可以更进一步的降低误差、提高精度，但是也使网络复杂化，从而增加了网络权值的训练时间。</p>\n<p>误差精度的提高也可以通过增加隐含层中的神经元数目来实现，其训练效果也比增加隐含层数更容易观察和调整，所以一般情况应优先考虑增加隐含层的神经元个数，再根据具体情况选择合适的隐含层数。</p>\n<p>（3）输出层输出层输出网络训练的结果矢量，输出矢量的维数应根据具体的应用要求来设计，在设计时，应尽可能减少系统的规模，使系统的复杂性减少。</p>\n<p>如果网络用作识别器，则识别的类别神经元接近1，而其它神经元输出接近0。</p>\n<p>以上三层网络的相邻层之间的各神经元实现全连接，即下一层的每一个神经元与上一层的每个神经元都实现全连接，而且每层各神经元之间无连接，连接强度构成网络的权值矩阵W。</p>\n<p>BP网络是以一种有教师示教的方式进行学习的。首先由教师对每一种输入模式设定一个期望输出值。然后对网络输入实际的学习记忆模式，并由输入层经中间层向输出层传播（称为“模式顺传播”）。</p>\n<p>实际输出与期望输出的差即是误差。按照误差平方最小这一规则，由输出层往中间层逐层修正连接权值，此过程称为“误差逆传播”（陈正昌，2005）。</p>\n<p>所以误差逆传播神经网络也简称BP（BackPropagation）网。随着“模式顺传播”和“误差逆传播”过程的交替反复进行。</p>\n<p>网络的实际输出逐渐向各自所对应的期望输出逼近，网络对输入模式的响应的正确率也不断上升。通过此学习过程，确定下各层间的连接权值后。</p>\n<p>典型三层BP神经网络学习及程序运行过程如下（标志渊，2006）：（1）首先，对各符号的形式及意义进行说明：网络输入向量Pk=（a1，a2，...，an）；网络目标向量Tk=（y1，y2，...，yn）；中间层单元输入向量Sk=（s1，s2，...，sp），输出向量Bk=（b1，b2，...，bp）；输出层单元输入向量Lk=（l1，l2，...，lq），输出向量Ck=（c1，c2，...，cq）；输入层至中间层的连接权wij，i=1，2，...，n，j=1，2，...p；中间层至输出层的连接权vjt，j=1，2，...，p，t=1，2，...，p；中间层各单元的输出阈值θj，j=1，2，...，p；输出层各单元的输出阈值γj，j=1，2，...，p；参数k=1，2，...，m。</p>\n<p>（2）初始化。给每个连接权值wij、vjt、阈值θj与γj赋予区间（-1，1）内的随机值。（3）随机选取一组输入和目标样本提供给网络。</p>\n<p>（4）用输入样本、连接权wij和阈值θj计算中间层各单元的输入sj，然后用sj通过传递函数计算中间层各单元的输出bj。</p>\n<p>基坑降水工程的环境效应与评价方法bj=f（sj）j=1，2，...，p（4.5）（5）利用中间层的输出bj、连接权vjt和阈值γt计算输出层各单元的输出Lt，然后通过传递函数计算输出层各单元的响应Ct。</p>\n<p>基坑降水工程的环境效应与评价方法Ct=f（Lt）t=1，2，...，q（4.7）（6）利用网络目标向量，网络的实际输出Ct，计算输出层的各单元一般化误差。</p>\n<p>基坑降水工程的环境效应与评价方法（7）利用连接权vjt、输出层的一般化误差dt和中间层的输出bj计算中间层各单元的一般化误差。</p>\n<p>基坑降水工程的环境效应与评价方法（8）利用输出层各单元的一般化误差与中间层各单元的输出bj来修正连接权vjt和阈值γt。</p>\n<p>基坑降水工程的环境效应与评价方法（9）利用中间层各单元的一般化误差，输入层各单元的输入Pk=（a1，a2，...，an）来修正连接权wij和阈值θj。</p>\n<p>基坑降水工程的环境效应与评价方法（10）随机选取下一个学习样本向量提供给网络，返回到步骤（3），直到m个训练样本训练完毕。</p>\n<p>（11）重新从m个学习样本中随机选取一组输入和目标样本，返回步骤（3），直到网路全局误差E小于预先设定的一个极小值，即网络收敛。如果学习次数大于预先设定的值，网络就无法收敛。（12）学习结束。</p>\n<p>可以看出，在以上学习步骤中，（8）、（9）步为网络误差的“逆传播过程”，（10）、（11）步则用于完成训练和收敛过程。通常，经过训练的网络还应该进行性能测试。</p>\n<p>测试的方法就是选择测试样本向量，将其提供给网络，检验网络对其分类的正确性。测试样本向量中应该包含今后网络应用过程中可能遇到的主要典型模式（宋大奇，2006）。</p>\n<p>这些样本可以直接测取得到，也可以通过仿真得到，在样本数据较少或者较难得到时，也可以通过对学习样本加上适当的噪声或按照一定规则插值得到。</p>\n<p>为了更好地验证网络的泛化能力，一个良好的测试样本集中不应该包含和学习样本完全相同的模式（董军，2007）。</p>\n<h2>神经网络参数如何确定</h2>\n<p>神经网络各个网络参数设定原则：①、网络节点 网络输入层神经元节点数就是系统的特征因子(自变量)个数，输出层神经元节点数就是系统目标个数。隐层节点选按经验选取，一般设为输入层节点数的75%。</p>\n<p>如果输入层有7个节点，输出层1个节点，那么隐含层可暂设为5个节点，即构成一个7-5-1BP神经网络模型。在系统训练时，实际还要对不同的隐层节点数4、5、6个分别进行比较，最后确定出最合理的网络结构。</p>\n<p>②、初始权值的确定 初始权值是不应完全相等的一组值。已经证明，即便确定 存在一组互不相等的使系统误差更小的权值，如果所设Wji的的初始值彼此相等，它们将在学习过程中始终保持相等。</p>\n<p>故而，在程序中，我们设计了一个随机发生器程序，产生一组一0.5~+0.5的随机数，作为网络的初始权值。</p>\n<p>③、最小训练速率 在经典的BP算法中，训练速率是由经验确定，训练速率越大，权重变化越大，收敛越快；但训练速率过大，会引起系统的振荡，因此，训练速率在不导致振荡前提下，越大越好。</p>\n<p>因此，在DPS中，训练速率会自动调整，并尽可能取大一些的值，但用户可规定一个最小训练速率。该值一般取0.9。④、动态参数 动态系数的选择也是经验性的，一般取0.6~0.8。</p>\n<p>⑤、允许误差 一般取0.001~0.00001，当2次迭代结果的误差小于该值时，系统结束迭代计算，给出结果。⑥、迭代次数 一般取1000次。</p>\n<p>由于神经网络计算并不能保证在各种参数配置下迭代结果收敛，当迭代结果不收敛时，允许最大的迭代次数。⑦、Sigmoid参数该参数调整神经元激励函数形式，一般取0.9~1.0之间。⑧、数据转换。</p>\n<p>在DPS系统中，允许对输入层各个节点的数据进行转换，提供转换的方法有取对数、平方根转换和数据标准化转换。扩展资料：神经网络的研究内容相当广泛，反映了多学科交叉技术领域的特点。</p>\n<p>主要的研究工作集中在以下几个方面：1.生物原型从生理学、心理学、解剖学、脑科学、病理学等方面研究神经细胞、神经网络、神经系统的生物原型结构及其功能机理。</p>\n<p>2.建立模型根据生物原型的研究，建立神经元、神经网络的理论模型。其中包括概念模型、知识模型、物理化学模型、数学模型等。</p>\n<p>3.算法在理论模型研究的基础上构作具体的神经网络模型，以实现计算机模拟或准备制作硬件，包括网络学习算法的研究。这方面的工作也称为技术模型研究。</p>\n<p>神经网络用到的算法就是向量乘法，并且广泛采用符号函数及其各种逼近。并行、容错、可以硬件实现以及自我学习特性，是神经网络的几个基本优点，也是神经网络计算方法与传统方法的区别所在。</p>\n<p>参考资料：百度百科-神经网络（通信定义）</p>\n<p> </p>\n</div>\n</div>", "first_tag": "Others", "cpp": 0, "csharp": 0, "python": 0, "javascript": 0, "java": 0, "sql": 0, "php": 0, "time": "2022-09-07 12:58:58", "summary": "神经网络的准确率是怎么计算的？其实神经网络的准确率的标准是自己定义的。我把你的例子赋予某种意义讲解：，期望输出，每个元素代表一个属性是否存在。像着个元素分别表示：是否肺炎，是否肝炎，是否肾炎，是否胆炎"}