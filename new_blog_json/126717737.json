{"blogid": "126717737", "writerAge": "码龄9年", "writerBlogNum": "529", "writerCollect": "2491", "writerComment": "255", "writerFan": "18315", "writerGrade": "7级", "writerIntegral": "17359", "writerName": "郭老二", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_126717737.jpg", "writerRankTotal": "562", "writerRankWeekly": "747", "writerThumb": "782", "writerVisitNum": "1461917", "blog_read_count": "93", "blog_time": "于 2022-09-06 15:38:49 发布", "blog_title": "【AI】PyTorch入门（二）：训练与预测", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-atom-one-dark\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<h1><a id=\"1AI_0\"></a>1、AI训练、预测步骤</h1>\n<p>采集数据<br/> 标注数据<br/> 加载数据<br/> 预处理数据<br/> 创建模型<br/> 训练——优化模型参数<br/> 保存模型及参数<br/> 加载模型及参数<br/> 预测</p>\n<h1><a id=\"2_11\"></a>2、采集数据</h1>\n<p>以图像处理为例，获取需要的图像数据，将它们缩放成需要的分辨率，分辨率大小和所创建的模型有关。</p>\n<h1><a id=\"3_14\"></a>3、标注数据</h1>\n<p>在有监督的机器学习中，一般会对图片做以下处理：</p>\n<pre><code>如果是检测，需要标记检测物在图片中的坐标、大小；\n如果是分类，将不同类别的图片放入不同的文件夹中。\n</code></pre>\n<h1><a id=\"4_19\"></a>4、加载数据</h1>\n<p>PyTorch 提供特定领域的库，例如：处理文本数据处理库TorchText、 图像数据处理库TorchVision和音频波形处理库TorchAudio。所有这些库都包含数据集，例如TorchVision中的datasets可以下载 CIFAR、COCO、FashionMNIST 等数据集。</p>\n<p>我们以FashionMNIST为例。<br/> Fashion-MNIST：替代MNIST手写数字集的图像数据集，该数据集由衣服、鞋子等服饰组成，包含70000张图像，其中60000张训练图像加10000张测试图像，图像大小为28x28，单通道，共分10个类，如下图，每三行为一类。<br/> <img alt=\"在这里插入图片描述\" src=\"..\\..\\static\\image\\fdd7e2e9e3ec4a54a78983bd64e0b13f.png\"/></p>\n<pre><code class=\"prism language-python\"><span class=\"token comment\"># 导入库</span>\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">from</span> torch <span class=\"token keyword\">import</span> nn\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>data <span class=\"token keyword\">import</span> DataLoader\n<span class=\"token keyword\">from</span> torchvision <span class=\"token keyword\">import</span> datasets\n<span class=\"token keyword\">from</span> torchvision<span class=\"token punctuation\">.</span>transforms <span class=\"token keyword\">import</span> ToTensor\n\n<span class=\"token comment\"># 训练集</span>\ntraining_data <span class=\"token operator\">=</span> datasets<span class=\"token punctuation\">.</span>FashionMNIST<span class=\"token punctuation\">(</span>\n    root<span class=\"token operator\">=</span><span class=\"token string\">\"data\"</span><span class=\"token punctuation\">,</span>\n    train<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    download<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    transform<span class=\"token operator\">=</span>ToTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token comment\">#测试集</span>\ntest_data <span class=\"token operator\">=</span> datasets<span class=\"token punctuation\">.</span>FashionMNIST<span class=\"token punctuation\">(</span>\n    root<span class=\"token operator\">=</span><span class=\"token string\">\"data\"</span><span class=\"token punctuation\">,</span>\n    train<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n    download<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    transform<span class=\"token operator\">=</span>ToTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n</code></pre>\n<p>幸运的话，会打印下载信息，如下所示：</p>\n<pre><code>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n100.0%\nExtracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n100.0%\nExtracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n100.0%\nExtracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n100.0%\nExtracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n</code></pre>\n<h1><a id=\"5_72\"></a>5、预处理数据</h1>\n<pre><code># 每批次大小设为64\nbatch_size = 64\n\n# 将 torchvision.datasets 交给 torch.utils.data.DataLoader 处理\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\n#  N代表数量， C代表channel，H代表高度，W代表宽度\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n</code></pre>\n<p>输出</p>\n<pre><code>Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64\n</code></pre>\n<p>N代表数量-每批次64张图片；<br/> C代表channel-单通道；<br/> H代表高度、W代表宽度，对应分辨率28*28</p>\n<h1><a id=\"6_94\"></a>6、创建模型</h1>\n<pre><code># 如果GPU可用，将数据导入GPU上\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# 从torch.nn.Module的类，nn.Module 是 PyTorch 体系下所有神经网络模块的基类\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        # torch.nn.flatten是一个类，作用为将连续的几个维度展平成一个tensor\n        self.flatten = nn.Flatten()\n        #  用于定义 linear_relu_stack，由多层神经网络构成；\n        # Sequential 意为其下定义的多层操作一个接一个按顺序进行，把它们前后全部拼接在一起。\n        self.linear_relu_stack = nn.Sequential(\n        \t# nn.Linear 是全连接层，28 * 28 表示输入维度数量，512 表示下一层输出数量\n            nn.Linear(28*28, 512),\n            # ReLU：激活函数\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\t\n\t# 向前传播\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n</code></pre>\n<p>打印信息</p>\n<pre><code>Using cpu device\n# 打印网络结构\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n</code></pre>\n<p>关于激活函数：<br/> 如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，需要引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。</p>\n<h1><a id=\"7_144\"></a>7、训练——优化模型参数</h1>\n<p>定义损失函数和优化器</p>\n<pre><code># loss_fn：损失函数，计算实际输出和真实相差多少；\n# CrossEntropyLoss：交叉熵损失函数，做图片分类任务时常用的损失函数。\nloss_fn = nn.CrossEntropyLoss()\n\n# optimizer：优化器，用来训练时候优化模型参数\n# SGD：表示随机梯度下降，用于控制实际输出y与真实y之间的相差有多大\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n</code></pre>\n<p>训练函数，基本流程是：预测、计算误差、梯度置0、反向传播、优化参数</p>\n<pre><code>def train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    # 训练模式\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # 获取预测结果\n        pred = model(X)\n        # 计算预测误差\n        loss = loss_fn(pred, y)\n\n        # 优化器工作之前先将梯度置0，进行归零操作\n        optimizer.zero_grad()\n        # 反向传播\n        loss.backward()\n        # 优化参数\n        optimizer.step()\n\t\t\n\t\t# 每100次打印一次信息\n        if batch % 100 == 0:\n        \t# loss 值越低越好，预测值与真实值越来越靠近，这说明模型设计成功\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n</code></pre>\n<p>评估函数</p>\n<pre><code>def test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    # 评估模式\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n</code></pre>\n<p>开始迭代训练、评估</p>\n<pre><code># 迭代训练五次\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n</code></pre>\n<p>打印信息</p>\n<pre><code>Epoch 1\n-------------------------------\nloss: 2.298977  [    0/60000]\nloss: 2.287373  [ 6400/60000]\nloss: 2.271695  [12800/60000]\nloss: 2.266561  [19200/60000]\nloss: 2.257005  [25600/60000]\nloss: 2.214672  [32000/60000]\nloss: 2.223222  [38400/60000]\nloss: 2.189051  [44800/60000]\nloss: 2.183618  [51200/60000]\nloss: 2.148876  [57600/60000]\nTest Error: \n Accuracy: 41.0%, Avg loss: 2.148859 \n略...\nEpoch 5\n-------------------------------\nloss: 1.321847  [    0/60000]\n略...\nloss: 1.059822  [57600/60000]\nTest Error: \n Accuracy: 64.3%, Avg loss: 1.081540 \nDone!\n</code></pre>\n<h1><a id=\"8_237\"></a>8、保存模型及参数</h1>\n<p>保存模型的常用方法是序列化模型及参数。将模型和参数保存到文件中。</p>\n<pre><code>torch.save(model.state_dict(), \"model.pth\")\n</code></pre>\n<h1><a id=\"9_242\"></a>9、加载模型及参数</h1>\n<pre><code>import torch\nfrom torch import nn\nfrom torchvision.transforms import ToTensor\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        # torch.nn.flatten是一个类，作用为将连续的几个维度展平成一个tensor\n        self.flatten = nn.Flatten()\n        #  用于定义 linear_relu_stack，由多层神经网络构成；\n        # Sequential 意为其下定义的多层操作一个接一个按顺序进行，把它们前后全部拼接在一起。\n        self.linear_relu_stack = nn.Sequential(\n        \t# nn.Linear 是全连接层，28 * 28 表示输入维度数量，512 表示下一层输出数量\n            nn.Linear(28*28, 512),\n            # ReLU：激活函数\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\nmodelRun = NeuralNetwork()\nmodelRun.load_state_dict(torch.load(\"model.pth\"))\n</code></pre>\n<p>加载成功，将会输出打印信息，大意是序列化文件中的模型及参数可以加载到运行模型中。</p>\n<pre><code>&lt;All keys matched successfully&gt;\n</code></pre>\n<h1><a id=\"10_272\"></a>10、预测</h1>\n<pre><code>FashionMNIST数据集标注为0~9，数字对应的服装名称定义在classes中\nclasses = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\n# 加载评估模型\nmodelRun.eval()\n# 从测试集中获取一张图片及对应的标签，例如获取第六张图片，下标为5\nx, y = test_data[5][0], test_data[5][1]\nprint(f'x.shape =  \"{x.shape}\" ')\nprint(f'y=  \"{y}\" ')\n\n# 预测时，不会向后传播、梯度更新，只会向前推理（no_grad）\nwith torch.no_grad():\n    pred = modelRun(x)\n    print(f'pred=  \"{pred}\" ')\n    # 得到预测类别中最高的那一类，再把最高的这一类对应 classes 中的哪一个标签。\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n</code></pre>\n<p>输出结果</p>\n<pre><code>x.shape =  \"torch.Size([1, 28, 28])\" \ny=  \"1\" \npred=  \"tensor([[ 2.1079,  3.4801,  0.4381,  2.6778,  1.0602, -2.3333,  0.9451, -3.2445,\n     -1.9010, -3.4177]])\" \nPredicted: \"Trouser\", Actual: \"Trouser\"\n</code></pre>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>", "first_tag": "Python", "cpp": 0, "csharp": 0, "python": 1, "javascript": 0, "java": 0, "sql": 0, "php": 0, "time": "2022-09-06 15:38:49", "summary": "、训练、预测步骤采集数据标注数据加载数据预处理数据创建模型训练优化模型参数保存模型及参数加载模型及参数预测、采集数据以图像处理为例，获取需要的图像数据，将它们缩放成需要的分辨率，分辨率大小和所创建的模"}