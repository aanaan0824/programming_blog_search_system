{"blogid": "126468733", "writerAge": "码龄9年", "writerBlogNum": "51", "writerCollect": "410", "writerComment": "40", "writerFan": "413", "writerGrade": "5级", "writerIntegral": "1803", "writerName": "天兰之珠", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_126468733.jpg", "writerRankTotal": "11580", "writerRankWeekly": "1896", "writerThumb": "102", "writerVisitNum": "211927", "blog_read_count": "12055", "blog_time": "已于 2022-08-25 10:39:46 修改", "blog_title": "人脸活体检测人脸识别：眨眼+张口", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<h1>一：<a href=\"https://so.csdn.net/so/search?q=dlib&amp;spm=1001.2101.3001.7020\" title=\"dlib\">dlib</a>的shape_predictor_68_face_landmarks模型</h1>\n<p>该模型能够检测人脸的68个特征点（facial landmarks），定位图像中的眼睛，眉毛，鼻子，嘴巴，下颌线（ROI，Region of Interest）</p>\n<p> <img alt=\"\" height=\"429\" src=\"..\\..\\static\\image\\20190409115510428.jpg\" width=\"532\"/></p>\n<p></p>\n<pre><code class=\"language-python\">下颌线[1,17]\n左眼眉毛[18,22]\n右眼眉毛[23,27]\n鼻梁[28,31]\n鼻子[32,36]\n左眼[37,42]\n右眼[43,48]\n       \n上嘴唇外边缘[49,55]  \n上嘴唇内边缘[66,68]   \n下嘴唇外边缘[56,60]  \n下嘴唇内边缘[61,65]\n\n</code></pre>\n<p>在使用的过程中对应的下标要减1，像数组的下标是从0开始。</p>\n<h1>二、眨眼检测</h1>\n<p>基本原理：计算眼睛长宽比 Eye Aspect Ratio，EAR.当人眼睁开时，EAR在某个值上下波动，当人眼闭合时，EAR迅速下降，理论上会接近于零，当时人脸检测模型还没有这么精确。所以我们认为当EAR低于某个阈值时，眼睛处于闭合状态。为检测眨眼次数，需要设置同一次眨眼的连续帧数。眨眼速度比较快，一般1~3帧就完成了眨眼动作。两个阈值都要根据实际情况设置。</p>\n<p><img alt=\"\" height=\"207\" src=\"..\\..\\static\\image\\20190409153629299.jpg\" width=\"308\"/><img alt=\"\" height=\"93\" src=\"..\\..\\static\\image\\20190409153649447.jpg\" width=\"320\"/></p>\n<p> 程序实现：</p>\n<pre><code class=\"language-python\">from imutils import face_utils\nimport numpy as np\nimport dlib\nimport cv2\n\n# 眼长宽比例\ndef eye_aspect_ratio(eye):\n    # (|e1-e5|+|e2-e4|) / (2|e0-e3|)\n    A = np.linalg.norm(eye[1] - eye[5])\n    B = np.linalg.norm(eye[2] - eye[4])\n    C = np.linalg.norm(eye[0] - eye[3])\n    ear = (A + B) / (2.0 * C)\n    return ear\n\n#  进行活体检测（包含眨眼和张嘴）\ndef liveness_detection():\n    vs = cv2.VideoCapture(0)  # 调用第一个摄像头的信息\n\n    # 眼长宽比例值\n    EAR_THRESH = 0.15\n    EAR_CONSEC_FRAMES_MIN = 1\n    EAR_CONSEC_FRAMES_MAX = 3  # 当EAR小于阈值时，接连多少帧一定发生眨眼动作\n\n    # 初始化眨眼的连续帧数\n    blink_counter = 0\n    # 初始化眨眼次数总数\n    blink_total = 0\n\n    print(\"[INFO] loading facial landmark predictor...\")\n    # 人脸检测器\n    detector = dlib.get_frontal_face_detector()\n    # 特征点检测器\n    predictor = dlib.shape_predictor(\"model/shape_predictor_68_face_landmarks.dat\")\n    # 获取左眼的特征点\n    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n    # 获取右眼的特征点\n    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n\n    print(\"[INFO] starting video stream thread...\")\n    while True:\n        flag, frame = vs.read()  # 返回一帧的数据\n        if not flag:\n            print(\"不支持摄像头\", flag)\n            break\n\n        if frame is not None:\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # 转成灰度图像\n            rects = detector(gray, 0)  # 人脸检测\n            # 只能处理一张人脸\n            if len(rects) == 1:\n                shape = predictor(gray, rects[0])  # 保存68个特征点坐标的&lt;class 'dlib.dlib.full_object_detection'&gt;对象\n                shape = face_utils.shape_to_np(shape)  # 将shape转换为numpy数组，数组中每个元素为特征点坐标\n\n                left_eye = shape[lStart:lEnd]  # 取出左眼对应的特征点\n                right_eye = shape[rStart:rEnd]  # 取出右眼对应的特征点\n                left_ear = eye_aspect_ratio(left_eye)  # 计算左眼EAR\n                right_ear = eye_aspect_ratio(right_eye)  # 计算右眼EAR\n                ear = (left_ear + right_ear) / 2.0   # 求左右眼EAR的均值\n\n                left_eye_hull = cv2.convexHull(left_eye)  # 寻找左眼轮廓\n                right_eye_hull = cv2.convexHull(right_eye)  # 寻找右眼轮廓\n                # mouth_hull = cv2.convexHull(mouth)  # 寻找嘴巴轮廓\n                cv2.drawContours(frame, [left_eye_hull], -1, (0, 255, 0), 1)   # 绘制左眼轮廓\n                cv2.drawContours(frame, [right_eye_hull], -1, (0, 255, 0), 1)  # 绘制右眼轮廓\n\n                # EAR低于阈值，有可能发生眨眼，眨眼连续帧数加一次\n                if ear &lt; EAR_THRESH:\n                    blink_counter += 1\n\n                # EAR高于阈值，判断前面连续闭眼帧数，如果在合理范围内，说明发生眨眼\n                else:\n                    if EAR_CONSEC_FRAMES_MIN &lt;= blink_counter &lt;= EAR_CONSEC_FRAMES_MAX:\n                        blink_total += 1\n                    blink_counter = 0\n\n                cv2.putText(frame, \"Blinks: {}\".format(blink_total), (0, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n\n                cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n\n            elif len(rects) == 0:\n                cv2.putText(frame, \"No face!\", (0, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            else:\n                cv2.putText(frame, \"More than one face!\", (0, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL)\n            cv2.imshow(\"Frame\", frame)\n            # 按下q键退出循环（鼠标要点击一下图片使图片获得焦点）\n            if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n                break\n    cv2.destroyAllWindows()\n    vs.release()\n\n\nliveness_detection()</code></pre>\n<p></p>\n<h1>三、张口检测</h1>\n<p>检测原理：类似眨眼检测，计算Mouth Aspect Ratio,MAR.当MAR大于设定的阈值时，认为张开了嘴巴。</p>\n<h2>1：采用的判定是张开后闭合计算一次张嘴动作。</h2>\n<p>mar     # 嘴长宽比例</p>\n<p>MAR_THRESH = 0.2    # 嘴长宽比例值</p>\n<p>mouth_status_open   # 初始化张嘴状态为闭嘴</p>\n<p>当mar大于设定的比例值表示张开，张开后闭合代表一次张嘴动作</p>\n<pre><code class=\"language-python\">   # 通过张、闭来判断一次张嘴动作\n                if mar &gt; MAR_THRESH:\n                     mouth_status_open = 1\n                else:\n                    if mouth_status_open:\n                        mouth_total += 1\n                    mouth_status_open = 0</code></pre>\n<h2>2： 嘴长宽比例的计算</h2>\n<pre><code class=\"language-python\"># 嘴长宽比例\ndef mouth_aspect_ratio(mouth):\n    A = np.linalg.norm(mouth[1] - mouth[7])  # 61, 67\n    B = np.linalg.norm(mouth[3] - mouth[5])  # 63, 65\n    C = np.linalg.norm(mouth[0] - mouth[4])  # 60, 64\n    mar = (A + B) / (2.0 * C)\n    return mar</code></pre>\n<p>原本采用嘴唇外边缘来计算，发现嘟嘴也会被判定为张嘴，故才用嘴唇内边缘进行计算，会更加准确。</p>\n<p>这里mouth下标的值取决于取的是<strong>“mouth”</strong>还是<strong>“inner_mouth”，</strong>由于我要画的轮廓是内嘴，所以我采用的是<strong>inner_mouth</strong></p>\n<pre><code class=\"language-python\"> (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"inner_mouth\"]</code></pre>\n<p>打开以下方法，进入到源码，可以看到每个特征点对应的下标是不一样的，对应的mouth特征点的下标也是不同的</p>\n<p><img alt=\"\" height=\"104\" src=\"..\\..\\static\\image\\d58093f4b0df41f0b411a1cbd62235af.png\" width=\"1129\"/></p>\n<p><img alt=\"\" height=\"1200\" src=\"..\\..\\static\\image\\f50acf8ea289438a9f2ee96152478f70.png\" width=\"1200\"/></p>\n<p> （以上的区间包左边代表开始下标，右边值-1）从上面可知<strong>mouth</strong>是从（48,68），<strong>inner_mouth</strong>从(60, 68)，mouth包含inner_mouth,如果取得是<strong>mouth</strong>的值，则嘴长宽比例的计算如下</p>\n<pre><code class=\"language-python\"># 嘴长宽比例\ndef mouth_aspect_ratio(mouth):\n    # (|m13-m19|+|m15-m17|)/(2|m12-m16|)\n    A = np.linalg.norm(mouth[13] - mouth[19])  # 61, 67\n    B = np.linalg.norm(mouth[15] - mouth[17])  # 63, 65\n    C = np.linalg.norm(mouth[12] - mouth[16])  # 60, 64\n    mar = (A + B) / (2.0 * C)\n    return mar</code></pre>\n<h2>3：完整程序实现如下</h2>\n<pre><code class=\"language-python\">from imutils import face_utils\nimport numpy as np\nimport dlib\nimport cv2\n\n\n# 嘴长宽比例\ndef mouth_aspect_ratio(mouth):\n    A = np.linalg.norm(mouth[1] - mouth[7])  # 61, 67\n    B = np.linalg.norm(mouth[3] - mouth[5])  # 63, 65\n    C = np.linalg.norm(mouth[0] - mouth[4])  # 60, 64\n    mar = (A + B) / (2.0 * C)\n    return mar\n\n#  进行活体检测（张嘴）\ndef liveness_detection():\n    vs = cv2.VideoCapture(0)  # 调用第一个摄像头的信息\n    # 嘴长宽比例值\n    MAR_THRESH = 0.2\n    # 初始化张嘴次数\n    mouth_total = 0\n    # 初始化张嘴状态为闭嘴\n    mouth_status_open = 0\n\n    print(\"[INFO] loading facial landmark predictor...\")\n    # 人脸检测器\n    detector = dlib.get_frontal_face_detector()\n    # 特征点检测器\n    predictor = dlib.shape_predictor(\"model/shape_predictor_68_face_landmarks.dat\")\n    # 获取嘴巴特征点\n    (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"inner_mouth\"]\n\n    print(\"[INFO] starting video stream thread...\")\n    while True:\n\n        flag, frame = vs.read()  # 返回一帧的数据\n        if not flag:\n            print(\"不支持摄像头\", flag)\n            break\n\n        if frame is not None:\n            # 图片转换成灰色（去除色彩干扰，让图片识别更准确）\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            rects = detector(gray, 0)  # 人脸检测\n            # 只能处理一张人脸\n            if len(rects) == 1:\n                shape = predictor(gray, rects[0])  # 保存68个特征点坐标的&lt;class 'dlib.dlib.full_object_detection'&gt;对象\n                shape = face_utils.shape_to_np(shape)  # 将shape转换为numpy数组，数组中每个元素为特征点坐标\n\n                inner_mouth = shape[mStart:mEnd]   # 取出嘴巴对应的特征点\n                mar = mouth_aspect_ratio(inner_mouth)  # 求嘴巴mar的均值\n\n                mouth_hull = cv2.convexHull(inner_mouth)  # 寻找内嘴巴轮廓\n                cv2.drawContours(frame, [mouth_hull], -1, (0, 255, 0), 1)  # 绘制嘴巴轮廓\n\n                # 通过张、闭来判断一次张嘴动作\n                if mar &gt; MAR_THRESH:\n                     mouth_status_open = 1\n                else:\n                    if mouth_status_open:\n                        mouth_total += 1\n                    mouth_status_open = 0\n\n                cv2.putText(frame, \"Mouth: {}\".format(mouth_total),\n                            (130, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n                cv2.putText(frame, \"MAR: {:.2f}\".format(mar), (450, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            elif len(rects) == 0:\n                cv2.putText(frame, \"No face!\", (0, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            else:\n                cv2.putText(frame, \"More than one face!\", (0, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL)\n            cv2.imshow(\"Frame\", frame)\n            # 按下q键退出循环（鼠标要点击一下图片使图片获得焦点）\n            if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n                break\n    cv2.destroyAllWindows()\n    vs.release()\n\n\nliveness_detection()</code></pre>\n<h1>三：眨眼和张嘴结合（摄像头）</h1>\n<pre><code class=\"language-python\">from imutils import face_utils\nimport numpy as np\nimport dlib\nimport cv2\n\n# 眼长宽比例\ndef eye_aspect_ratio(eye):\n    # (|e1-e5|+|e2-e4|) / (2|e0-e3|)\n    A = np.linalg.norm(eye[1] - eye[5])\n    B = np.linalg.norm(eye[2] - eye[4])\n    C = np.linalg.norm(eye[0] - eye[3])\n    ear = (A + B) / (2.0 * C)\n    return ear\n\n# 嘴长宽比例\ndef mouth_aspect_ratio(mouth):\n    A = np.linalg.norm(mouth[1] - mouth[7])  # 61, 67\n    B = np.linalg.norm(mouth[3] - mouth[5])  # 63, 65\n    C = np.linalg.norm(mouth[0] - mouth[4])  # 60, 64\n    mar = (A + B) / (2.0 * C)\n    return mar\n\n#  进行活体检测（包含眨眼和张嘴）\ndef liveness_detection():\n    vs = cv2.VideoCapture(0)  # 调用第一个摄像头的信息\n\n    # 眼长宽比例值\n    EAR_THRESH = 0.15\n    EAR_CONSEC_FRAMES_MIN = 1\n    EAR_CONSEC_FRAMES_MAX = 5  # 当EAR小于阈值时，接连多少帧一定发生眨眼动作\n\n    # 嘴长宽比例值\n    MAR_THRESH = 0.2\n\n    # 初始化眨眼的连续帧数\n    blink_counter = 0\n    # 初始化眨眼次数总数\n    blink_total = 0\n    # 初始化张嘴次数\n    mouth_total = 0\n    # 初始化张嘴状态为闭嘴\n    mouth_status_open = 0\n\n    print(\"[INFO] loading facial landmark predictor...\")\n    # 人脸检测器\n    detector = dlib.get_frontal_face_detector()\n    # 特征点检测器\n    predictor = dlib.shape_predictor(\"model/shape_predictor_68_face_landmarks.dat\")\n    # 获取左眼的特征点\n    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n    # 获取右眼的特征点\n    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n    # 获取嘴巴特征点\n    (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"inner_mouth\"]\n\n    print(\"[INFO] starting video stream thread...\")\n    while True:\n\n        flag, frame = vs.read()  # 返回一帧的数据\n        if not flag:\n            print(\"不支持摄像头\", flag)\n            break\n\n        if frame is not None:\n            # 图片转换成灰色（去除色彩干扰，让图片识别更准确）\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            rects = detector(gray, 0)  # 人脸检测\n            # 只能处理一张人脸\n            if len(rects) == 1:\n                shape = predictor(gray, rects[0])  # 保存68个特征点坐标的&lt;class 'dlib.dlib.full_object_detection'&gt;对象\n                shape = face_utils.shape_to_np(shape)  # 将shape转换为numpy数组，数组中每个元素为特征点坐标\n\n                left_eye = shape[lStart:lEnd]  # 取出左眼对应的特征点\n                right_eye = shape[rStart:rEnd]  # 取出右眼对应的特征点\n                left_ear = eye_aspect_ratio(left_eye)  # 计算左眼EAR\n                right_ear = eye_aspect_ratio(right_eye)  # 计算右眼EAR\n                ear = (left_ear + right_ear) / 2.0   # 求左右眼EAR的均值\n\n                inner_mouth = shape[mStart:mEnd]  # 取出嘴巴对应的特征点\n                mar = mouth_aspect_ratio(inner_mouth)  # 求嘴巴mar的均值\n                left_eye_hull = cv2.convexHull(left_eye)  # 寻找左眼轮廓\n                right_eye_hull = cv2.convexHull(right_eye)  # 寻找右眼轮廓\n                mouth_hull = cv2.convexHull(inner_mouth)  # 寻找内嘴巴轮廓\n                cv2.drawContours(frame, [left_eye_hull], -1, (0, 255, 0), 1)   # 绘制左眼轮廓\n                cv2.drawContours(frame, [right_eye_hull], -1, (0, 255, 0), 1)  # 绘制右眼轮廓\n                cv2.drawContours(frame, [mouth_hull], -1, (0, 255, 0), 1)  # 绘制嘴巴轮廓\n\n                # EAR低于阈值，有可能发生眨眼，眨眼连续帧数加一次\n                if ear &lt; EAR_THRESH:\n                    blink_counter += 1\n\n                # EAR高于阈值，判断前面连续闭眼帧数，如果在合理范围内，说明发生眨眼\n                else:\n                    # if the eyes were closed for a sufficient number of\n                    # then increment the total number of blinks\n                    if EAR_CONSEC_FRAMES_MIN &lt;= blink_counter &lt;= EAR_CONSEC_FRAMES_MAX:\n                        blink_total += 1\n                    blink_counter = 0\n                # 通过张、闭来判断一次张嘴动作\n                if mar &gt; MAR_THRESH:\n                     mouth_status_open = 1\n                else:\n                    if mouth_status_open:\n                        mouth_total += 1\n                    mouth_status_open = 0\n\n                cv2.putText(frame, \"Blinks: {}\".format(blink_total), (0, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n                cv2.putText(frame, \"Mouth: {}\".format(mouth_total),\n                            (130, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n                cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n                cv2.putText(frame, \"MAR: {:.2f}\".format(mar), (450, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            elif len(rects) == 0:\n                cv2.putText(frame, \"No face!\", (0, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            else:\n                cv2.putText(frame, \"More than one face!\", (0, 30),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL)\n            cv2.imshow(\"Frame\", frame)\n            # 按下q键退出循环（鼠标要点击一下图片使图片获得焦点）\n            if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n                break\n    cv2.destroyAllWindows()\n    vs.release()\n\n#  调用摄像头进行张嘴眨眼活体检测\nliveness_detection()</code></pre>\n<h1>四：采用视频进行活体检测</h1>\n<p>最大的区别是原来通过摄像头获取一帧一帧的视频流进行判断，现在是通过视频获取一帧一帧的视频流进行判断</p>\n<h2>1：先看下获取摄像头的图像信息 </h2>\n<pre><code class=\"language-python\"># -*-coding:GBK -*-\nimport cv2\nfrom PIL import Image, ImageDraw\nimport numpy as np\n\n# 1.调用摄像头\n# 2.读取摄像头图像信息\n# 3.在图像上添加文字信息\n# 4.保存图像\n\ncap = cv2.VideoCapture(0)  # 调用第一个摄像头信息\n\nwhile True:\n    flag, frame = cap.read()  # 返回一帧的数据\n    # #返回值：flag：bool值：True：读取到图片，False：没有读取到图片  frame：一帧的图片\n    # BGR是cv2 的图像保存格式，RGB是PIL的图像保存格式，在转换时需要做格式上的转换\n    img_PIL = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    draw = ImageDraw.Draw(img_PIL)\n    draw.text((100, 100), 'press q to exit', fill=(255, 255, 255))\n    # 将frame对象转换成cv2的格式\n    frame = cv2.cvtColor(np.array(img_PIL), cv2.COLOR_RGB2BGR)\n    cv2.imshow('capture', frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        cv2.imwrite('images/out.jpg', frame)\n        break\n\ncap.release()</code></pre>\n<h2>2：获取视频的图像信息 </h2>\n<pre><code class=\"language-python\"># -*-coding:GBK -*-\nimport cv2\nfrom PIL import Image, ImageDraw\nimport numpy as np\n\n# 1.调用摄像头\n# 2.读取摄像头图像信息\n# 3.在图像上添加文字信息\n# 4.保存图像\n\ncap = cv2.VideoCapture(r'video\\face13.mp4')  # 调用第一个摄像头信息\n\nwhile True:\n    flag, frame = cap.read()  # 返回一帧的数据\n    if not flag:\n        break\n    if frame is not None:\n        # BGR是cv2 的图像保存格式，RGB是PIL的图像保存格式，在转换时需要做格式上的转换\n        img_PIL = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        draw = ImageDraw.Draw(img_PIL)\n        draw.text((100, 100), 'press q to exit', fill=(255, 255, 255))\n        # # 将frame对象转换成cv2的格式\n        frame = cv2.cvtColor(np.array(img_PIL), cv2.COLOR_RGB2BGR)\n        cv2.imshow('capture', frame)\n\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            cv2.imwrite('images/out.jpg', frame)\n            break\n\ncv2.destroyAllWindows()\ncap.release()</code></pre>\n<h1>五：视频进行人脸识别和活体检测</h1>\n<h2>1：原理</h2>\n<p>计算当出现1次眨眼或1次张嘴就判断为活人，记录下一帧的人脸图片，和要判定的人员图片进行比对，获取比对后的相似度，进行判断是否是同一个人，为了增加判断的速度，才用2帧进行一次活体检测判断。</p>\n<h2>2：代码实现</h2>\n<pre><code class=\"language-python\">import face_recognition\nfrom imutils import face_utils\nimport numpy as np\nimport dlib\nimport cv2\nimport sys\n\n# 初始化眨眼次数\nblink_total = 0\n# 初始化张嘴次数\nmouth_total = 0\n# 设置图片存储路径\npic_path = r'images\\viode_face.jpg'\n# 图片数量\npic_total = 0\n# 初始化眨眼的连续帧数以及总的眨眼次数\nblink_counter = 0\n# 初始化张嘴状态为闭嘴\nmouth_status_open = 0\n\ndef getFaceEncoding(src):\n    image = face_recognition.load_image_file(src)  # 加载人脸图片\n    # 获取图片人脸定位[(top,right,bottom,left )]\n    face_locations = face_recognition.face_locations(image)\n    img_ = image[face_locations[0][0]:face_locations[0][2], face_locations[0][3]:face_locations[0][1]]\n    img_ = cv2.cvtColor(img_, cv2.COLOR_BGR2RGB)\n    # display(img_)\n    face_encoding = face_recognition.face_encodings(image, face_locations)[0]  # 对人脸图片进行编码\n    return face_encoding\n\n\ndef simcos(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    dist = np.linalg.norm(a - b)  # 二范数\n    sim = 1.0 / (1.0 + dist)  #\n    return sim\n\n# 提供对外比对的接口 返回比对的相似度\ndef comparison(face_src1, face_src2):\n    xl1 = getFaceEncoding(face_src1)\n    xl2 = getFaceEncoding(face_src2)\n    value = simcos(xl1, xl2)\n    print(value)\n\n# 眼长宽比例\ndef eye_aspect_ratio(eye):\n    # (|e1-e5|+|e2-e4|) / (2|e0-e3|)\n    A = np.linalg.norm(eye[1] - eye[5])\n    B = np.linalg.norm(eye[2] - eye[4])\n    C = np.linalg.norm(eye[0] - eye[3])\n    ear = (A + B) / (2.0 * C)\n    return ear\n\n# 嘴长宽比例\ndef mouth_aspect_ratio(mouth):\n    A = np.linalg.norm(mouth[1] - mouth[7])  # 61, 67\n    B = np.linalg.norm(mouth[3] - mouth[5])  # 63, 65\n    C = np.linalg.norm(mouth[0] - mouth[4])  # 60, 64\n    mar = (A + B) / (2.0 * C)\n    return mar\n\n#  进行活体检测（包含眨眼和张嘴）\n#  filePath 视频路径\ndef liveness_detection():\n    global blink_total  # 使用global声明blink_total，在函数中就可以修改全局变量的值\n    global mouth_total\n    global pic_total\n    global blink_counter\n    global mouth_status_open\n    # 眼长宽比例值\n    EAR_THRESH = 0.15\n    EAR_CONSEC_FRAMES_MIN = 1\n    EAR_CONSEC_FRAMES_MAX = 5  # 当EAR小于阈值时，接连多少帧一定发生眨眼动作\n    # 嘴长宽比例值\n    MAR_THRESH = 0.2\n\n    # 人脸检测器\n    detector = dlib.get_frontal_face_detector()\n    # 特征点检测器\n    predictor = dlib.shape_predictor(\"model/shape_predictor_68_face_landmarks.dat\")\n    # 获取左眼的特征点\n    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n    # 获取右眼的特征点\n    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n    # 获取嘴巴特征点\n    (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"inner_mouth\"]\n    vs = cv2.VideoCapture(video_path)\n    # 总帧数(frames)\n    frames = vs.get(cv2.CAP_PROP_FRAME_COUNT)\n    frames_total = int(frames)\n    for i in range(frames_total):\n        ok, frame = vs.read(i)  # 读取视频流的一帧\n        if not ok:\n            break\n        if frame is not None and i % 2 == 0:\n            # 图片转换成灰色（去除色彩干扰，让图片识别更准确）\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            rects = detector(gray, 0)  # 人脸检测\n            # 只能处理一张人脸\n            if len(rects) == 1:\n                if pic_total == 0:\n                    cv2.imwrite(pic_path, frame)  # 存储为图像,保存名为 文件夹名_数字（第几个文件）.jpg\n                    cv2.waitKey(1)\n                    pic_total += 1\n\n                shape = predictor(gray, rects[0])  # 保存68个特征点坐标的&lt;class 'dlib.dlib.full_object_detection'&gt;对象\n                shape = face_utils.shape_to_np(shape)  # 将shape转换为numpy数组，数组中每个元素为特征点坐标\n\n                left_eye = shape[lStart:lEnd]  # 取出左眼对应的特征点\n                right_eye = shape[rStart:rEnd]  # 取出右眼对应的特征点\n                left_ear = eye_aspect_ratio(left_eye)  # 计算左眼EAR\n                right_ear = eye_aspect_ratio(right_eye)  # 计算右眼EAR\n                ear = (left_ear + right_ear) / 2.0   # 求左右眼EAR的均值\n\n                mouth = shape[mStart:mEnd]   # 取出嘴巴对应的特征点\n                mar = mouth_aspect_ratio(mouth)  # 求嘴巴mar的均值\n\n                # EAR低于阈值，有可能发生眨眼，眨眼连续帧数加一次\n                if ear &lt; EAR_THRESH:\n                    blink_counter += 1\n\n                # EAR高于阈值，判断前面连续闭眼帧数，如果在合理范围内，说明发生眨眼\n                else:\n                    if EAR_CONSEC_FRAMES_MIN &lt;= blink_counter &lt;= EAR_CONSEC_FRAMES_MAX:\n                        blink_total += 1\n                    blink_counter = 0\n                # 通过张、闭来判断一次张嘴动作\n                if mar &gt; MAR_THRESH:\n                    mouth_status_open = 1\n                else:\n                    if mouth_status_open:\n                        mouth_total += 1\n                    mouth_status_open = 0\n            elif len(rects) == 0 and i == 90:\n                print(\"No face!\")\n                break\n            elif len(rects) &gt; 1:\n                print(\"More than one face!\")\n        # 判断眨眼次数大于2、张嘴次数大于1则为活体,退出循环\n        if blink_total &gt;= 1 or mouth_total &gt;= 1:\n            break\n    cv2.destroyAllWindows()\n    vs.release()\n\n\n# video_path, src = sys.argv[1], sys.argv[2]\n\nvideo_path = r'video\\face13.mp4'      # 输入的video文件夹位置\n# src = r'C:\\Users\\666\\Desktop\\zz5.jpg'\nliveness_detection()\nprint(\"眨眼次数》》\", blink_total)\nprint(\"张嘴次数》》\", mouth_total)\n# comparison(pic_path, src)</code></pre>\n<h1>六：涉及到的代码</h1>\n<p>代码包含face_recognition库所有功能的用例，和上面涉及到的dilb库进行人脸识别的所有代码</p>\n<p><a href=\"https://download.csdn.net/download/u012693479/86438215\" title=\"使用dilb、face_recognition库实现，眨眼+张嘴的活体检测、和人脸识别功能。包含摄像头和视频-Python文档类资源-CSDN下载\">使用dilb、face_recognition库实现，眨眼+张嘴的活体检测、和人脸识别功能。包含摄像头和视频-Python文档类资源-CSDN下载</a></p>\n<p></p>\n<p>参考：</p>\n<p><a href=\"https://blog.csdn.net/Lee_01/article/details/89151044?spm=1001.2101.3001.6650.14&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-14-89151044-blog-116206175.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-14-89151044-blog-116206175.pc_relevant_default&amp;utm_relevant_index=18\" title=\"使用dlib人脸检测模型进行人脸活体检测：眨眼+张口_Lee_01的博客-CSDN博客\">使用dlib人脸检测模型进行人脸活体检测：眨眼+张口_Lee_01的博客-CSDN博客</a></p>\n<p><a href=\"https://blog.csdn.net/hongbin_xu/article/details/79033116?spm=1001.2101.3001.6650.8&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-8-79033116-blog-89151044.pc_relevant_show_downloadRating&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-8-79033116-blog-89151044.pc_relevant_show_downloadRating&amp;utm_relevant_index=13\" title=\"python dlib学习（十一）：眨眼检测_hongbin_xu的博客-CSDN博客_眨眼检测算法\">python dlib学习（十一）：眨眼检测_hongbin_xu的博客-CSDN博客_眨眼检测算法</a>       </p>\n<p><a href=\"https://blog.csdn.net/IT_charge/article/details/119087178\" title=\"Python开发系统实战项目：人脸识别门禁监控系统_闭关修炼——暂退的博客-CSDN博客_face_encodings\">Python开发系统实战项目：人脸识别门禁监控系统_闭关修炼——暂退的博客-CSDN博客_face_encodings</a></p>\n</div>\n</div>", "first_tag": "Python", "cpp": 0, "csharp": 0, "python": 1, "javascript": 0, "java": 0, "sql": 0, "php": 0, "time": "2022-08-25 10:39:46", "summary": "一：的模型该模型能够检测人脸的个特征点，定位图像中的眼睛，眉毛，鼻子，嘴巴，下颌线，下颌线左眼眉毛右眼眉毛鼻梁鼻子左眼右眼上嘴唇外边缘上嘴唇内边缘下嘴唇外边缘下嘴唇内边缘在使用的过程中对应的下标要减，"}