{"blogid": "126716285", "writerAge": "码龄14年", "writerBlogNum": "538", "writerCollect": "7548", "writerComment": "2852", "writerFan": "19530", "writerGrade": "8级", "writerIntegral": "39902", "writerName": "铭毅天下", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_126716285.jpg", "writerRankTotal": "142", "writerRankWeekly": "3426", "writerThumb": "4219", "writerVisitNum": "5711747", "blog_read_count": "14", "blog_time": "于 2022-09-05 07:05:39 发布", "blog_title": "Kibana 8.X 如何做出靠谱的词云图？", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<div id=\"js_content\">\n<h2>1、问题产生</h2>\n<p style=\"text-align:justify;\">Kibana 实现一个词云效果并不复杂，甚至可以说非常简单。</p>\n<p style=\"text-align:justify;\">大致可以分成如下几个步骤：</p>\n<ul><li><p style=\"text-align:justify;\">步骤1：已有索引待做词云的 text 类型字段设置：fielddata 为true，以便基于分词结果聚合操作。</p></li><li><p style=\"text-align:justify;\">步骤2：在 8.X 的 kibana 的 Data Views关联索引。</p></li><li><p style=\"text-align:justify;\">步骤3：在dashboard控制面板选择 Aggregation Based 下的 Tag cloud，选择步骤1设定的字段，选择好时间范围，词云就可以生成。</p></li></ul>\n<p style=\"text-align:justify;\">以构造微博数据（假数据）为例，词云效果如下所示：</p>\n<img alt=\"cfaf7ca5ef3983ad0768d0f4c5e48833.png\" src=\"..\\..\\static\\image\\cfaf7ca5ef3983ad0768d0f4c5e48833.png\"/>\n<p style=\"text-align:justify;\">问题来了！怎么那么多单字效果，有没有办法去掉，让词云效果相对靠谱可靠？</p>\n<h2>2、方案探讨</h2>\n<p style=\"text-align:justify;\">从目标出发思考，既然分词结果大局已定。把单字的分词全部删除掉不就可以了吗？于是有了方案一。</p>\n<h3>方案一：Kibana 控制面板过滤掉单字索引</h3>\n<img alt=\"41bdc8a225b9406514c930d9ec8e8a75.png\" src=\"..\\..\\static\\image\\41bdc8a225b9406514c930d9ec8e8a75.png\"/>\n<p>加上后，看看效果：</p>\n<img alt=\"e9b91edd4ee71e8756e10d7c95deb66c.png\" src=\"..\\..\\static\\image\\e9b91edd4ee71e8756e10d7c95deb66c.png\"/>\n<p style=\"text-align:justify;\">并不乐观，因为我们的方案仅是将能看到的 Top 50 里的单字给去掉了。</p>\n<p style=\"text-align:justify;\">新的 Top 50 单字仍然会出现。</p>\n<p style=\"text-align:justify;\">也就是说：方案一仅“治疗表明”，不能由表及里。</p>\n<p style=\"text-align:justify;\">这个问题曾困惑我很久，我一度认为，把单字穷举出来，全部删掉即可。</p>\n<p style=\"text-align:justify;\">后来，思来死去，发现思考问题方向不对，应该从“源头”解决问题。</p>\n<p style=\"text-align:justify;\">于是，有了方案二。</p>\n<h3>2.2 方案二：分词阶段过滤掉单字词项</h3>\n<p style=\"text-align:justify;\">中文分词我们依然选择的 medcl 大佬开源的 IK 分词下的 ik_smart 粗粒度分词器。ik 中文分词插件支持两种分词效果：</p>\n<p style=\"text-align:justify;\">其一：ik_max_word，细粒度分词。</p>\n<p style=\"text-align:justify;\">其二：ik_smart, 粗粒度分词。</p>\n<p style=\"text-align:justify;\">原有的分词已经构建完毕，如何基于已有成熟分词再构建新的分词器呢？</p>\n<p style=\"text-align:justify;\">这时候，脑海里要对分词 analysis 的三部分组成要“门儿清”。</p>\n<img alt=\"a98328c8dc389976462d96b2491995ed.png\" src=\"..\\..\\static\\image\\a98328c8dc389976462d96b2491995ed.png\"/>\n<p style=\"text-align:justify;\">我们的 <code>tokenizer</code> 已选定 <code>ik_smart</code>，不能修改。可动的只有：<code>character filter</code> 和 <code>token filter</code>，而能实现仅保留 两个 或者两个以上分词的效果的需要借助：<code>token filter</code> 下的 <code>length token filter</code> 实现。</p>\n<p style=\"text-align:justify;\"><code>length token filter</code> 的本质如其定义：</p>\n<blockquote>\n<p>emoves tokens shorter or longer than specified character lengths. ”</p>\n</blockquote>\n<p style=\"text-align:justify;\">中文释义为：“删除比指定字符长度更短或更长的标记”。</p>\n<p style=\"text-align:justify;\">接下来，我们实战一把。</p>\n<h2>3、基于自定义分词实现靠谱词云效果</h2>\n<p style=\"text-align:justify;\">如前方案二所述，在分词处做“手脚”，能实现自主、可控的分词粒度。</p>\n<h3>3.1 步骤1：自定义分词</h3>\n<p style=\"text-align:justify;\">如下 DSL 实现了自定义索引。</p>\n<p style=\"text-align:justify;\">在原有 <code>ik_smart</code> 分词器的基础上，添加了“bigger_than_2” 过滤器，实现了将小于2个字符的 分词项过滤掉的效果。</p>\n<pre class=\"has\"><code class=\"language-go\">PUT weibo_index\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"ik_smart_ext\": {\n          \"tokenizer\": \"ik_smart\",\n          \"filter\": [\n            \"bigger_than_2\"\n          ]\n        }\n      },\n      \"filter\": {\n        \"bigger_than_2\": {\n          \"type\": \"length\",\n          \"min\": 2\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"analyzer\": \"ik_smart_ext\",\n        \"fielddata\": true\n      },\n      \"insert_time\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}</code></pre>\n<h3>3.2 步骤2：重新生成索引且reindex数据</h3>\n<pre class=\"has\"><code class=\"language-go\">POST _reindex\n{\n  \"source\": {\"index\": \"weibo_index_20220901\"},\n  \"dest\": {\"index\": \"weibo_index_20220904\"}\n}</code></pre>\n<h3>3.3 步骤3：重新生成词云</h3>\n<img alt=\"b4d7d5da8cc9739fa795537761b4db37.png\" src=\"..\\..\\static\\image\\b4d7d5da8cc9739fa795537761b4db37.png\"/>\n<p>依然不是最完美的词云效果，但是，比未处理前已经好很多。</p>\n<h2>4、小结</h2>\n<p style=\"text-align:justify;\">解决问题的时候，多从源头思考，换一个思路，效果会好很多。</p>\n<p style=\"text-align:justify;\">大家有任何 ElasticStack 相关技术问题都欢迎留言交流。</p>\n<p style=\"text-align:justify;\">ElasticStack 视频不定期更新中：</p>\n<ul><li><p>B站：https://space.bilibili.com/471049389</p></li><li><p>视频号：铭毅天下</p></li></ul>\n<h2>推荐阅读</h2>\n<ol><li><p style=\"text-align:left;\"><a href=\"\">如何从0到1打磨一门 Elasticsearch 线上直播课？</a></p></li><li><p style=\"text-align:left;\"><a href=\"\">重磅 | 死磕 Elasticsearch 方法论认知清单（2021年国庆更新版）</a></p></li><li><p style=\"text-align:left;\"><a href=\"\">如何系统的学习 Elasticsearch ？</a> </p></li><li><p style=\"text-align:left;\"><a href=\"\">你“寒”你的，我“暖”我的</a></p></li></ol>\n</div>\n</div>\n</div>", "first_tag": "Others", "cpp": 0, "csharp": 0, "python": 0, "javascript": 0, "java": 0, "sql": 0, "php": 0, "time": "2022-09-05 07:05:39", "summary": "、问题产生实现一个词云效果并不复杂，甚至可以说非常简单。大致可以分成如下几个步骤：步骤：已有索引待做词云的类型字段设置：为，以便基于分词结果聚合操作。步骤：在的的关联索引。步骤：在控制面板选择下的，选"}