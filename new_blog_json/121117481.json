{"blogid": "121117481", "writerAge": "码龄2年", "writerBlogNum": "110", "writerCollect": "378", "writerComment": "72", "writerFan": "4839", "writerGrade": "4级", "writerIntegral": "1353", "writerName": "hah杨大仙", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_121117481.jpg", "writerRankTotal": "42760", "writerRankWeekly": "2206", "writerThumb": "183", "writerVisitNum": "193381", "blog_read_count": "5808", "blog_time": "于 2021-11-03 12:16:42 发布", "blog_title": "JAVA高效率 (秒级) 将千万条数据导入数据库 (已封装工具类)【详解】【一看就懂】", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<p><img alt=\"\" height=\"718\" src=\"https://img-blog.csdnimg.cn/9aee32319e15472ab75c6e435816c583.gif\" width=\"1200\"/></p>\n<p><em><span style=\"background-color:#f3f3f4;\">该gif做了加速处理，便于观看~ </span></em></p>\n<blockquote>\n<p>今天在将一个<strong>500w+条数据</strong>的文件导入至数据库时，遇到一个异常，相信做大数据应该都有遇到。500w条数据说多不多，说少也不少。既然问题出现了，那么就一定要解决。</p>\n</blockquote>\n<p><strong>异常如下图所示：</strong></p>\n<p>造成异常的方法代码在如下链接：</p>\n<p><a class=\"link-info\" href=\"https://blog.csdn.net/m0_55710969/article/details/121019684?spm=1001.2014.3001.5501\" title=\"MySQL数据库10秒内插入百万条数据 （多字段）【详解】\">MySQL数据库10秒内插入百万条数据 （多字段）【详解】</a></p>\n<p><img alt=\"\" height=\"276\" src=\"..\\..\\static\\image\\b57fd560ac534933965626fa6dfcce25.png\" width=\"1101\"/></p>\n<hr/>\n<p><img alt=\"\" height=\"251\" src=\"..\\..\\static\\image\\05d2589b3a30431184e426fd7961e3b0.png\" width=\"1074\"/></p>\n<blockquote>\n<p><strong>由于数据通过该方式转换为一条sql，执行读取工作量过于庞大，导致所创建的对象都为强引用，垃圾回收机制无法释放内存，所导致堆内存溢出而造成的异常。</strong></p>\n</blockquote>\n<p>想了一下，虽然通过prepareStatement的addBatch( )方法可以做到只访问一次数据库，面对100w的数据CPU还可以处理，但是遇到千万级的数据或更多就会出现问题（甚至损耗cpu）。</p>\n<blockquote>\n<p><strong>于是便在此基础上做了升级，并封装了工具类，代码如下：（一行代码一行注释，不理解之处留言即可）</strong></p>\n</blockquote>\n<pre><code class=\"language-java\">public class DataImport {\n    //   参数一：数据库连接对象、参数二：流文件读取出的集合、参数三：从第几条数据开始读取，目的是排除表头、参数四：是否包含主键、参数五：每次批量执行添加数据的数量、参数六：sql语句\n    public static void dispose(Connection conn, List&lt;String&gt; list, Integer startRows, boolean includePrimaryKey, Integer size, String sql) {\n        try {\n            conn.setAutoCommit(false);  //  设置事物手动提交\n            PreparedStatement ps = conn.prepareStatement(sql);\n            String[] split = null;\n            if (includePrimaryKey) {    //  包含主键,只需判断一次\n                for (int i = startRows; i &lt; list.size(); i++) {\n                    //  按逗号切割字符串，-1代表忽略数组长度，避免数组长度越界异常\n                    split = list.get(i).split(\",\", -1);\n                    /*下方代码产生警告提示的原因：同一项目中，有重复的代码块（idea很好的提示。但是这里无法将判断放在循环内，不然会多出百万次判断使程序缓慢）*/\n                    for (int j = 0; j &lt; split.length; j++) {   //  遍历刚刚获取的数组\n                        //   对集合中的每条数据进行处理，将字符串中多出的引号去掉，避免录入数据库时因字段类型不匹配而导致的格式转换异常\n                        ps.setObject(j + 1, split[j].replace(\"\\\"\", \"\"));    //  循环赋值\n                    }\n                    ps.addBatch();   //  将所有数据转为一条sql\n                    if (i % size == 0 &amp;&amp; i != 0) {   //  如果i能整除size，即执行循环体\n                        ps.executeBatch();           //  批量执行sql\n                        conn.commit();               //  事物手动提交\n                        conn.setAutoCommit(false);   //  重新设置事物为手动提交\n                        ps = conn.prepareStatement(sql);   //  再次为ps对象赋值\n                    }\n                }\n            } else {    //  不包含主键\n                for (int i = startRows; i &lt; list.size(); i++) {\n                    String s = list.get(i);\n                    //  将集合中的对象从第一个逗号切割，substring包头不包尾，因此此处需加1\n                    split = s.substring(s.indexOf(\",\") + 1).split(\",\", -1);\n                    for (int j = 0; j &lt; split.length; j++) {\n                        ps.setObject(j + 1, split[j].replace(\"\\\"\", \"\"));\n                    }\n                    ps.addBatch();\n                    if (i % size == 0 &amp;&amp; i != 0) {\n                        ps.executeBatch();\n                        conn.commit();\n                        conn.setAutoCommit(false);\n                        ps = conn.prepareStatement(sql);\n                    }\n                }\n            }\n            ps.executeBatch();  //  循环外提交是因为可能会出现循环内条件不成立而未提交过的情况\n            conn.commit();      //  提交事物，避免脏数据（事物太长也有弊端）\n            ps.close();         //  关闭资源\n            conn.close();\n        } catch (Exception throwables) {\n            throwables.printStackTrace();\n        }\n    }\n}</code></pre>\n<ul><li><strong>参数一</strong>：数据库连接对象；</li><li><strong>参数二</strong>：IO流读取文件得到的集合；</li><li><strong>参数三</strong>：代表从文件的第几条数据开始读取，主要目的是为了排除表头；</li><li><strong>参数四</strong>：存入数据库时是否需要包含主键</li><li><strong>参数五</strong>：每次批量执行sql时添加数据的数量；</li><li><strong>参数六</strong>：所要执行的sql语句；</li></ul>\n<blockquote>\n<p><strong> 测试代码如下（拿去测试）：</strong></p>\n<p><em>（所用到的<strong>工具类源码</strong>可通过下方链接获取：</em></p>\n<p><em>BaseDao（JDBCUtil）工具类：<em><a class=\"link-info\" href=\"https://blog.csdn.net/m0_55710969/article/details/121030508?spm=1001.2014.3001.5501\" title=\"JDBC访问数据库的BaseDao工具类代码【拿去使用】\">JDBC访问数据库的BaseDao工具类代码【拿去使用】</a></em></em></p>\n<p><em>IO流读取文件工具类：<em><a class=\"link-info\" href=\"https://blog.csdn.net/m0_55710969/article/details/121110079?spm=1001.2014.3001.5501\" title=\"IO流读取文件 工具类 【拿去使用】\">IO流读取文件 工具类 【拿去使用】</a>）</em></em></p>\n</blockquote>\n<pre><code class=\"language-java\"> @Test\n public void BigData1000wTest() throws Exception {\n     //   通过JDBCUtil工具类获取数据库连接对象\n     Connection conn = BaseDao.getConn(\"million-test\", \"root\", \"root\");\n     //   StreamUtil是已经封装好的使用流读取文件的工具类\n     List&lt;String&gt; list = StreamUtil.readingLineFormTextFile(new File(\"D://milliondatatest//test(500W).csv\"));\n     String sql = \"insert into mysqltest values(?,?,?,?)\";   //  定义要导入数据的sql,无需主键将第一个?设置为null\n     long start = System.currentTimeMillis();   //   获取方法开始执行前的时间（单位：毫秒）\n     //  调用刚刚封装好的工具类\n     DataImport.dispose(conn, list, 0, true, 1000000, sql);\n     long end = System.currentTimeMillis();     //   获取方法执行结束后的时间\n     //   相减即可得到插入所有数据的耗时   秒=毫秒/1000;\n     System.out.println(\"成功导入\" + list.size() + \"条数据！！时长：\" + (end - start) / 1000 + \"秒\");\n }</code></pre>\n<blockquote>\n<p>效果如图所示：</p>\n</blockquote>\n<p><img alt=\"\" height=\"731\" src=\"..\\..\\static\\image\\b88bab60b43f495ea62100e7cb705de7.png\" width=\"1190\"/></p>\n<p>数据库如下：<img alt=\"\" height=\"811\" src=\"..\\..\\static\\image\\1e75631aeaeb4994bc85a5526480c540.png\" width=\"1109\"/></p>\n<p><strong><span style=\"color:#fe2c24;\">成功！</span></strong></p>\n<p>这么运行的原理就是让程序<strong>分批处理</strong>sql语句，不会像之前那么吃cpu，我的cpu大概稳定在30%~50%之间。</p>\n<blockquote>\n<p>当然如果你的数据在100W左右，还是升级前的快一些，毕竟只造访一次数据库，执行一条sql语句与一次事物。升级前如下：</p>\n<p><a class=\"link-info\" href=\"https://blog.csdn.net/m0_55710969/article/details/121019684?spm=1001.2014.3001.5501\" title=\"MySQL数据库10秒内插入百万条数据 （多字段）【详解】\">MySQL数据库10秒内插入百万条数据 （多字段）【详解】</a></p>\n<hr/>\n<p><strong>千万级数据</strong>甚至更多数据使用本文工具类也是<strong>没有问题</strong>的，该工具类尽可能多的避免了创建对象，使用时只需根据不同电脑性能控制每次执行sql要导入的数据量即可。</p>\n</blockquote>\n<hr/>\n<p>如有错误，欢迎指正 </p>\n<p></p>\n<p>Thanks </p>\n<p></p>\n</div>\n</div>", "first_tag": "SQL", "cpp": 0, "csharp": 0, "python": 0, "javascript": 0, "java": 1, "sql": 1, "php": 0, "time": "2021-11-03 12:16:42", "summary": "该做了加速处理，便于观看今天在将一个条数据的文件导入至数据库时，遇到一个异常，相信做大数据应该都有遇到。条数据说多不多，说少也不少。既然问题出现了，那么就一定要解决。异常如下图所示：造成异常的方法代码"}