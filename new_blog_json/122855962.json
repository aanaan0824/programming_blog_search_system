{"blogid": "122855962", "writerAge": "码龄5年", "writerBlogNum": "25", "writerCollect": "631", "writerComment": "283", "writerFan": "3227", "writerGrade": "3级", "writerIntegral": "670", "writerName": "围白的尾巴", "writerProfileAdress": "..\\..\\static\\writer_image\\profile_122855962.jpg", "writerRankTotal": "30627", "writerRankWeekly": "4798", "writerThumb": "148", "writerVisitNum": "66511", "blog_read_count": "9986", "blog_time": "已于 2022-04-16 17:11:48 修改", "blog_title": "YOLOv5+姿态估计HRnet与SimDR检测视频中的人体关键点", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"../../static/bootstrap/css/csdnstyle.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<h1>一、前言</h1>\n<p>        由于工程项目中需要对视频中的person进行关键点检测，我测试各个算法后，并没有采用比较应用化成熟的Openpose，决定采用检测精度更高的HRnet系列。但是由于官方给的算法只能测试数据集，需要自己根据算法模型编写实例化代码。<br/>         本文根据<a class=\"link-info\" href=\"https://github.com/leeyegy/SimDR\" title=\"SimDR\">SimDR</a>工程实现视频关键点检测。SimDR根据HRnet改进而来，整个工程既包括HRnet又包括改进后的算法，使用起来较为方便，而且本文仅在cpu上就可以跑通整个工程。</p>\n<h1>二、环境配置</h1>\n<p>        python的环境主要就是按照工程中SimDR与yolov5的requirement.txt安装即可。总之缺啥装啥。</p>\n<h1>三、工程准备</h1>\n<h2>1、克隆工程</h2>\n<pre><code class=\"language-bash\">git clone https://github.com/leeyegy/SimDR.git  #克隆姿态估计工程\ncd SimDR\ngit clone -b v5.0 https://github.com/ultralytics/yolov5.git #在姿态估计工程中添加yolov5算法\n</code></pre>\n<h2>2、目标检测</h2>\n<h3>①添加权重文件</h3>\n<p>        添加yolov5x.pt（见评论区网盘）到‘ SimDR/yolov5/weights/ ’文件夹下。</p>\n<h3>②获取边界框</h3>\n<p>        在yolov5文件夹下新建YOLOv5.py，复制以下内容到文件中。<span style=\"color:#fe2c24;\">注意：根据大家的反馈，不同的电脑，导入yolov5相关包时会不同的方式，代码中我是from yolov5.xxx import xxx，但是有些可以不用前面的yolov5，大家自行尝试哈。一般出现No module xxx 都是有关yolov5 的包导入出错哈。</span></p>\n<pre><code class=\"language-python\">import argparse\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom numpy import random\nimport sys\nimport os\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadStreams, LoadImages\nfrom yolov5.utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\nfrom yolov5.utils.plots import plot_one_box\nfrom yolov5.utils.torch_utils import select_device, load_classifier, time_synchronized\nfrom  yolov5.utils.datasets import letterbox\n\n\nclass Yolov5():\n    def __init__(self, weights=None, opt=None, device=None):\n        \"\"\"\n\n        @param weights:\n        @param save_txt:\n        @param opt:\n        @param device:\n        \"\"\"\n        self.weights = weights\n        self.device = device\n        # save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n        # save_dir.mkdir(parents=True, exist_ok=True)  # make dir\n        self.img_size = 640\n        self.model = attempt_load(weights, map_location=self.device)\n        self.stride = int(self.model.stride.max())\n        self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names\n        self.colors = [[random.randint(0, 255) for _ in range(3)] for _ in self.names]\n        self.opt = opt\n\n    def detect(self,img0):\n        \"\"\"\n\n        @param img0: 输入图片  shape=[h,w,3]\n        @return:\n        \"\"\"\n        person_boxes = np.ones((6))\n        img = letterbox(img0, self.img_size, stride=self.stride)[0]\n\n        # Convert\n        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n        img = np.ascontiguousarray(img)\n        img = torch.from_numpy(img).to(self.device)\n        img = img.float()  # uint8 to fp16/32\n        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n        pred = self.model(img, augment=self.opt.augment)[0]\n        # Apply NMS\n        pred = non_max_suppression(pred, self.opt.conf_thres, self.opt.iou_thres, classes=self.opt.classes, agnostic=self.opt.agnostic_nms)\n        for i, det in enumerate(pred):\n            if len(det):\n                # Rescale boxes from img_size to im0 size\n                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n                boxes = reversed(det)\n                boxes = boxes.cpu().numpy() #2022.04.06修改，在GPU上跑boxes无法直接转numpy数据\n                #for i , box in enumerate(np.array(boxes)):\n                for i , box in enumerate(boxes):\n                    if int(box[-1]) == 0 and box[-2]&gt;=0.7:\n                        person_boxes=np.vstack((person_boxes , box))\n\n        #                 label = f'{self.names[int(box[-1])]} {box[-2]:.2f}'\n        #                 print(label)\n        #                 plot_one_box(box, img0, label=label, color=self.colors[int(box[-1])], line_thickness=3)\n        # cv2.imwrite('result1.jpg',img0)\n        # print(s)\n        # print(person_boxes,np.ndim(person_boxes))\n        if np.ndim(person_boxes)&gt;=2 :\n            person_boxes_result = person_boxes[1:]\n            boxes_result = person_boxes[1:,:4]\n        else:\n            person_boxes_result = []\n            boxes_result = []\n        return person_boxes_result,boxes_result\n\ndef yolov5test(opt,path = ''):\n    detector = Yolov5(weights='weights/yolov5x.pt',opt=opt,device=torch.device('cpu'))\n    img0 = cv2.imread(path)\n    personboxes ,boxes= detector.detect(img0)\n    for i,(x1,y1,x2,y2) in enumerate(boxes):\n        print(x1,y1,x2,y2)\n    print(personboxes,'\\n',boxes)\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n    parser.add_argument('--augment', action='store_true', help='augmented inference')\n    parser.add_argument('--update', action='store_true', help='update all model')\n    parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n    parser.add_argument('--name', default='exp', help='save results to project/name')\n    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n    opt = parser.parse_args()\n    print(opt)\n    # check_requirements(exclude=('pycocotools', 'thop'))\n\n    with torch.no_grad():\n\n        yolov5test(opt,'data/images/zidane.jpg')\n</code></pre>\n<h3>③路径问题</h3>\n<p>        本文代码是在pycharm中运行，yolov5工程的加入导致有些文件夹名称相同，pycharm会搞混，可能会出现某些包找不到。这里需要先运行一下YOLOv5.py脚本，根据报错改一下import的内容。举个例子，./SimDR/yolov5/models/experimental.py 文件中会出现图片中的问题</p>\n<p style=\"text-align:center;\"><img alt=\"\" height=\"66\" src=\"..\\..\\static\\image\\341b3901cb094996ad7c70287daa4513.png\" width=\"390\"/></p>\n<p>改成如下即可，其他的文件改法相同。</p>\n<p style=\"text-align:center;\"><img alt=\"\" height=\"53\" src=\"..\\..\\static\\image\\786f163fece34419b9bb329e95be1c07.png\" width=\"438\"/></p>\n<h3>④添加SPPF模块</h3>\n<p><img alt=\"\" height=\"29\" src=\"..\\..\\static\\image\\d8f4632aba5648ba9637dcaa9bd3cb01.png\" width=\"1000\"/></p>\n<p> yolov5 v5.0工程中没有SPPF模块，此时我们需要在./SimDR/yolov5/models/common.py文件末尾加入以下代码。</p>\n<pre><code class=\"language-python\">import warnings\n\nclass SPPF(nn.Module):\n    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher\n    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n\n    def forward(self, x):\n        x = self.cv1(x)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning\n            y1 = self.m(x)\n            y2 = self.m(y1)\n            return self.cv2(torch.cat([x, y1, y2, self.m(y2)], 1))</code></pre>\n<h2>3、姿态估计</h2>\n<h3>①添加权重</h3>\n<p>        在SimDR文件夹下新建weight/hrnet文件夹，添加pose_hrnet_w48_384x288.pth等文件（见评论区网盘）</p>\n<h3>②修改yaml文件</h3>\n<p>        SimDR/experiments/文件夹下是coco与mpii数据集的配置文件，本文以coco为例。</p>\n<p style=\"text-align:center;\"><img alt=\"\" height=\"139\" src=\"..\\..\\static\\image\\04728efa87314056901cc9cb2d3c02ba.png\" width=\"355\"/></p>\n<p>         接下来，修改./SimDR/experiments/coco/hrnet/heatmap/w48_384x288_adam_lr1e-3.yaml文件中的TEST部分的MODEL_FILE路径，如图所示。（SimDR算法的配置文件同理改动。）</p>\n<p style=\"text-align:center;\"><img alt=\"\" height=\"287\" src=\"..\\..\\static\\image\\bcb420ec4f534afc9b1266e5677ef8d2.png\" width=\"764\"/></p>\n<h3>③获取关键点</h3>\n<p>        在’ SimDR/ ‘文件夹下新建Point_detect.py ,复制以下内容到文件中。</p>\n<p>       <span style=\"color:#fe2c24;\"> 注意：代码第12行的路径要改成自己yolov5工程的路径，有这条代码才能正常运行。</span></p>\n<p><span style=\"color:#38d8f0;\">【2022.04.16更新：根据评论区的建议，为关键点增加置信度值，这个值我是根据模型输出经过softmax后取最大值（关键点坐标就是这个最大值的索引），仅供参考。根据这个置信度可以解决半身照也会绘制全部点的问题。】</span></p>\n<pre><code class=\"language-python\">import cv2\nimport numpy as np\nimport torch\nfrom torchvision.transforms import transforms\nimport torch.nn.functional as F\nfrom lib.config import cfg\n\nfrom yolov5.YOLOv5 import Yolov5\nfrom lib.utils.transforms import  flip_back_simdr,transform_preds,get_affine_transform\nfrom lib import models\nimport argparse\nimport sys\nsys.path.insert(0, 'D:\\\\Study\\\\Pose Estimation\\\\SimDR\\\\yolov5')\n\n\nclass Points():\n    def __init__(self,\n                 model_name='sa-simdr',\n                 resolution=(384,288),\n                 opt=None,\n                 yolo_weights_path=\"./yolov5/weights/yolov5x.pt\",\n                ):\n        \"\"\"\n        Initializes a new SimpleHRNet object.\n        HRNet (and YOLOv3) are initialized on the torch.device(\"device\") and\n        its (their) pre-trained weights will be loaded from disk.\n\n        Args:\n            c (int): number of channels (when using HRNet model) or resnet size (when using PoseResNet model).\n            nof_joints (int): number of joints.\n            checkpoint_path (str): path to an official hrnet checkpoint or a checkpoint obtained with `train_coco.py`.\n            model_name (str): model name (HRNet or PoseResNet).\n                Valid names for HRNet are: `HRNet`, `hrnet`\n                Valid names for PoseResNet are: `PoseResNet`, `poseresnet`, `ResNet`, `resnet`\n                Default: \"HRNet\"\n            resolution (tuple): hrnet input resolution - format: (height, width).\n                Default: (384, 288)\n            interpolation (int): opencv interpolation algorithm.\n                Default: cv2.INTER_CUBIC\n            multiperson (bool): if True, multiperson detection will be enabled.\n                This requires the use of a people detector (like YOLOv3).\n                Default: True\n            return_heatmaps (bool): if True, heatmaps will be returned along with poses by self.predict.\n                Default: False\n            return_bounding_boxes (bool): if True, bounding boxes will be returned along with poses by self.predict.\n                Default: False\n            max_batch_size (int): maximum batch size used in hrnet inference.\n                Useless without multiperson=True.\n                Default: 16\n            yolo_model_def (str): path to yolo model definition file.\n                Default: \"./model/detectors/yolo/config/yolov3.cfg\"\n            yolo_class_path (str): path to yolo class definition file.\n                Default: \"./model/detectors/yolo/data/coco.names\"\n            yolo_weights_path (str): path to yolo pretrained weights file.\n                Default: \"./model/detectors/yolo/weights/yolov3.weights.cfg\"\n            device (:class:`torch.device`): the hrnet (and yolo) inference will be run on this device.\n                Default: torch.device(\"cpu\")\n        \"\"\"\n        self.model_name = model_name\n        self.resolution = resolution  # in the form (height, width) as in the original implementation\n        self.aspect_ratio = resolution[1]/resolution[0]\n        self.yolo_weights_path = yolo_weights_path\n        self.flip_pairs = [[1, 2], [3, 4], [5, 6], [7, 8],\n                           [9, 10], [11, 12], [13, 14], [15, 16]]\n        self.device = torch.device(opt.device)\n        cfg.defrost()\n        if model_name in ('sa-simdr','sasimdr','sa_simdr'):\n            if resolution ==(384,288):\n                cfg.merge_from_file('./experiments/coco/hrnet/sa_simdr/w48_384x288_adam_lr1e-3_split1_5_sigma4.yaml')\n            elif resolution == (256,192):\n                cfg.merge_from_file('./experiments/coco/hrnet/sa_simdr/w48_256x192_adam_lr1e-3_split2_sigma4.yaml')\n            else:\n                raise ValueError('Wrong cfg file')\n        elif model_name in ('simdr'):\n                if resolution == (256, 192):\n                    cfg.merge_from_file('./experiments/coco/hrnet/simdr/nmt_w48_256x192_adam_lr1e-3.yaml')\n                else:\n                    raise ValueError('Wrong cfg file')\n        elif model_name in ('hrnet','HRnet','Hrnet'):\n            if resolution == (384,288):\n                cfg.merge_from_file('./experiments/coco/hrnet/heatmap/w48_384x288_adam_lr1e-3.yaml')\n            elif resolution == (256,192):\n                cfg.merge_from_file('./experiments/coco/hrnet/heatmap/w48_256x192_adam_lr1e-3.yaml')\n            else:\n                raise ValueError('Wrong cfg file')\n        else:\n            raise ValueError('Wrong model name.')\n        cfg.freeze()\n        self.model = eval('models.' + cfg.MODEL.NAME + '.get_pose_net')(\n            cfg, is_train=False)\n\n        print('=&gt; loading model from {}'.format(cfg.TEST.MODEL_FILE))\n        checkpoint_path = cfg.TEST.MODEL_FILE\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        if 'model' in checkpoint:\n            self.model.load_state_dict(checkpoint['model'])\n        else:\n            self.model.load_state_dict(checkpoint)\n\n        if 'cuda' in str(self.device):\n            print(\"device: 'cuda' - \", end=\"\")\n\n            if 'cuda' == str(self.device):\n                # if device is set to 'cuda', all available GPUs will be used\n                print(\"%d GPU(s) will be used\" % torch.cuda.device_count())\n                device_ids = None\n            else:\n                # if device is set to 'cuda:IDS', only that/those device(s) will be used\n                print(\"GPU(s) '%s' will be used\" % str(self.device))\n                device_ids = [int(x) for x in str(self.device)[5:].split(',')]\n        elif 'cpu' == str(self.device):\n            print(\"device: 'cpu'\")\n        else:\n            raise ValueError('Wrong device name.')\n\n        self.model = self.model.to(self.device)\n        self.model.eval()\n        self.detector = Yolov5(\n                               weights=yolo_weights_path,\n                               opt=opt ,\n                               device=self.device)\n        self.transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((self.resolution[0], self.resolution[1])),  # (height, width)\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n\n    def _box2cs(self, box):\n        x, y, w, h = box[:4]\n        return self._xywh2cs(x, y, w, h)\n\n    def _xywh2cs(self, x, y, w, h):\n        center = np.zeros((2), dtype=np.float32)\n        center[0] = x + w * 0.5\n        center[1] = y + h * 0.5\n\n        if w &gt; self.aspect_ratio * h:\n            h = w * 1.0 / self.aspect_ratio\n        elif w &lt; self.aspect_ratio * h:\n            w = h * self.aspect_ratio\n        scale = np.array(\n            [w * 1.0 / 200, h * 1.0 / 200],\n            dtype=np.float32)\n        if center[0] != -1:\n            scale = scale * 1.25\n\n        return center, scale\n    def predict(self, image):\n        \"\"\"\n        Predicts the human pose on a single image or a stack of n images.\n\n        Args:\n            image (:class:`np.ndarray`):\n                the image(s) on which the human pose will be estimated.\n\n                image is expected to be in the opencv format.\n                image can be:\n                    - a single image with shape=(height, width, BGR color channel)\n                    - a stack of n images with shape=(n, height, width, BGR color channel)\n\n        Returns:\n            :class:`np.ndarray` or list:\n                a numpy array containing human joints for each (detected) person.\n\n                Format:\n                    if image is a single image:\n                        shape=(# of people, # of joints (nof_joints), 3);  dtype=(np.float32).\n                    if image is a stack of n images:\n                        list of n np.ndarrays with\n                        shape=(# of people, # of joints (nof_joints), 3);  dtype=(np.float32).\n\n                Each joint has 3 values: (y position, x position, joint confidence).\n\n                If self.return_heatmaps, the class returns a list with (heatmaps, human joints)\n                If self.return_bounding_boxes, the class returns a list with (bounding boxes, human joints)\n                If self.return_heatmaps and self.return_bounding_boxes, the class returns a list with\n                    (heatmaps, bounding boxes, human joints)\n        \"\"\"\n        if len(image.shape) == 3:\n            return self._predict_single(image)\n        else:\n            raise ValueError('Wrong image format.')\n\n    def sa_simdr_pts(self,img,detection,images,boxes):\n        c, s = [], []\n        if detection is not None:\n            for i, (x1, y1, x2, y2) in enumerate(detection):\n                x1 = int(round(x1.item()))\n                x2 = int(round(x2.item()))\n                y1 = int(round(y1.item()))\n                y2 = int(round(y2.item()))\n                boxes[i] = [x1, y1, x2, y2]\n                w, h = x2 - x1, y2 - y1\n                xx1 = np.max((0, x1))\n                yy1 = np.max((0, y1))\n                xx2 = np.min((img.shape[1] - 1, x1 + np.max((0, w - 1))))\n                yy2 = np.min((img.shape[0] - 1, y1 + np.max((0, h - 1))))\n                box = [xx1, yy1, xx2 - xx1, yy2 - yy1]\n                center, scale = self._box2cs(box)\n                c.append(center)\n                s.append(scale)\n\n                trans = get_affine_transform(center, scale, 0, np.array(cfg.MODEL.IMAGE_SIZE))\n                input = cv2.warpAffine(\n                    img,\n                    trans,\n                    (int(self.resolution[1]), int(self.resolution[0])),\n                    flags=cv2.INTER_LINEAR)\n                images[i] = self.transform(input)\n            if images.shape[0] &gt; 0:\n                images = images.to(self.device)\n                with torch.no_grad():\n                    output_x, output_y = self.model(images)\n\n                    if cfg.TEST.FLIP_TEST:\n                        input_flipped = images.flip(3)\n                        output_x_flipped_, output_y_flipped_ = self.model(input_flipped)\n                        output_x_flipped = flip_back_simdr(output_x_flipped_.cpu().numpy(),\n                                                           self.flip_pairs, type='x')\n                        output_y_flipped = flip_back_simdr(output_y_flipped_.cpu().numpy(),\n                                                           self.flip_pairs, type='y')\n                        output_x_flipped = torch.from_numpy(output_x_flipped.copy()).to(self.device)\n                        output_y_flipped = torch.from_numpy(output_y_flipped.copy()).to(self.device)\n\n                        # feature is not aligned, shift flipped heatmap for higher accuracy\n                        if cfg.TEST.SHIFT_HEATMAP:\n                            output_x_flipped[:, :, 0:-1] = \\\n                                output_x_flipped.clone()[:, :, 1:]\n                        output_x = F.softmax((output_x + output_x_flipped) * 0.5, dim=2)\n                        output_y = F.softmax((output_y + output_y_flipped) * 0.5, dim=2)\n                    else:\n                        output_x = F.softmax(output_x, dim=2)\n                        output_y = F.softmax(output_y, dim=2)\n                    max_val_x, preds_x = output_x.max(2, keepdim=True)\n                    max_val_y, preds_y = output_y.max(2, keepdim=True)\n\n                    mask = max_val_x &gt; max_val_y\n                    max_val_x[mask] = max_val_y[mask]\n                    maxvals = max_val_x * 10.0\n\n                    output = torch.ones([images.size(0), preds_x.size(1), 3])\n                    output[:, :, 0] = torch.squeeze(torch.true_divide(preds_x, cfg.MODEL.SIMDR_SPLIT_RATIO))\n                    output[:, :, 1] = torch.squeeze(torch.true_divide(preds_y, cfg.MODEL.SIMDR_SPLIT_RATIO))\n                    # output[:, :, 2] = maxvals.squeeze(2)\n                    output = output.cpu().numpy()\n                    preds = output.copy()\n                    for i in range(output.shape[0]):\n                        preds[i] = transform_preds(\n                            output[i], c[i], s[i], [cfg.MODEL.IMAGE_SIZE[0], cfg.MODEL.IMAGE_SIZE[1]]\n                        )\n                    preds[:, :, 2] = maxvals.squeeze(2)\n            else:\n                preds = np.empty((0, 0, 3), dtype=np.float32)\n        return preds\n    def simdr_pts(self,img,detection,images,boxes):\n        c, s = [], []\n        if detection is not None:\n            for i, (x1, y1, x2, y2) in enumerate(detection):\n                x1 = int(round(x1.item()))\n                x2 = int(round(x2.item()))\n                y1 = int(round(y1.item()))\n                y2 = int(round(y2.item()))\n                boxes[i] = [x1, y1, x2, y2]\n                w, h = x2 - x1, y2 - y1\n                xx1 = np.max((0, x1))\n                yy1 = np.max((0, y1))\n                xx2 = np.min((img.shape[1] - 1, x1 + np.max((0, w - 1))))\n                yy2 = np.min((img.shape[0] - 1, y1 + np.max((0, h - 1))))\n                box = [xx1, yy1, xx2 - xx1, yy2 - yy1]\n                center, scale = self._box2cs(box)\n                c.append(center)\n                s.append(scale)\n\n                trans = get_affine_transform(center, scale, 0, np.array(cfg.MODEL.IMAGE_SIZE))\n                input = cv2.warpAffine(\n                    img,\n                    trans,\n                    (int(self.resolution[1]), int(self.resolution[0])),\n                    flags=cv2.INTER_LINEAR)\n                images[i] = self.transform(input)\n            if images.shape[0] &gt; 0:\n                images = images.to(self.device)\n                with torch.no_grad():\n                    output_x, output_y = self.model(images)\n                    if cfg.TEST.FLIP_TEST:\n                        input_flipped = images.flip(3)\n                        output_x_flipped_, output_y_flipped_ = self.model(input_flipped)\n                        output_x_flipped = flip_back_simdr(output_x_flipped_.cpu().numpy(),\n                                                           self.flip_pairs, type='x')\n                        output_y_flipped = flip_back_simdr(output_y_flipped_.cpu().numpy(),\n                                                           self.flip_pairs, type='y')\n                        output_x_flipped = torch.from_numpy(output_x_flipped.copy()).to(self.device)\n                        output_y_flipped = torch.from_numpy(output_y_flipped.copy()).to(self.device)\n\n                        # feature is not aligned, shift flipped heatmap for higher accuracy\n                        if cfg.TEST.SHIFT_HEATMAP:\n                            output_x_flipped[:, :, 0:-1] = \\\n                                output_x_flipped.clone()[:, :, 1:]\n                        output_x = (F.softmax(output_x, dim=2) + F.softmax(output_x_flipped, dim=2)) * 0.5\n                        output_y = (F.softmax(output_y, dim=2) + F.softmax(output_y_flipped, dim=2)) * 0.5\n                    else:\n                        output_x = F.softmax(output_x, dim=2)\n                        output_y = F.softmax(output_y, dim=2)\n                    max_val_x, preds_x = output_x.max(2, keepdim=True)\n                    max_val_y, preds_y = output_y.max(2, keepdim=True)\n\n                    mask = max_val_x &gt; max_val_y\n                    max_val_x[mask] = max_val_y[mask]\n                    maxvals = max_val_x * 10.0\n\n                    output = torch.ones([images.size(0), preds_x.size(1), 3])\n                    output[:, :, 0] = torch.squeeze(torch.true_divide(preds_x, cfg.MODEL.SIMDR_SPLIT_RATIO))\n                    output[:, :, 1] = torch.squeeze(torch.true_divide(preds_y, cfg.MODEL.SIMDR_SPLIT_RATIO))\n\n                    output = output.cpu().numpy()\n                    preds = output.copy()\n                    for i in range(output.shape[0]):\n                        preds[i] = transform_preds(\n                            output[i], c[i], s[i], [cfg.MODEL.IMAGE_SIZE[0], cfg.MODEL.IMAGE_SIZE[1]]\n                        )\n                    preds[:, :, 2] = maxvals.squeeze(2)\n            else:\n                preds = np.empty((0, 0, 3), dtype=np.float32)\n        return preds\n    def hrnet_pts(self,img,detection,images,boxes):\n        if detection is not None:\n            for i, (x1, y1, x2, y2) in enumerate(detection):\n                x1 = int(round(x1.item()))\n                x2 = int(round(x2.item()))\n                y1 = int(round(y1.item()))\n                y2 = int(round(y2.item()))\n\n                # Adapt detections to match HRNet input aspect ratio (as suggested by xtyDoge in issue #14)\n                correction_factor = self.resolution[0] / self.resolution[1] * (x2 - x1) / (y2 - y1)\n                if correction_factor &gt; 1:\n                    # increase y side\n                    center = y1 + (y2 - y1) // 2\n                    length = int(round((y2 - y1) * correction_factor))\n                    y1 = max(0, center - length // 2)\n                    y2 = min(img.shape[0], center + length // 2)\n                elif correction_factor &lt; 1:\n                    # increase x side\n                    center = x1 + (x2 - x1) // 2\n                    length = int(round((x2 - x1) * 1 / correction_factor))\n                    x1 = max(0, center - length // 2)\n                    x2 = min(img.shape[1], center + length // 2)\n\n                boxes[i] = [x1, y1, x2, y2]\n                images[i] = self.transform(img[y1:y2, x1:x2, ::-1])\n\n        if images.shape[0] &gt; 0:\n            images = images.to(self.device)\n\n            with torch.no_grad():\n                out = self.model(images)\n\n                out = out.detach().cpu().numpy()\n                pts = np.empty((out.shape[0], out.shape[1], 3), dtype=np.float32)\n                # For each human, for each joint: y, x, confidence\n                for i, human in enumerate(out):\n                    for j, joint in enumerate(human):\n                        pt = np.unravel_index(np.argmax(joint), (self.resolution[0] // 4, self.resolution[1] // 4))\n                        # 0: pt_x / (height // 4) * (bb_y2 - bb_y1) + bb_y1\n                        # 1: pt_y / (width // 4) * (bb_x2 - bb_x1) + bb_x1\n                        # 2: confidences\n                        pts[i, j, 0] = pt[1] * 1. / (self.resolution[1] // 4) * (boxes[i][2] - boxes[i][0]) + boxes[i][0]\n                        pts[i, j, 1] = pt[0] * 1. / (self.resolution[0] // 4) * (boxes[i][3] - boxes[i][1]) + boxes[i][1]\n                        pts[i, j, 2] = joint[pt]\n\n        else:\n            pts = np.empty((0, 0, 3), dtype=np.float32)\n\n        return pts\n\n    def _predict_single(self, image):\n\n        _,detections = self.detector.detect(image)\n\n        nof_people = len(detections) if detections is not None else 0\n        boxes = np.empty((nof_people, 4), dtype=np.int32)\n        images = torch.empty((nof_people, 3, self.resolution[0], self.resolution[1]))  # (height, width)\n        if self.model_name in ('sa-simdr','sasimdr'):\n            pts=self.sa_simdr_pts(image,detections,images,boxes)\n        elif self.model_name in ('hrnet','HRnet','hrnet'):\n            pts = self.hrnet_pts(image, detections, images, boxes)\n        elif self.model_name in ('simdr'):\n            pts = self.simdr_pts(image, detections, images, boxes)\n\n        return pts\n        # c,s=[],[]\n        # if detections is not None:\n        #     for i, (x1, y1, x2, y2) in enumerate(detections):\n        #         x1 = int(round(x1.item()))\n        #         x2 = int(round(x2.item()))\n        #         y1 = int(round(y1.item()))\n        #         y2 = int(round(y2.item()))\n        #         boxes[i] = [x1,y1,x2,y2]\n        #         w ,h= x2-x1,y2-y1\n        #         xx1 = np.max((0, x1))\n        #         yy1 = np.max((0, y1))\n        #         xx2 = np.min((image.shape[1] - 1, x1 + np.max((0, w - 1))))\n        #         yy2 = np.min((image.shape[0] - 1, y1 + np.max((0, h - 1))))\n        #         box = [xx1, yy1, xx2-xx1, yy2-yy1]\n        #         center,scale = self._box2cs(box)\n        #         c.append(center)\n        #         s.append(scale)\n        #\n        #         trans = get_affine_transform(center, scale, 0, np.array(cfg.MODEL.IMAGE_SIZE))\n        #         input = cv2.warpAffine(\n        #             image,\n        #             trans,\n        #             (int(self.resolution[1]), int(self.resolution[0])),\n        #             flags=cv2.INTER_LINEAR)\n        #         images[i] = self.transform(input)\n\n\n        # if images.shape[0] &gt; 0:\n        #     images = images.to(self.device)\n        #     with torch.no_grad():\n        #         output_x,output_y = self.model(images)\n        #         if cfg.TEST.FLIP_TEST:\n        #             input_flipped = images.flip(3)\n        #             output_x_flipped_, output_y_flipped_ = self.model(input_flipped)\n        #             output_x_flipped = flip_back_simdr(output_x_flipped_.cpu().numpy(),\n        #                                                self.flip_pairs, type='x')\n        #             output_y_flipped = flip_back_simdr(output_y_flipped_.cpu().numpy(),\n        #                                                self.flip_pairs, type='y')\n        #             output_x_flipped = torch.from_numpy(output_x_flipped.copy()).to(self.device)\n        #             output_y_flipped = torch.from_numpy(output_y_flipped.copy()).to(self.device)\n        #\n        #             # feature is not aligned, shift flipped heatmap for higher accuracy\n        #             if cfg.TEST.SHIFT_HEATMAP:\n        #                 output_x_flipped[:, :, 0:-1] = \\\n        #                     output_x_flipped.clone()[:, :, 1:]\n        #             output_x = F.softmax((output_x + output_x_flipped) * 0.5, dim=2)\n        #             output_y = F.softmax((output_y + output_y_flipped) * 0.5, dim=2)\n        #         else:\n        #             output_x = F.softmax(output_x, dim=2)\n        #             output_y = F.softmax(output_y, dim=2)\n        #         max_val_x, preds_x = output_x.max(2, keepdim=True)\n        #         max_val_y, preds_y = output_y.max(2, keepdim=True)\n        #\n        #         mask = max_val_x &gt; max_val_y\n        #         max_val_x[mask] = max_val_y[mask]\n        #         maxvals = max_val_x.cpu().numpy()\n        #\n        #         output = torch.ones([images.size(0), preds_x.size(1), 2])\n        #         output[:, :, 0] = torch.squeeze(torch.true_divide(preds_x, cfg.MODEL.SIMDR_SPLIT_RATIO))\n        #         output[:, :, 1] = torch.squeeze(torch.true_divide(preds_y, cfg.MODEL.SIMDR_SPLIT_RATIO))\n        #\n        #         output = output.cpu().numpy()\n        #         preds = output.copy()\n        #         for i in range(output.shape[0]):\n        #             preds[i] = transform_preds(\n        #                 output[i], c[i], s[i], [cfg.MODEL.IMAGE_SIZE[0], cfg.MODEL.IMAGE_SIZE[1]]\n        #             )\n        # else:\n        #     preds = np.empty((0, 0, 2), dtype=np.float32)\n        # return preds\n\n# parser = argparse.ArgumentParser()\n# parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n# parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n# parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n# parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n# parser.add_argument('--augment', action='store_true', help='augmented inference')\n# parser.add_argument('--update', action='store_true', help='update all model')\n# parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n# parser.add_argument('--name', default='exp', help='save results to project/name')\n# parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n# opt = parser.parse_args()\n# model = Points(model_name='hrnet',opt=opt)\n# img0 = cv2.imread('./data/test1.jpg')\n# pts = model.predict(img0)\n# print(pts.shape)\n# for point in pts[0]:\n#     image = cv2.circle(img0, (int(point[0]), int(point[1])), 3, [255,0,255], -1 , lineType= cv2.LINE_AA)\n#     cv2.imwrite('./data/test11_result.jpg',image)</code></pre>\n<h3>④绘制骨骼关键点 </h3>\n<p>        根据以上步骤，我们已经得到了关键点的坐标值，接下来需要在图片中描绘出来，以便展示检测结果。</p>\n<p>        首先在’ ./SimDR/lib/utils/ ‘文件夹下新建visualization.py文件，将以下内容复制到文件中。骨架绘制代码结合了simple-hrnet与Openpose工程。</p>\n<p><span style=\"color:#38d8f0;\">【2022.04.16更新：由于之前的绘制代码被我魔改过，现在恢复成所有点与骨骼都绘制的模样，但是总觉得好丑，没有openpose那种美观，如果有人绘制出比较美观的骨架，希望能分享一下哈，共同进步！】</span></p>\n<pre><code class=\"language-python\">import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\nimport ffmpeg\nimport random\nimport math\nimport copy\ndef plot_one_box(x, img, color=None, label=None, line_thickness=3):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n    return img\n\ndef joints_dict():\n    joints = {\n        \"coco\": {\n            \"keypoints\": {\n                0: \"nose\",\n                1: \"left_eye\",\n                2: \"right_eye\",\n                3: \"left_ear\",\n                4: \"right_ear\",\n                5: \"left_shoulder\",\n                6: \"right_shoulder\",\n                7: \"left_elbow\",\n                8: \"right_elbow\",\n                9: \"left_wrist\",\n                10: \"right_wrist\",\n                11: \"left_hip\",\n                12: \"right_hip\",\n                13: \"left_knee\",\n                14: \"right_knee\",\n                15: \"left_ankle\",\n                16: \"right_ankle\"\n            },\n            \"skeleton\": [\n                # # [16, 14], [14, 12], [17, 15], [15, 13], [12, 13], [6, 12], [7, 13], [6, 7], [6, 8],\n                # # [7, 9], [8, 10], [9, 11], [2, 3], [1, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5, 7]\n                # [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7],\n                # [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2], [1, 3], [2, 4], [3, 5], [4, 6]\n                [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7],\n                [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2], [1, 3], [2, 4],  # [3, 5], [4, 6]\n                [0, 5], [0, 6]\n                # [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7],\n                # [6, 8], [7, 9], [8, 10],  [0, 3], [0, 4], [1, 3], [2, 4],  # [3, 5], [4, 6]\n                # [0, 5], [0, 6]\n            ]\n        },\n        \"mpii\": {\n            \"keypoints\": {\n                0: \"right_ankle\",\n                1: \"right_knee\",\n                2: \"right_hip\",\n                3: \"left_hip\",\n                4: \"left_knee\",\n                5: \"left_ankle\",\n                6: \"pelvis\",\n                7: \"thorax\",\n                8: \"upper_neck\",\n                9: \"head top\",\n                10: \"right_wrist\",\n                11: \"right_elbow\",\n                12: \"right_shoulder\",\n                13: \"left_shoulder\",\n                14: \"left_elbow\",\n                15: \"left_wrist\"\n            },\n            \"skeleton\": [\n                # [5, 4], [4, 3], [0, 1], [1, 2], [3, 2], [13, 3], [12, 2], [13, 12], [13, 14],\n                # [12, 11], [14, 15], [11, 10], # [2, 3], [1, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5, 7]\n                [5, 4], [4, 3], [0, 1], [1, 2], [3, 2], [3, 6], [2, 6], [6, 7], [7, 8], [8, 9],\n                [13, 7], [12, 7], [13, 14], [12, 11], [14, 15], [11, 10],\n            ]\n        },\n    }\n    return joints\n\n\ndef draw_points(image, points, color_palette='tab20', palette_samples=16, confidence_threshold=0.1,color=None):\n    \"\"\"\n    Draws `points` on `image`.\n\n    Args:\n        image: image in opencv format\n        points: list of points to be drawn.\n            Shape: (nof_points, 3)\n            Format: each point should contain (y, x, confidence)\n        color_palette: name of a matplotlib color palette\n            Default: 'tab20'\n        palette_samples: number of different colors sampled from the `color_palette`\n            Default: 16\n        confidence_threshold: only points with a confidence higher than this threshold will be drawn. Range: [0, 1]\n            Default: 0.1\n\n    Returns:\n        A new image with overlaid points\n\n    \"\"\"\n\n    circle_size = max(2, int(np.sqrt(np.max(np.max(points, axis=0) - np.min(points, axis=0)) // 16)))\n    for i, pt in enumerate(points):\n\n        if pt[2] &gt;= confidence_threshold:\n            image = cv2.circle(image, (int(pt[0]), int(pt[1])), circle_size, color[i] ,-1, lineType= cv2.LINE_AA)\n\n    return image\n\n\ndef draw_skeleton(image, points, skeleton, color_palette='Set2', palette_samples=8, person_index=0,\n                  confidence_threshold=0.1,sk_color=None):\n    \"\"\"\n    Draws a `skeleton` on `image`.\n\n    Args:\n        image: image in opencv format\n        points: list of points to be drawn.\n            Shape: (nof_points, 3)\n            Format: each point should contain (y, x, confidence)\n        skeleton: list of joints to be drawn\n            Shape: (nof_joints, 2)\n            Format: each joint should contain (point_a, point_b) where `point_a` and `point_b` are an index in `points`\n        color_palette: name of a matplotlib color palette\n            Default: 'Set2'\n        palette_samples: number of different colors sampled from the `color_palette`\n            Default: 8\n        person_index: index of the person in `image`\n            Default: 0\n        confidence_threshold: only points with a confidence higher than this threshold will be drawn. Range: [0, 1]\n            Default: 0.1\n\n    Returns:\n        A new image with overlaid joints\n\n    \"\"\"\n    canvas = copy.deepcopy(image)\n    cur_canvas = canvas.copy()\n    for i, joint in enumerate(skeleton):\n\n        pt1, pt2 = points[joint]\n\n        if pt1[2] &gt;= confidence_threshold and pt2[2]&gt;= confidence_threshold :\n            length = ((pt1[0] - pt2[0]) ** 2 + (pt1[1] - pt2[1]) ** 2) ** 0.5\n            angle = math.degrees(math.atan2(pt1[1] - pt2[1],pt1[0] - pt2[0]))\n            polygon = cv2.ellipse2Poly((int(np.mean((pt1[0],pt2[0]))), int(np.mean((pt1[1],pt2[1])))), (int(length / 2), 2), int(angle), 0, 360, 1)\n            cv2.fillConvexPoly(cur_canvas, polygon, sk_color[i],lineType=cv2.LINE_AA)\n            # cv2.fillConvexPoly(cur_canvas, polygon, sk_color,lineType=cv2.LINE_AA)\n            canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n\n    return canvas\n\n\ndef draw_points_and_skeleton(image, points, skeleton, points_color_palette='tab20', points_palette_samples=16,\n                             skeleton_color_palette='Set2', skeleton_palette_samples=8, person_index=0,\n                             confidence_threshold=0.1,color=None,sk_color=None):\n    \"\"\"\n    Draws `points` and `skeleton` on `image`.\n\n    Args:\n        image: image in opencv format\n        points: list of points to be drawn.\n            Shape: (nof_points, 3)\n            Format: each point should contain (y, x, confidence)\n        skeleton: list of joints to be drawn\n            Shape: (nof_joints, 2)\n            Format: each joint should contain (point_a, point_b) where `point_a` and `point_b` are an index in `points`\n        points_color_palette: name of a matplotlib color palette\n            Default: 'tab20'\n        points_palette_samples: number of different colors sampled from the `color_palette`\n            Default: 16\n        skeleton_color_palette: name of a matplotlib color palette\n            Default: 'Set2'\n        skeleton_palette_samples: number of different colors sampled from the `color_palette`\n            Default: 8\n        person_index: index of the person in `image`\n            Default: 0\n        confidence_threshold: only points with a confidence higher than this threshold will be drawn. Range: [0, 1]\n            Default: 0.1\n\n    Returns:\n        A new image with overlaid joints\n\n    \"\"\"\n    colors1 = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\n               [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\n               [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85], [255, 0, 85]]\n    image = draw_skeleton(image, points, skeleton, color_palette=skeleton_color_palette,\n                          palette_samples=skeleton_palette_samples, person_index=person_index,\n                          confidence_threshold=confidence_threshold,sk_color=colors1)\n    image = draw_points(image, points, color_palette=points_color_palette, palette_samples=points_palette_samples,\n                        confidence_threshold=confidence_threshold,color=colors1)\n    return image\n\n\ndef save_images(images, target, joint_target, output, joint_output, joint_visibility, summary_writer=None, step=0,\n                prefix=''):\n    \"\"\"\n    Creates a grid of images with gt joints and a grid with predicted joints.\n    This is a basic function for debugging purposes only.\n\n    If summary_writer is not None, the grid will be written in that SummaryWriter with name \"{prefix}_images\" and\n    \"{prefix}_predictions\".\n\n    Args:\n        images (torch.Tensor): a tensor of images with shape (batch x channels x height x width).\n        target (torch.Tensor): a tensor of gt heatmaps with shape (batch x channels x height x width).\n        joint_target (torch.Tensor): a tensor of gt joints with shape (batch x joints x 2).\n        output (torch.Tensor): a tensor of predicted heatmaps with shape (batch x channels x height x width).\n        joint_output (torch.Tensor): a tensor of predicted joints with shape (batch x joints x 2).\n        joint_visibility (torch.Tensor): a tensor of joint visibility with shape (batch x joints).\n        summary_writer (tb.SummaryWriter): a SummaryWriter where write the grids.\n            Default: None\n        step (int): summary_writer step.\n            Default: 0\n        prefix (str): summary_writer name prefix.\n            Default: \"\"\n\n    Returns:\n        A pair of images which are built from torchvision.utils.make_grid\n    \"\"\"\n    # Input images with gt\n    images_ok = images.detach().clone()\n    images_ok[:, 0].mul_(0.229).add_(0.485)\n    images_ok[:, 1].mul_(0.224).add_(0.456)\n    images_ok[:, 2].mul_(0.225).add_(0.406)\n    for i in range(images.shape[0]):\n        joints = joint_target[i] * 4.\n        joints_vis = joint_visibility[i]\n\n        for joint, joint_vis in zip(joints, joints_vis):\n            if joint_vis[0]:\n                a = int(joint[1].item())\n                b = int(joint[0].item())\n                # images_ok[i][:, a-1:a+1, b-1:b+1] = torch.tensor([1, 0, 0])\n                images_ok[i][0, a - 1:a + 1, b - 1:b + 1] = 1\n                images_ok[i][1:, a - 1:a + 1, b - 1:b + 1] = 0\n    grid_gt = torchvision.utils.make_grid(images_ok, nrow=int(images_ok.shape[0] ** 0.5), padding=2, normalize=False)\n    if summary_writer is not None:\n        summary_writer.add_image(prefix + 'images', grid_gt, global_step=step)\n\n    # Input images with prediction\n    images_ok = images.detach().clone()\n    images_ok[:, 0].mul_(0.229).add_(0.485)\n    images_ok[:, 1].mul_(0.224).add_(0.456)\n    images_ok[:, 2].mul_(0.225).add_(0.406)\n    for i in range(images.shape[0]):\n        joints = joint_output[i] * 4.\n        joints_vis = joint_visibility[i]\n\n        for joint, joint_vis in zip(joints, joints_vis):\n            if joint_vis[0]:\n                a = int(joint[1].item())\n                b = int(joint[0].item())\n                # images_ok[i][:, a-1:a+1, b-1:b+1] = torch.tensor([1, 0, 0])\n                images_ok[i][0, a - 1:a + 1, b - 1:b + 1] = 1\n                images_ok[i][1:, a - 1:a + 1, b - 1:b + 1] = 0\n    grid_pred = torchvision.utils.make_grid(images_ok, nrow=int(images_ok.shape[0] ** 0.5), padding=2, normalize=False)\n    if summary_writer is not None:\n        summary_writer.add_image(prefix + 'predictions', grid_pred, global_step=step)\n\n    # Heatmaps\n    # ToDo\n    # for h in range(0,17):\n    #     heatmap = torchvision.utils.make_grid(output[h].detach(), nrow=int(np.sqrt(output.shape[0])),\n    #                                            padding=2, normalize=True, range=(0, 1))\n    #     summary_writer.add_image('train_heatmap_%d' % h, heatmap, global_step=step + epoch*len_dl_train)\n\n    return grid_gt, grid_pred\n\n\ndef check_video_rotation(filename):\n    # thanks to\n    # https://stackoverflow.com/questions/53097092/frame-from-video-is-upside-down-after-extracting/55747773#55747773\n\n    # this returns meta-data of the video file in form of a dictionary\n    meta_dict = ffmpeg.probe(filename)\n\n    # from the dictionary, meta_dict['streams'][0]['tags']['rotate'] is the key\n    # we are looking for\n    rotation_code = None\n    try:\n        if int(meta_dict['streams'][0]['tags']['rotate']) == 90:\n            rotation_code = cv2.ROTATE_90_CLOCKWISE\n        elif int(meta_dict['streams'][0]['tags']['rotate']) == 180:\n            rotation_code = cv2.ROTATE_180\n        elif int(meta_dict['streams'][0]['tags']['rotate']) == 270:\n            rotation_code = cv2.ROTATE_90_COUNTERCLOCKWISE\n        else:\n            raise ValueError\n    except KeyError:\n        pass\n\n    return rotation_code\n</code></pre>\n<h2>4、测试算法</h2>\n<h3>①主程序</h3>\n<p>        在SimDR文件夹下新建main.py ,复制以下代码到文件中，修改parser参数source的默认值，运行代码。</p>\n<pre><code class=\"language-python\">import argparse\nimport time\nimport os\nimport cv2 as cv\nimport numpy as np\nfrom pathlib import Path\nfrom Point_detect import Points\nfrom lib.utils.visualization import draw_points_and_skeleton,joints_dict\n\ndef image_detect(opt):\n    skeleton = joints_dict()['coco']['skeleton']\n    hrnet_model = Points(model_name='hrnet', opt=opt,resolution=(384,288))  #resolution = (384,288)  or (256,192)\n    # simdr_model = Points(model_name='simdr', opt=opt,resolution=(256,192))  #resolution = (256,192)\n    # sa_simdr_model = Points(model_name='sa-simdr', opt=opt,resolution=(384,288))  #resolution = (384,288)  or (256,192)\n\n    img0 = cv.imread(opt.source)\n    frame = img0.copy()\n   #predict\n    pred = hrnet_model.predict(img0)\n    # pred = simdr_model.predict(frame)\n    # pred = sa_simdr_model.predict(frame)\n   #vis\n    for i, pt in enumerate(pred):\n        frame = draw_points_and_skeleton(frame, pt, skeleton)\n\n    #save\n    cv.imwrite('test_result.jpg', frame)\n\n\ndef video_detect(opt):\n    hrnet_model = Points(model_name='hrnet', opt=opt, resolution=(384, 288))  # resolution = (384,288)  or (256,192)\n    # simdr_model = Points(model_name='simdr', opt=opt,resolution=(256,192))  #resolution = (256,192)\n    # sa_simdr_model = Points(model_name='sa-simdr', opt=opt,resolution=(384,288))  #resolution = (384,288)  or (256,192)\n    skeleton = joints_dict()['coco']['skeleton']\n\n    cap = cv.VideoCapture(opt.source)\n    if opt.save_video:\n        fourcc = cv.VideoWriter_fourcc(*'MJPG')\n        out = cv.VideoWriter('data/runs/{}_out.avi'.format(os.path.basename(opt.source).split('.')[0]), fourcc, 24, (int(cap.get(3)), int(cap.get(4))))\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        pred = hrnet_model.predict(frame)\n        # pred = simdr_model.predict(frame)\n        # pred = sa_simdr_model.predict(frame)\n        for pt in pred:\n            frame = draw_points_and_skeleton(frame,pt,skeleton)\n        if opt.show:\n            cv.imshow('result', frame)\n        if opt.save_video:\n            out.write(frame)\n        if cv.waitKey(1) == 27:\n            break\n    out.release()\n    cap.release()\n    cv.destroyAllWindows()\n# video_detect(0)\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--source', type=str, default='./data/images/test1.jpg', help='source')  # file/folder, 0 for webcam\n    parser.add_argument('--detect_weight', type=str, default=\"./yolov5/weights/yolov5x.pt\", help='e.g \"./yolov5/weights/yolov5x.pt\"')\n    parser.add_argument('--save_video', action='store_true', default=False,help='save results to *.avi')\n    parser.add_argument('--show', action='store_true', default=True, help='save results to *.avi')\n    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n    parser.add_argument('--augment', action='store_true', help='augmented inference')\n\n    opt = parser.parse_args()\n    image_detect(opt)\n</code></pre>\n<h3>②结果展示</h3>\n<p style=\"text-align:center;\"><img alt=\"\" height=\"405\" src=\"..\\..\\static\\image\\c6f743e45c924bc1927427cf65af47a6.png\" width=\"608\"/></p>\n<p style=\"text-align:center;\"><img alt=\"\" height=\"406\" src=\"..\\..\\static\\image\\e75e98b4a2a24ab0b4769d9c4d3c0af8.png\" width=\"612\"/></p>\n<h1>四、总结</h1>\n<p>        全文较长，主要都是些代码，整个工程从跑数据集到实际检测需要对代码工程有一定的理解，整个项目不难，主要考验类的构造。如果需要整个工程可以私聊我。由于我也是刚入门的萌新，所以代码格式写法或者理论看法有很多错误，欢迎指正，共同进步，如果有帮助欢迎点赞评论，万分感谢。</p>\n<h1>五、参考内容 </h1>\n<p>1、<a href=\"https://github.com/leeyegy/SimDR\" title=\"GitHub - leeyegy/SimDR: PyTorch implementation for: Is 2D Heatmap Representation Even Necessary for Human Pose Estimation? (http://arxiv.org/abs/2107.03332)\">GitHub - leeyegy/SimDR: PyTorch implementation for: Is 2D Heatmap Representation Even Necessary for Human Pose Estimation? (http://arxiv.org/abs/2107.03332)</a></p>\n<p>2、<a href=\"https://github.com/ultralytics/yolov5\" title=\"https://github.com/ultralytics/yolov5\">https://github.com/ultralytics/yolov5</a></p>\n<p>3、<a href=\"https://github.com/GreenTeaHua/simple-HRNet\" title=\"GitHub - GreenTeaHua/simple-HRNet: Multi-person Human Pose Estimation with HRNet in Pytorch\">GitHub - GreenTeaHua/simple-HRNet: Multi-person Human Pose Estimation with HRNet in Pytorch</a></p>\n<p></p>\n<p></p>\n<p></p>\n</div>\n</div>", "first_tag": "Python", "cpp": 0, "csharp": 0, "python": 1, "javascript": 0, "java": 0, "sql": 0, "php": 0, "time": "2022-04-16 17:11:48", "summary": "一、前言由于工程项目中需要对视频中的进行关键点检测，我测试各个算法后，并没有采用比较应用化成熟的，决定采用检测精度更高的系列。但是由于官方给的算法只能测试数据集，需要自己根据算法模型编写实例化代码。本"}