{"blogid": "126600912", "writerAge": "码龄6年", "writerBlogNum": "835", "writerCollect": "11242", "writerComment": "9962", "writerFan": "46054", "writerGrade": "8级", "writerIntegral": "34233", "writerName": "Lansonli", "writerProfileAdress": "writer_image\\profile_126600912.jpg", "writerRankTotal": "141", "writerRankWeekly": "24", "writerThumb": "9288", "writerVisitNum": "1321836", "blog_read_count": "799", "blog_time": "已于 2022-09-02 16:17:01 修改", "blog_title": "湖仓一体电商项目（八）：业务实现之编写写入ODS层业务代码", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<p style=\"text-align:center;\"><img alt=\"\" src=\"image\\ad88a6320b7447179630b9db8c76f722.jpeg\"/></p>\n<p id=\"main-toc\"><strong>文章目录</strong></p>\n<p id=\"%E4%B8%9A%E5%8A%A1%E5%AE%9E%E7%8E%B0%E4%B9%8B%E7%BC%96%E5%86%99%E5%86%99%E5%85%A5ODS%E5%B1%82%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81-toc\" style=\"margin-left:0px;\"><a href=\"#%E4%B8%9A%E5%8A%A1%E5%AE%9E%E7%8E%B0%E4%B9%8B%E7%BC%96%E5%86%99%E5%86%99%E5%85%A5ODS%E5%B1%82%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81\">业务实现之编写写入ODS层业务代码</a></p>\n<p id=\"%E4%B8%80%E3%80%81%E4%BB%A3%E7%A0%81%E9%80%BB%E8%BE%91%E5%92%8C%E6%9E%B6%E6%9E%84%E5%9B%BE-toc\" style=\"margin-left:40px;\"><a href=\"#%E4%B8%80%E3%80%81%E4%BB%A3%E7%A0%81%E9%80%BB%E8%BE%91%E5%92%8C%E6%9E%B6%E6%9E%84%E5%9B%BE\">一、代码逻辑和架构图</a></p>\n<p id=\"%E4%B8%80%E3%80%81%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99-toc\" style=\"margin-left:40px;\"><a href=\"#%E4%B8%80%E3%80%81%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99\">二、​​​​​​​代码编写</a></p>\n<p id=\"%E4%BA%8C%E3%80%81%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E5%88%9B%E5%BB%BAIceberg-ODS%E5%B1%82%E8%A1%A8-toc\" style=\"margin-left:40px;\"><a href=\"#%E4%BA%8C%E3%80%81%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E5%88%9B%E5%BB%BAIceberg-ODS%E5%B1%82%E8%A1%A8\">三、​​​​​​​​​​​​​​创建Iceberg-ODS层表</a></p>\n<p id=\"1%E3%80%81%E5%9C%A8Hive%E4%B8%AD%E6%B7%BB%E5%8A%A0Iceberg%E8%A1%A8%E6%A0%BC%E5%BC%8F%E9%9C%80%E8%A6%81%E7%9A%84%E5%8C%85-toc\" style=\"margin-left:80px;\"><a href=\"#1%E3%80%81%E5%9C%A8Hive%E4%B8%AD%E6%B7%BB%E5%8A%A0Iceberg%E8%A1%A8%E6%A0%BC%E5%BC%8F%E9%9C%80%E8%A6%81%E7%9A%84%E5%8C%85\">1、在Hive中添加Iceberg表格式需要的包</a></p>\n<p id=\"2%E3%80%81%E5%88%9B%E5%BB%BAIceberg%E8%A1%A8-toc\" style=\"margin-left:80px;\"><a href=\"#2%E3%80%81%E5%88%9B%E5%BB%BAIceberg%E8%A1%A8\">2、创建Iceberg表</a></p>\n<p id=\"%E4%B8%89%E3%80%81%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95-toc\" style=\"margin-left:40px;\"><a href=\"#%E4%B8%89%E3%80%81%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95\">四、代码测试</a></p>\n<p id=\"1%E3%80%81%E5%9C%A8Kafka%E4%B8%AD%E5%88%9B%E5%BB%BA%E5%AF%B9%E5%BA%94%E7%9A%84topic-toc\" style=\"margin-left:80px;\"><a href=\"#1%E3%80%81%E5%9C%A8Kafka%E4%B8%AD%E5%88%9B%E5%BB%BA%E5%AF%B9%E5%BA%94%E7%9A%84topic\">1、在Kafka中创建对应的topic</a></p>\n<p id=\"2%E3%80%81%E5%B0%86%E4%BB%A3%E7%A0%81%E4%B8%AD%E6%B6%88%E8%B4%B9Kafka%E6%95%B0%E6%8D%AE%E6%94%B9%E6%88%90%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E6%B6%88%E8%B4%B9-toc\" style=\"margin-left:80px;\"><a href=\"#2%E3%80%81%E5%B0%86%E4%BB%A3%E7%A0%81%E4%B8%AD%E6%B6%88%E8%B4%B9Kafka%E6%95%B0%E6%8D%AE%E6%94%B9%E6%88%90%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E6%B6%88%E8%B4%B9\">2、将代码中消费Kafka数据改成从头开始消费</a></p>\n<p id=\"3%E3%80%81%E6%89%A7%E8%A1%8C%E4%BB%A3%E7%A0%81%EF%BC%8C%E6%9F%A5%E7%9C%8B%E5%AF%B9%E5%BA%94topic%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%9C-toc\" style=\"margin-left:80px;\"><a href=\"#3%E3%80%81%E6%89%A7%E8%A1%8C%E4%BB%A3%E7%A0%81%EF%BC%8C%E6%9F%A5%E7%9C%8B%E5%AF%B9%E5%BA%94topic%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%9C\">3、执行代码，查看对应topic中的结果</a></p>\n<hr id=\"hr-toc\"/>\n<h1>业务实现之编写写入ODS层业务代码</h1>\n<h2 id=\"%E4%B8%80%E3%80%81%E4%BB%A3%E7%A0%81%E9%80%BB%E8%BE%91%E5%92%8C%E6%9E%B6%E6%9E%84%E5%9B%BE\"><strong>一、代码逻辑和架构图</strong></h2>\n<p style=\"margin-left:.0001pt;text-align:justify;\">ODS层在湖仓一体架构中主要是存储原始数据，这里主要是读取Kafka “KAFKA-DB-BUSSINESS-DATA”topic中的数据实现如下两个方面功能:</p>\n<ul><li style=\"text-align:justify;\">将MySQL业务数据原封不动的存储在Iceberg-ODS层中方便项目临时业务需求使用。</li><li style=\"text-align:justify;\">将事实数据和维度数据进行分离，分别存储Kafka对应的topic中</li></ul>\n<p style=\"margin-left:.0001pt;text-align:justify;\">以上两个方面中第一个方面需要再Hive中预先创建对应的Iceberg表，才能写入，第二个方面不好分辨topic“KAFKA-DB-BUSSINESS-DATA”中哪些binlog数据是事实数据哪些binlog是维度数据，所以这里我们在mysql 配置表“lakehousedb.dim_tbl_config_info”中写入表信息，这样通过Flink获取此表维度表信息进行广播与Kafka实时流进行关联将事实数据和维度数据进行区分。</p>\n<p style=\"margin-left:.0001pt;text-align:justify;\"><img alt=\"\" height=\"626\" src=\"image\\35d9b4464d4f47a691de70c61a5c8577.png\" width=\"913\"/></p>\n<p><img alt=\"\" height=\"350\" src=\"image\\c99490dc555f406382d45b3cc039da4a.png\" width=\"1033\"/></p>\n<p></p>\n<h2 id=\"%E4%B8%80%E3%80%81%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99\" style=\"margin-left:.0001pt;text-align:justify;\"><strong>二、​​​​​​​​​​​​​​代码编写</strong></h2>\n<p style=\"margin-left:.0001pt;text-align:justify;\">数据写入ODS层代码是“ProduceKafkaDBDataToODS.scala”，主要代码逻辑实现如下：</p>\n<pre><code class=\"language-Scala\">object ProduceKafkaDBDataToODS {\n  private val mysqlUrl: String = ConfigUtil.MYSQL_URL\n  private val mysqlUser: String = ConfigUtil.MYSQL_USER\n  private val mysqlPassWord: String = ConfigUtil.MYSQL_PASSWORD\n  private val kafkaBrokers: String = ConfigUtil.KAFKA_BROKERS\n  private val kafkaDimTopic: String = ConfigUtil.KAFKA_DIM_TOPIC\n  private val kafkaOdsTopic: String = ConfigUtil.KAFKA_ODS_TOPIC\n  private val kafkaDwdUserLogTopic: String = ConfigUtil.KAFKA_DWD_USERLOG_TOPIC\n\n  def main(args: Array[String]): Unit = {\n    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n    val tblEnv: StreamTableEnvironment = StreamTableEnvironment.create(env)\n\n    import org.apache.flink.streaming.api.scala._\n\n    env.enableCheckpointing(5000)\n\n    /**\n      * 1.需要预先创建 Catalog\n      * 创建Catalog,创建表需要在Hive中提前创建好，不在代码中创建，因为在Flink中创建iceberg表不支持create table if not exists ...语法\n      */\n    tblEnv.executeSql(\n      \"\"\"\n        |create catalog hadoop_iceberg with (\n        | 'type'='iceberg',\n        | 'catalog-type'='hadoop',\n        | 'warehouse'='hdfs://mycluster/lakehousedata'\n        |)\n      \"\"\".stripMargin)\n\n    /**\n      * 2.创建 Kafka Connector,连接消费Kafka中数据\n      * 注意：1).关键字要使用 \" 飘\"符号引起来 2).对于json对象使用 map &lt; String,String&gt;来接收\n      */\n    tblEnv.executeSql(\n      \"\"\"\n        |create table kafka_db_bussiness_tbl(\n        |   database string,\n        |   `table` string,\n        |   type string,\n        |   ts string,\n        |   xid string,\n        |   `commit` string,\n        |   data map&lt;string,string&gt;\n        |) with (\n        | 'connector' = 'kafka',\n        | 'topic' = 'KAFKA-DB-BUSSINESS-DATA',\n        | 'properties.bootstrap.servers'='node1:9092,node2:9092,node3:9092',\n        | 'scan.startup.mode'='latest-offset', --也可以指定 earliest-offset 、latest-offset\n        | 'properties.group.id' = 'my-group-id',\n        | 'format' = 'json'\n        |)\n      \"\"\".stripMargin)\n\n    /**\n      * 3.将不同的业务库数据存入各自的Iceberg表\n      */\n    tblEnv.executeSql(\n      \"\"\"\n        |insert into hadoop_iceberg.icebergdb.ODS_MEMBER_INFO\n        |select\n        |   data['id'] as id ,\n        |   data['user_id'] as user_id,\n        |   data['member_growth_score'] as member_growth_score,\n        |   data['member_level'] as member_level,\n        |   data['balance'] as balance,\n        |   data['gmt_create'] as gmt_create,\n        |   data['gmt_modified'] as  gmt_modified\n        | from kafka_db_bussiness_tbl where `table` = 'mc_member_info'\n      \"\"\".stripMargin)\n\n\n    tblEnv.executeSql(\n      \"\"\"\n        |insert into hadoop_iceberg.icebergdb.ODS_MEMBER_ADDRESS\n        |select\n        |   data['id'] as id ,\n        |   data['user_id'] as user_id,\n        |   data['province'] as province,\n        |   data['city'] as city,\n        |   data['area'] as area,\n        |   data['address'] as address,\n        |   data['log'] as log,\n        |   data['lat'] as lat,\n        |   data['phone_number'] as phone_number,\n        |   data['consignee_name'] as consignee_name,\n        |   data['gmt_create'] as gmt_create,\n        |   data['gmt_modified'] as  gmt_modified\n        | from kafka_db_bussiness_tbl where `table` = 'mc_member_address'\n      \"\"\".stripMargin)\n\n    tblEnv.executeSql(\n      \"\"\"\n        |insert into hadoop_iceberg.icebergdb.ODS_USER_LOGIN\n        |select\n        |   data['id'] as id ,\n        |   data['user_id'] as user_id,\n        |   data['ip'] as ip,\n        |   data['login_tm'] as login_tm,\n        |   data['logout_tm'] as logout_tm\n        | from kafka_db_bussiness_tbl where `table` = 'mc_user_login'\n      \"\"\".stripMargin)\n\n    //4.读取 Kafka 中的数据，将维度数据另外存储到 Kafka 中\n    val kafkaTbl: Table = tblEnv.sqlQuery(\"select database,`table`,type,ts,xid,`commit`,data from kafka_db_bussiness_tbl\")\n\n    //5.将kafkaTbl Table 转换成DStream 与MySql中的数据\n    val kafkaDS: DataStream[Row] = tblEnv.toAppendStream[Row](kafkaTbl)\n\n    //6.设置mapState,用于广播流\n    val mapStateDescriptor = new MapStateDescriptor[String,JSONObject](\"mapStateDescriptor\",classOf[String],classOf[JSONObject])\n\n    //7.从MySQL中获取配置信息，并广播\n    val bcConfigDs: BroadcastStream[JSONObject] = env.addSource(MySQLUtil.getMySQLData(mysqlUrl,mysqlUser,mysqlPassWord)).broadcast(mapStateDescriptor)\n\n    //8.设置维度数据侧输出流标记\n    val dimDataTag = new OutputTag[String](\"dim_data\")\n\n    //9.只监控mysql 数据库lakehousedb 中的数据，其他库binlog不监控，连接两个流进行处理\n    val factMainDs: DataStream[String] = kafkaDS.filter(row=&gt;{\"lakehousedb\".equals(row.getField(0).toString)}).connect(bcConfigDs).process(new BroadcastProcessFunction[Row, JSONObject, String] {\n      override def processElement(row: Row, ctx: BroadcastProcessFunction[Row, JSONObject, String]#ReadOnlyContext, out: Collector[String]): Unit = {\n        //最后返回给Kafka 事实数据的json对象\n        val returnJsonObj = new JSONObject()\n        //获取广播状态\n        val robcs: ReadOnlyBroadcastState[String, JSONObject] = ctx.getBroadcastState(mapStateDescriptor)\n        //解析事件流数据\n        val nObject: JSONObject = CommonUtil.rowToJsonObj(row)\n        //获取当前时间流来自的库和表 ，样例数据如下\n        //lackhousedb,pc_product,insert,1646659263,21603,null,{gmt_create=1645493074001, category_id=220, product_name=黄金, product_id=npfSpLHB8U}\n        val dbName: String = nObject.getString(\"database\")\n        val tableName: String = nObject.getString(\"table\")\n        val key = dbName + \":\" + tableName\n        if (robcs.contains(key)) {\n          //维度数据\n          val jsonValue: JSONObject = robcs.get(key)\n          //维度数据，将对应的 jsonValue中的信息设置到流事件中\n          nObject.put(\"tbl_name\", jsonValue.getString(\"tbl_name\"))\n          nObject.put(\"tbl_db\", jsonValue.getString(\"tbl_db\"))\n          nObject.put(\"pk_col\", jsonValue.getString(\"pk_col\"))\n          nObject.put(\"cols\", jsonValue.getString(\"cols\"))\n          nObject.put(\"phoenix_tbl_name\", jsonValue.getString(\"phoenix_tbl_name\"))\n          ctx.output(dimDataTag, nObject.toString)\n        }else{\n          //事实数据，加入iceberg 表名写入Kafka ODS-DB-TOPIC topic中\n          if(\"mc_user_login\".equals(tableName)){\n            returnJsonObj.put(\"iceberg_ods_tbl_name\",\"ODS_USER_LOGIN\")\n            returnJsonObj.put(\"kafka_dwd_topic\",kafkaDwdUserLogTopic)\n            returnJsonObj.put(\"data\",nObject.toString)\n          }\n          out.collect(returnJsonObj.toJSONString)\n        }\n      }\n\n      override def processBroadcastElement(jsonObject: JSONObject, ctx: BroadcastProcessFunction[Row, JSONObject, String]#Context, out: Collector[String]): Unit = {\n        val tblDB: String = jsonObject.getString(\"tbl_db\")\n        val tblName: String = jsonObject.getString(\"tbl_name\")\n        //向状态中更新数据\n        val bcs: BroadcastState[String, JSONObject] = ctx.getBroadcastState(mapStateDescriptor)\n        bcs.put(tblDB + \":\" + tblName, jsonObject)\n        println(\"广播数据流设置完成...\")\n      }\n    })\n\n\n    //10.结果写入到Kafka -  dim_data_topic topic中\n    val props = new Properties()\n    props.setProperty(\"bootstrap.servers\",kafkaBrokers)\n    factMainDs.addSink(new FlinkKafkaProducer[String](kafkaOdsTopic,new KafkaSerializationSchema[String] {\n      override def serialize(element: String, timestamp: java.lang.Long): ProducerRecord[Array[Byte], Array[Byte]] = {\n        new ProducerRecord[Array[Byte],Array[Byte]](kafkaOdsTopic,null,element.getBytes())\n      }\n    },props,FlinkKafkaProducer.Semantic.AT_LEAST_ONCE))//暂时使用at_least_once语义，exactly_once语义有些bug问题\n\n    factMainDs.getSideOutput(dimDataTag).addSink(new FlinkKafkaProducer[String](kafkaDimTopic,new KafkaSerializationSchema[String] {\n      override def serialize(element: String, timestamp: java.lang.Long): ProducerRecord[Array[Byte], Array[Byte]] = {\n        new ProducerRecord[Array[Byte],Array[Byte]](kafkaDimTopic,null,element.getBytes())\n      }\n    },props,FlinkKafkaProducer.Semantic.AT_LEAST_ONCE))//暂时使用at_least_once语义，exactly_once语义有些bug问题\n\n    env.execute()\n\n  }\n\n}</code></pre>\n<p></p>\n<h2 id=\"%E4%BA%8C%E3%80%81%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E5%88%9B%E5%BB%BAIceberg-ODS%E5%B1%82%E8%A1%A8\"><strong>三、​​​​​​​​​​​​​​创建Iceberg-ODS层表</strong></h2>\n<p>代码在执行之前需要在Hive中预先创建对应的Iceberg表，创建Icebreg表方式如下：</p>\n<h3 id=\"1%E3%80%81%E5%9C%A8Hive%E4%B8%AD%E6%B7%BB%E5%8A%A0Iceberg%E8%A1%A8%E6%A0%BC%E5%BC%8F%E9%9C%80%E8%A6%81%E7%9A%84%E5%8C%85\"><strong>1、在Hive中添加Iceberg表格式需要的包</strong></h3>\n<p style=\"margin-left:.0001pt;text-align:justify;\">启动HDFS集群，node1启动Hive metastore服务，在Hive客户端启动Hive添加Iceberg依赖包：</p>\n<pre><code class=\"language-bash\">#node1节点启动Hive metastore服务\n[root@node1 ~]# hive --service metastore &amp;\n\n#在hive客户端node3节点加载两个jar包\nadd jar /software/hive-3.1.2/lib/iceberg-hive-runtime-0.12.1.jar;\nadd jar /software/hive-3.1.2/lib/libfb303-0.9.3.jar;</code></pre>\n<p></p>\n<h3 id=\"2%E3%80%81%E5%88%9B%E5%BB%BAIceberg%E8%A1%A8\"><strong>2、创建Iceberg表</strong></h3>\n<p>这里创建Iceberg表有“ODS_MEMBER_INFO”、“ODS_MEMBER_ADDRESS”、“ODS_USER_LOGIN”，创建语句如下：</p>\n<pre><code class=\"language-sql\">#在Hive客户端执行以下建表语句\nCREATE TABLE ODS_MEMBER_INFO  (\nid string,\nuser_id string,\nmember_growth_score string,\nmember_level string,\nbalance string,\ngmt_create string,\ngmt_modified string\n)STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \nLOCATION 'hdfs://mycluster/lakehousedata/icebergdb/ODS_MEMBER_INFO/' \nTBLPROPERTIES ('iceberg.catalog'='location_based_table',\n'write.metadata.delete-after-commit.enabled'= 'true',\n'write.metadata.previous-versions-max' = '3'\n);\n\n\nCREATE TABLE ODS_MEMBER_ADDRESS  (\nid string,\nuser_id string,\nprovince string,\ncity string,\narea string,\naddress string,\nlog string,\nlat string,\nphone_number string,\nconsignee_name string,\ngmt_create string,\ngmt_modified string\n)STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \nLOCATION 'hdfs://mycluster/lakehousedata/icebergdb/ODS_MEMBER_ADDRESS/' \nTBLPROPERTIES ('iceberg.catalog'='location_based_table',\n'write.metadata.delete-after-commit.enabled'= 'true',\n'write.metadata.previous-versions-max' = '3'\n);\n\nCREATE TABLE ODS_USER_LOGIN (\nid string,\nuser_id string,\nip string,\nlogin_tm string,\nlogout_tm string\n)STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \nLOCATION 'hdfs://mycluster/lakehousedata/icebergdb/ODS_USER_LOGIN/' \nTBLPROPERTIES ('iceberg.catalog'='location_based_table',\n'write.metadata.delete-after-commit.enabled'= 'true',\n'write.metadata.previous-versions-max' = '3'\n);</code></pre>\n<p>以上语句在Hive客户端执行完成之后，在HDFS中可以看到对应的Iceberg数据目录：</p>\n<p><img alt=\"\" height=\"353\" src=\"image\\777075dbd7e840fe8823f1e89936e264.png\" width=\"963\"/></p>\n<p></p>\n<h2 id=\"%E4%B8%89%E3%80%81%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95\"><strong>四、代码测试</strong></h2>\n<p style=\"margin-left:.0001pt;text-align:justify;\">以上代码编写完成后，代码执行测试步骤如下：</p>\n<h3 id=\"1%E3%80%81%E5%9C%A8Kafka%E4%B8%AD%E5%88%9B%E5%BB%BA%E5%AF%B9%E5%BA%94%E7%9A%84topic\" style=\"margin-left:.0001pt;text-align:justify;\"><strong>1、在Kafka中创建对应的topic</strong></h3>\n<pre><code class=\"language-bash\">#在Kafka 中创建 KAFKA-ODS-TOPIC topic\n./kafka-topics.sh --zookeeper node3:2181,node4:2181,node5:2181 --create --topic KAFKA-ODS-TOPIC --partitions 3 --replication-factor 3\n\n#在Kafka 中创建 KAFKA-DIM-TOPIC topic\n./kafka-topics.sh --zookeeper node3:2181,node4:2181,node5:2181 --create --topic KAFKA-DIM-TOPIC --partitions 3 --replication-factor 3\n\n#监控以上两个topic数据\n[root@node1 bin]# ./kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic KAFKA-ODS-TOPIC\n\n[root@node1 bin]# ./kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic KAFKA-DIM-TOPIC</code></pre>\n<p></p>\n<h3 id=\"2%E3%80%81%E5%B0%86%E4%BB%A3%E7%A0%81%E4%B8%AD%E6%B6%88%E8%B4%B9Kafka%E6%95%B0%E6%8D%AE%E6%94%B9%E6%88%90%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E6%B6%88%E8%B4%B9\"><strong>2、将代码中消费Kafka数据改成从头开始消费</strong></h3>\n<p style=\"margin-left:.0001pt;text-align:justify;\">代码中Kafka Connector中属性“scan.startup.mode”设置为“earliest-offset”，从头开始消费数据。</p>\n<p style=\"margin-left:.0001pt;text-align:justify;\">这里也可以不设置从头开始消费Kafka数据，而是直接启动实时向MySQL表中写入数据代码“RTMockDBData.java”代码，实时向MySQL对应的表中写入数据，这里需要启动maxwell监控数据，代码才能实时监控到写入MySQL的业务数据。</p>\n<p style=\"margin-left:.0001pt;text-align:justify;\"></p>\n<h3 id=\"3%E3%80%81%E6%89%A7%E8%A1%8C%E4%BB%A3%E7%A0%81%EF%BC%8C%E6%9F%A5%E7%9C%8B%E5%AF%B9%E5%BA%94topic%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%9C\" style=\"margin-left:.0001pt;text-align:justify;\"><strong>3、执行代码，查看对应topic中的结果</strong></h3>\n<p style=\"margin-left:.0001pt;text-align:justify;\">以上代码执行后在，在对应的Kafka “KAFKA-DIM-TOPIC”和“KAFKA-ODS-TOPIC”中都有对应的数据。在Iceberg-ODS层中对应的表中也有数据。</p>\n<hr/>\n<ul><li>📢博客主页：<a href=\"https://lansonli.blog.csdn.net/\" title=\"https://lansonli.blog.csdn.net\">https://lansonli.blog.csdn.net</a></li><li>📢欢迎点赞 👍 收藏 ⭐留言 📝 如有错误敬请指正！</li><li>📢本文由 Lansonli 原创，首发于 CSDN博客🙉</li><li>📢停下休息的时候不要忘了别人还在奔跑，希望大家抓紧时间学习，全力奔赴更美好的生活✨</li></ul>\n</div>\n</div>"}