{"blogid": "126701018", "writerAge": "码龄5年", "writerBlogNum": "83", "writerCollect": "768", "writerComment": "61", "writerFan": "168", "writerGrade": "4级", "writerIntegral": "1544", "writerName": "Jack Ju", "writerProfileAdress": "writer_image\\profile_126701018.jpg", "writerRankTotal": "12385", "writerRankWeekly": "57167", "writerThumb": "117", "writerVisitNum": "79419", "blog_read_count": "302", "blog_time": "已于 2022-09-05 18:32:53 修改", "blog_title": "K-Means聚类算法---C++", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-atom-one-dark\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<h2><a id=\"1Introduction_0\"></a>1.Introduction</h2>\n<p>K-Means算法是无监督的聚类算法，它实现起来比较简单，聚类效果也不错，因此应用很广泛。K-Means算法有大量的变体，本文就从最传统的K-Means算法讲起，在其基础上讲述K-Means的优化变体方法。包括初始化优化K-Means++, 距离计算优化elkan K-Means算法和大数据情况下的优化Mini Batch K-Means算法。</p>\n<h2><a id=\"2Algorithm_3\"></a>2.Algorithm</h2>\n<p><img alt=\"在这里插入图片描述\" src=\"image\\3004504ae1f547ea88c04c450645bf83.png\"/></p>\n<p>K-Means采用的启发式方式很简单，用下面一组图就可以形象的描述。</p>\n<p><img alt=\"在这里插入图片描述\" src=\"image\\269f5abe1ada4b9b8dcce7b9d989f6c3.png\"/></p>\n<p>上图a表达了初始的数据集，假设k=2。在图b中，我们随机选择了两个k类所对应的类别质心，即图中的红色质心和蓝色质心，然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，如图c所示，经过计算样本和红色质心和蓝色质心的距离，我们得到了所有样本点的第一轮迭代后的类别。此时我们对我们当前标记为红色和蓝色的点分别求其新的质心，如图4所示，新的红色质心和蓝色质心的位置已经发生了变动。图e和图f重复了我们在图c和图d的过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。最终我们得到的两个类别如图f。</p>\n<p>当然在实际K-Mean算法中，我们一般会多次运行图c和图d，才能达到最终的比较优的</p>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>"}