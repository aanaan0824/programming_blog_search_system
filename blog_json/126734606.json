{"blogid": "126734606", "writerAge": "码龄3年", "writerBlogNum": "58", "writerCollect": "30", "writerComment": "3", "writerFan": "292", "writerGrade": "3级", "writerIntegral": "622", "writerName": "群马视觉", "writerProfileAdress": "writer_image\\profile_126734606.jpg", "writerRankTotal": "32713", "writerRankWeekly": "2715", "writerThumb": "39", "writerVisitNum": "5954", "blog_read_count": "75", "blog_time": "于 2022-09-06 21:46:33 发布", "blog_title": "Python基于OpenCV＆YOLO台球击球路线规划系统（源码＆部署教程）", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-atom-one-dark\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<h1><a id=\"1_0\"></a>1.项目效果展示</h1>\n<p><img alt=\"1.png\" src=\"image\\a7f4d71f890c4a66019bcf4eb8328ca0.png\"/></p>\n<p><img alt=\"2.png\" src=\"image\\5ce831ecc131fe21c2d9713343cb5916.png\"/></p>\n<p><img alt=\"3.png\" src=\"image\\f703a3663da86c9547367e2f0a93fce5.png\"/></p>\n<p><img alt=\"4.png\" src=\"image\\083d334b748545edb60e423dae5f52fa.png\"/></p>\n<h1><a id=\"2_12\"></a>2.视频演示</h1>\n<p><a href=\"https://www.bilibili.com/video/BV18t4y1776D/?vd_source=bc9aec86d164b67a7004b996143742dc\">Python基于OpenCV＆YOLO台球击球路线规划系统（源码＆部署教程）_哔哩哔哩_bilibili</a></p>\n<h1><a id=\"3YOLOv7_19\"></a>3.YOLOv7算法简介</h1>\n<h2><a id=\"YOLOv7__5_FPS__160_FPS__20\"></a>YOLOv7 在 5 FPS 到 160 FPS 范围内，速度和精度都超过了所有已知的目标检测器</h2>\n<p>并在 <a href=\"https://cloud.tencent.com/product/gpu?from=10680\">GPU</a> V100 上，30 FPS 的情况下达到实时目标检测器的最高精度 56.8% AP。YOLOv7 是在 MS COCO 数据集上从头开始训练的，不使用任何其他数据集或预训练权重。<br/> 相对于其他类型的工具，YOLOv7-E6 目标检测器（56 FPS V100，55.9% AP）比基于 transformer 的检测器 SWINL Cascade-Mask R-CNN（9.2 FPS A100，53.9% AP）速度上高出 509%，精度高出 2%，比基于卷积的检测器 ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) 速度高出 551%，精度高出 0.7%。<br/> <img alt=\"5.png\" src=\"image\\5d5f914f57b4353560105be64b657c9a.png\"/></p>\n<p>此外， YOLOv7 的在速度和精度上的表现也优于 YOLOR、YOLOX、Scaled-YOLOv4、YOLOv5、DETR 等多种目标检测器。</p>\n<h1><a id=\"4YOLOv7__27\"></a>4.YOLOv7 技术方法</h1>\n<p>近年来，实时目标检测器仍在针对不同的边缘设备进行开发。例如，MCUNet 和 NanoDet 的开发专注于生产低功耗单芯片并提高边缘 CPU 的推理速度；YOLOX、YOLOR 等方法专注于提高各种 GPU 的推理速度；实时目标检测器的发展集中在高效架构的设计上；在 CPU 上使用的实时目标检测器的设计主要基于 MobileNet、ShuffleNet 或 GhostNet；为 GPU 开发的实时目标检测器则大多使用 ResNet、DarkNet 或 DLA，并使用 CSPNet 策略来优化架构。</p>\n<p>YOLOv7 的发展方向与当前主流的实时目标检测器不同，研究团队希望它能够同时支持移动 GPU 和从边缘到云端的 GPU 设备。除了架构优化之外，该研究提出的方法还专注于训练过程的优化，将重点放在了一些优化模块和优化方法上。这可能会增加训练成本以提高目标检测的准确性，但不会增加推理成本。研究者将提出的模块和优化方法称为可训练的「bag-of-freebies」。</p>\n<p>对于模型重参数化，该研究使用梯度传播路径的概念分析了适用于不同网络层的模型重参数化策略，并提出了有计划的重参数化模型。此外，研究者发现使用动态标签分配技术时，具有多个输出层的模型在训练时会产生新的问题：「如何为不同分支的输出分配动态目标？」针对这个问题，研究者提出了一种新的标签分配方法，称为从粗粒度到细粒度（coarse-to-fine）的引导式标签分配。</p>\n<h2><a id=\"_34\"></a>该研究的主要贡献包括：</h2>\n<p>(1) 设计了几种可训练的 bag-of-freebies 方法，使得实时目标检测可以在不增加推理成本的情况下大大提高检测精度；</p>\n<p>(2) 对于目标检测方法的演进，研究者发现了两个新问题：一是重参数化的模块如何替换原始模块，二是动态标签分配策略如何处理分配给不同输出层的问题，并提出了解决这两个问题的方法；</p>\n<p>(3) 提出了实时目标检测器的「扩充（extend）」和「复合扩展（compound scale）」方法，以有效地利用参数和计算；</p>\n<p>(4) 该研究提出的方法可以有效减少 SOTA 实时目标检测器约 40% 的参数和 50% 的计算量，并具有更快的推理速度和更高的检测精度。</p>\n<p>在大多数关于设计高效架构的文献中，人们主要考虑的因素包括参数的数量、计算量和计算密度。下图 2（b）中 CSPVoVNet 的设计是 VoVNet 的变体。CSPVoVNet 的架构分析了梯度路径，以使不同层的权重能够学习更多不同的特征，使推理更快、更准确。图 2 © 中的 ELAN 则考虑了「如何设计一个高效网络」的问题。</p>\n<p>YOLOv7 研究团队提出了基于 ELAN 的扩展 E-ELAN，其主要架构如图所示。<br/> <img alt=\"6.png\" src=\"image\\6d74f2291c696850e7b078bc6e141bf6.png\"/><br/> 新的 E-ELAN 完全没有改变原有架构的梯度传输路径，其中使用组卷积来增加添加特征的基数（cardinality），并以 shuffle 和 merge cardinality 的方式组合不同组的特征。这种操作方式可以增强不同特征图学得的特征，改进参数的使用和计算效率。</p>\n<p>无论梯度路径长度和大规模 ELAN 中计算块的堆叠数量如何，它都达到了稳定状态。如果无限堆叠更多的计算块，可能会破坏这种稳定状态，参数利用率会降低。新提出的 E-ELAN 使用 expand、shuffle、merge cardinality 在不破坏原有梯度路径的情况下让网络的学习能力不断增强。</p>\n<p>在架构方面，E-ELAN 只改变了计算块的架构，而过渡层（transition layer）的架构完全没有改变。YOLOv7 的策略是使用组卷积来扩展计算块的通道和基数。研究者将对计算层的所有计算块应用相同的组参数和通道乘数。然后，每个计算块计算出的特征图会根据设置的组参数 g 被打乱成 g 个组，再将它们连接在一起。此时，每组特征图的通道数将与原始架构中的通道数相同。最后，该方法添加 g 组特征图来执行 merge cardinality。除了保持原有的 ELAN 设计架构，E-ELAN 还可以引导不同组的计算块学习更多样化的特征。<br/> 因此，对基于串联的模型，我们不能单独分析不同的扩展因子，而必须一起考虑。该研究提出图 （c），即在对基于级联的模型进行扩展时，只需要对计算块中的深度进行扩展，其余传输层进行相应的宽度扩展。这种复合扩展方法可以保持模型在初始设计时的特性和最佳结构。</p>\n<p>此外，该研究使用梯度流传播路径来分析如何重参数化卷积，以与不同的网络相结合。下图展示了该研究设计的用于 PlainNet 和 ResNet 的「计划重参数化卷积」。<br/> <img alt=\"7.png\" src=\"image\\96c6f2923b4255ab77936ec12e5a4934.png\"/></p>\n<h1><a id=\"5_58\"></a>5.台球击球路线原理</h1>\n<p>(1)球路的选定</p>\n<p>台球的瞄准方法，是根据力的直线传递原理，通过主球撞击目标球，目标球被撞击后，便沿着直线进入球袋。因为球台上有6个球袋，分别固定在四角和边岸中部，而球是可以在球台上到处滚动的，要想把其中某一个球打进球袋，必须在球群中观察选择，哪个球的球路、角度最合适、容易进袋，在袋口附近有一个目标球，要想使这个球进袋，便要先看看球路是否合适，然后由球袋口中心，通过目标球中心，划一条直线，这条直线便是目标球进袋要走的路线。</p>\n<p>(2)目标球上的击点</p>\n<p>因为目标球没有外力推动本身是不能滚动的，必须通过主球的撞击才行。根据力的直线传递原理，要把某个目标球打进球袋，不能随便乱撞目标球上的任何点位，必须根据目标球的进袋线路，确定主球应该撞击目标球上的击点位置，才能完成打球入袋。<br/> 　　由目标球所对着的球袋中心，经过目标球中心延长线，这条线和目标球外圆相交在M点，这个点便是目标球上的击点。</p>\n<p>(3)瞄准点</p>\n<p>目标球上的击点确定之后，接着要确定瞄准点的位置。从目标球上的击点M，再沿这条直线向后，量出一段与球的半径相等的长度，最远点T就是瞄准点。以T点为圆心划一个圆形虚线，这个圆球形表示主球要撞击目标球必须来到的位置，主球与目标球的外圆才能在M点相交，与目标球相撞击，主球上的力量便通过M点传递给目标球，使它沿目标球球路入袋。<br/> <img alt=\"7.png\" src=\"image\\27b5c6c31d52126fd91661a8966b15f2.png\"/></p>\n<h1><a id=\"6_73\"></a>6.整合代码实现</h1>\n<pre><code>import torch, cv2, os, tqdm\nimport numpy as np\n\nimport numpy as np\n\nfrom yolov5.yolo import YOLOV5_Detect, opt\n\ndef get_hole(pred):\n    pred[pred == 1] = 0\n    pred[pred != 0] = 255\n    contours, hierarchy = cv2.findContours(pred[:, :, 0], cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return contours\n\ndef distance(x1,y1,x2,y2):\n    return ((x1 - x2)**2+(y1 - y2)**2)**0.5\n\nif __name__ == '__main__':\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(DEVICE)\n\n    model = torch.load('model.pkl')\n    model.to(DEVICE)\n\n    if not os.path.exists('output'):\n        os.mkdir('output')\n\n    path = 'detect'\n\n\n    yolo_detect = YOLOV5_Detect(**vars(opt))\n\n    for i in tqdm.tqdm(os.listdir(path)):\n        # 读取图像\n        ori_img = cv2.imdecode(np.fromfile('{}/{}'.format(path, i), np.uint8), cv2.IMREAD_COLOR)\n        yolov5_res,balllist,ballvalue = yolo_detect.detect(ori_img)\n        # 记录原图尺寸\n        img_shape = ori_img.shape\n        # Resize到训练大小 640*320\n        img_ = cv2.resize(ori_img, (640, 320))\n        # 转换通道 归一化\n        img = np.transpose(np.expand_dims(img_, axis=0), (0, 3, 1, 2)) / 255.0\n        # 转换成tensor格式\n        img = torch.from_numpy(img).to(DEVICE).float()\n        # 预测\n        pred = np.argmax(model(img).cpu().detach().numpy()[0], axis=0)\n\n        # 1 2 对应着目标类别\n        pred_mask = []\n        for j in pred.reshape((-1)):\n            if j == 0:\n                pred_mask.append(np.array([0, 0, 0]))\n            elif j == 1:\n                pred_mask.append(np.array([0, 0, 255]))\n            elif j == 2:\n                pred_mask.append(np.array([255, 0, 0]))\n        pred_mask = np.array(pred_mask, dtype=np.uint8).reshape((pred.shape[0], pred.shape[1], 3))\n\n        pred = np.expand_dims(pred, axis=-1)\n        pred = np.repeat(pred, axis=-1, repeats=3)\n        pred = np.array(pred, dtype=np.uint8)\n        pred = cv2.resize(pred, (img_shape[1], img_shape[0]), interpolation=cv2.INTER_NEAREST)\n        pred_mask = cv2.resize(pred_mask, (img_shape[1], img_shape[0]), interpolation=cv2.INTER_NEAREST)\n        contours = get_hole(pred)\n\n        pred[pred == 0] = 255\n        pred[pred != 255] = 0\n        yolov5_res = yolov5_res &amp; pred\n        # yolov5_res = cv2.addWeighted(yolov5_res, 0.5, pred_mask, 0.5, 0)\n        # ori_img = yolov5_res &amp; pred\n\n        holes_coordinate = []\n        radius = 0\n        number = 0\n        for cnts in contours:\n            x, y, w, h = cv2.boundingRect(cnts)\n            holes_coordinate.append([x, y, w, h])\n            radius = radius + (w + h)/2\n            number = number + 1\n        holllist = []\n        hollvalue = []\n        holes_coordinate = sorted(holes_coordinate, key=lambda x:x[2] * x[3], reverse=True)[:6]\n        holes_coordinate = sorted(holes_coordinate, key=lambda x:x[1])\n        for idx, (x, y, w, h) in enumerate(sorted(holes_coordinate[:3], key=lambda x:x[0])):\n            cv2.rectangle(yolov5_res, (x, y), (x + w, y + h), (255, 0, 0), 2)\n            cv2.putText(yolov5_res, '{:.0f}'.format(idx + 1),\n                        (x, y + 25), cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n                        (255, 0, 0), 2)\n            holllist.append(idx + 1)\n            hollvalue.append([x + w / 2, y + h / 2, (w + h) / 2])\n            #print('hole {} x_center:{:.2f} y_center:{:.2f} radius:{:.2f}'.format(idx + 1, x + w / 2, y + h / 2, (w + h) / 2))\n\n        for idx, (x, y, w, h) in enumerate(sorted(holes_coordinate[3:], key=lambda x:x[0])):\n            cv2.rectangle(yolov5_res, (x, y), (x + w, y + h), (255, 0, 0), 2)\n            cv2.putText(yolov5_res, '{:.0f}'.format(idx + 4),\n                        (x, y + 25), cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n                        (255, 0, 0), 2)\n            holllist.append(idx + 4)\n            hollvalue.append([x + w / 2, y + h / 2, (w + h) / 2])\n            #print('hole {} x_center:{:.2f} y_center:{:.2f} radius:{:.2f}'.format(idx + 4, x + w / 2, y + h / 2, (w + h) / 2))\n        for m in range(len(balllist)):\n            cv2.circle(yolov5_res, (int(ballvalue[m][0]),int(ballvalue[m][1])), int(ballvalue[m][2]), [0,0,255], 2)\n        cv2.imshow('input', yolov5_res)\n        cv2.waitKey(0)\n        print(balllist)\n        print(ballvalue)\n        print(holllist)\n        print(hollvalue)\n        a = input(\"请输入母球编号: \")\n        b = input(\"请输入目标球编号: \")\n        c = input(\"请输入袋口编号: \")\n        x1 = int(ballvalue[balllist.index(int(a))][0])\n        y1 = int(ballvalue[balllist.index(int(a))][1])\n        r1 = int(ballvalue[balllist.index(int(a))][2])\n        x2 = int(ballvalue[balllist.index(int(b))][0])\n        y2 = int(ballvalue[balllist.index(int(b))][1])\n        r2 = int(ballvalue[balllist.index(int(b))][2])\n        x3 = int(hollvalue[holllist.index(int(c))][0])\n        y3 = int(hollvalue[holllist.index(int(c))][1])\n        r3 = int(hollvalue[holllist.index(int(c))][2])\n        #画出目标球的可能行进路线：\n        def drawline(yolov5_res,x1,y1,x2,y2,r):\n            gen = ((x2-x1)**2+(y2-y1)**2)**0.5\n            x3 = int(x1 - (y2-y1)*r/gen)\n            y3 = int(y1 + (x2-x1)*r/gen)\n            x4 = int(x2 - (y2-y1)*r/gen)\n            y4 = int(y2 + (x2-x1)*r/gen)\n            x5 = int(x1 + (y2 - y1) * r / gen)\n            y5 = int(y1 - (x2 - x1) * r / gen)\n            x6 = int(x2 + (y2 - y1) * r / gen)\n            y6 = int(y2 - (x2 - x1) * r / gen)\n            cv2.line(yolov5_res, (x3, y3), (x4, y4), (255, 255, 255), 3)\n            cv2.line(yolov5_res, (x5, y5), (x6, y6), (255, 255, 255), 3)\n            return yolov5_res\n\n        cv2.line(yolov5_res,(x2,y2),(x3,y3),(255, 0, 0),3)\n        yolov5_res = drawline(yolov5_res,x2,y2,x3,y3,r2)\n        #画出撞击点\n        xz = int(x3 + (x2 - x3)*((r1+r2) + distance(x2,y2,x3,y3))/(distance(x2,y2,x3,y3)))\n        yz = int(y3 + (y2 - y3)*((r1+r2) + distance(x2,y2,x3,y3))/(distance(x2,y2,x3,y3)))\n        cv2.circle(yolov5_res, (xz,yz), int(ballvalue[balllist.index(int(a))][2]), [255, 0, 0], 2)\n        #画出母球的行进路线\n        cv2.line(yolov5_res, (xz, yz), (x1, y1), (255, 0, 0), 3)\n        yolov5_res = drawline(yolov5_res, xz, yz, x1, y1,r1)\n        cv2.imshow('output',yolov5_res)\n        cv2.waitKey(0)\n\n        #cv2.imwrite('output/{}'.format(i), yolov5_res)\n</code></pre>\n<h1><a id=\"7_224\"></a>7.项目文件展示</h1>\n<p><img alt=\"5.png\" src=\"image\\4954bd853d9a30628827ca20c31b7464.png\"/></p>\n<h1><a id=\"8_227\"></a>8.完整源码&amp;环境部署视频教程</h1>\n<p><a href=\"https://mianbaoduo.com/o/bread/Y5WZlJ5p\">Python基于OpenCV＆YOLO台球击球路线规划系统（源码＆部署教程） (mianbaoduo.com)</a></p>\n<h1><a id=\"9_230\"></a>9.参考文献</h1>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>"}