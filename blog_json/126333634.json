{"blogid": "126333634", "writerAge": "ç é¾„2å¹´", "writerBlogNum": "180", "writerCollect": "4030", "writerComment": "2963", "writerFan": "23586", "writerGrade": "7çº§", "writerIntegral": "11032", "writerName": "ä¾¯å°å•¾", "writerProfileAdress": "writer_image\\profile_126333634.jpg", "writerRankTotal": "1009", "writerRankWeekly": "23", "writerThumb": "3361", "writerVisitNum": "332879", "blog_read_count": "1125", "blog_time": "å·²äºÂ 2022-09-02 22:28:50Â ä¿®æ”¹", "blog_title": "æ–‡æœ¬ç‰¹å¾æå–ä¸“é¢˜_ä»¥pythonä¸ºå·¥å…·ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆåäºŒï¼‰ã€‘", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-kimbie-light\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<p></p>\n<div class=\"toc\">\n<h3>æ–‡ç« ç›®å½•</h3>\n<ul><li><a href=\"#1_DictVectorizer_13\">1.å­—å…¸æ–‡æœ¬ç‰¹å¾æå– DictVectorizer()</a></li><li><ul><li><a href=\"#11_onehot_14\">1.1 one-hotç¼–ç </a></li><li><a href=\"#12_sparse_36\">1.2 å­—å…¸æ•°æ®è½¬sparseçŸ©é˜µ</a></li></ul>\n</li><li><a href=\"#2_51\">2.è‹±æ–‡æ–‡æœ¬ç‰¹å¾æå–</a></li><li><a href=\"#3_89\">3.ä¸­æ–‡æ–‡æœ¬ç‰¹å¾æå–</a></li><li><a href=\"#4_TFIDF__TfidfVectorizer_136\">4. TF-IDF æ–‡æœ¬ç‰¹å¾æå– TfidfVectorizer()</a></li></ul>\n</div>\n<p></p>\n<hr/>\n<p><font size=\"5\">â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”É</font><br/> â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"image\\7cc703968fc94dd7a64c66270f4a6193.png\"/><img alt=\"è¯·æ·»åŠ å›¾ç‰‡æè¿°\" src=\"https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif\"/><img alt=\"è¯·æ·»åŠ å›¾ç‰‡æè¿°\" src=\"https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif\"/><img alt=\"è¯·æ·»åŠ å›¾ç‰‡æè¿°\" src=\"https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif\"/><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"image\\f9d8225c11fe42508a5bd938f8d99e24.png\"/><br/> <font size=\"5\">â€ƒâ€ƒâ€ƒâ€ƒÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”ÉÊšÊ•Ì¯â€¢Í¡Ë”â€¢Ì¯á·…Ê”É</font></p>\n<hr/>\n<p><font color=\"purple\" size=\"4\">å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä¾¯å°å•¾ï¼</font><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"image\\3ed5e82a513a4a41b019a3b3d747ef7f.png\"/></p>\n<p><font color=\"brown\" size=\"4\">â€ƒ<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"image\\4e3c21f29fd0442a8bf8b90dcf152434.png\"/>ä»Šå¤©åˆ†äº«çš„è¯é¢˜æ˜¯ï¼Œæ–‡æœ¬ç‰¹å¾çš„æå–ã€‚å…·ä½“å†…å®¹è¯·è§ä¸‹æ–‡ï¼š</font></p>\n<hr/>\n<h1><a id=\"1_DictVectorizer_13\"></a>1.å­—å…¸æ–‡æœ¬ç‰¹å¾æå– DictVectorizer()</h1>\n<h2><a id=\"11_onehot_14\"></a>1.1 one-hotç¼–ç </h2>\n<p><font color=\"purple\" size=\"4\">åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œè§‚å¯Ÿå¦‚ä¸‹æ•°æ®å½¢å¼çš„å˜åŒ–ï¼š</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction <span class=\"token keyword\">import</span> DictVectorizer\n\n\ndata <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">{<!-- --></span><span class=\"token string\">'city'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'æ´›é˜³'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'temperature'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">39</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">{<!-- --></span><span class=\"token string\">'city'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'æˆéƒ½'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'temperature'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">41</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">{<!-- --></span><span class=\"token string\">'city'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'å®æ³¢'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'temperature'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">42</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">{<!-- --></span><span class=\"token string\">'city'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'ä½›å±±'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'temperature'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">38</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span>\n\ndf1 <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>df1<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># one-hotç¼–ç  å› ä¸ºtemperatureæ˜¯æ•°å€¼å‹çš„ï¼Œæ‰€ä»¥ä¼šä¿ç•™åŸå§‹å€¼ï¼Œåªæœ‰å­—ç¬¦ä¸²ç±»å‹çš„æ‰ä¼šç”Ÿæˆè™šæ‹Ÿå˜é‡</span>\ndf2 <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>get_dummies<span class=\"token punctuation\">(</span>df1<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>df2<span class=\"token punctuation\">)</span>\n</code></pre>\n<p><font color=\"purple\" size=\"4\">è¾“å‡ºå¦‚ä¸‹ï¼š<br/> â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"image\\2606ffc3283e41e4afdcc2729b2d6742.png\"/></font></p>\n<hr/>\n<h2><a id=\"12_sparse_36\"></a>1.2 å­—å…¸æ•°æ®è½¬sparseçŸ©é˜µ</h2>\n<p><font color=\"brown\" size=\"4\">ä½¿ç”¨DictVectorizer()åˆ›å»ºå­—å…¸ç‰¹å¾æå–æ¨¡å‹</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token comment\"># 1.åˆ›å»ºå¯¹è±¡  é»˜è®¤sparse=True è¿”å›çš„æ˜¯sparseçŸ©é˜µï¼›  sparse=False  è¿”å›çš„æ˜¯ndarrayçŸ©é˜µ</span>\ntransfer <span class=\"token operator\">=</span> DictVectorizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 2.è½¬åŒ–æ•°æ®å¹¶è®­ç»ƒ</span>\ntrans_data <span class=\"token operator\">=</span> transfer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>transfer<span class=\"token punctuation\">.</span>get_feature_names_out<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> \n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>trans_data<span class=\"token punctuation\">)</span>\n</code></pre>\n<p>â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"image\\54722bb1cefa42949e619201346b3e3e.png\"/><br/> â€ƒ<br/> <font color=\"brown\" size=\"4\">ä½¿ç”¨sparseçŸ©é˜µæ²¡æœ‰æ˜¾ç¤º0æ•°æ®ï¼ŒèŠ‚çº¦äº†å†…å­˜ï¼Œæ›´ä¸ºç®€æ´ï¼Œè¿™ä¸€ç‚¹æ¯”ndarrayçŸ©é˜µæ›´å¥½ã€‚</font></p>\n<hr/>\n<h1><a id=\"2_51\"></a>2.è‹±æ–‡æ–‡æœ¬ç‰¹å¾æå–</h1>\n<p><font color=\"brown\" size=\"4\">æ–‡æœ¬ç‰¹å¾æå–ä½¿ç”¨çš„æ˜¯CountVectorizeræ–‡æœ¬ç‰¹å¾æå–æ¨¡å‹ï¼Œè¿™é‡Œå‡†å¤‡äº†ä¸€æ®µè‹±æ–‡æ–‡æœ¬ï¼ˆI have a dreamï¼‰ã€‚ç»Ÿè®¡è¯é¢‘å¹¶å¾—åˆ°sparseçŸ©é˜µï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š<br/> â€ƒ<br/> <font color=\"purple\" size=\"4\">CountVectorizer()æ²¡æœ‰sparseå‚æ•°ï¼Œé»˜è®¤é‡‡ç”¨sparseçŸ©é˜µæ ¼å¼ã€‚ä¸”å¯ä»¥é€šè¿‡stop_wordsæŒ‡å®šåœç”¨è¯ã€‚</font></font></p>\n<pre><code class=\"prism language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> CountVectorizer\n\n\ndata <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"I have a dream that one day this nation will rise up and live out the true meaning of its creed\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"We hold these truths to be self-evident, that all men are created equal\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"I have a dream that one day on the red hills of Georgia, \"</span>\n        <span class=\"token string\">\"the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"I have a dream that one day even the state of Mississippi\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\" a state sweltering with the heat of injustice\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"sweltering with the heat of oppression\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"will be transformed into an oasis of freedom and justice\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"I have a dream today\"</span><span class=\"token punctuation\">]</span>\n\n\n<span class=\"token comment\"># CountVectorizeræ–‡æœ¬ç‰¹å¾æå–æ¨¡å‹</span>\n\n<span class=\"token comment\"># 1.å®ä¾‹åŒ–  å°†\"is\"æ ‡è®°ä¸ºåœç”¨è¯</span>\nc_transfer <span class=\"token operator\">=</span> CountVectorizer<span class=\"token punctuation\">(</span>stop_words<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"is\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2.è°ƒç”¨fit_transform</span>\nc_trans_data <span class=\"token operator\">=</span> c_transfer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\"># æ‰“å°ç‰¹å¾åç§°</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>c_transfer<span class=\"token punctuation\">.</span>get_feature_names_out<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># æ‰“å°sparseçŸ©é˜µ</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>c_trans_data<span class=\"token punctuation\">)</span>\n</code></pre>\n<p><font color=\"brown\" size=\"4\">è¾“å‡ºç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š<br/> â€ƒâ€ƒâ€ƒâ€ƒ<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"image\\a11093bd4a5a43b0a67bfc3dbafd00df.png\"/></font></p>\n<hr/>\n<h1><a id=\"3_89\"></a>3.ä¸­æ–‡æ–‡æœ¬ç‰¹å¾æå–</h1>\n<p><font color=\"brown\" size=\"4\">å‡†å¤‡ä¸€æ®µä¸­æ–‡æ–‡æœ¬ï¼ˆdata.txtï¼‰ï¼Œä»¥æ°´æµ’ä¼ ä¸­é£é›ªå±±ç¥åº™æƒ…èŠ‚ä¸ºä¾‹ï¼š</font></p>\n<blockquote>\n<p>å¤§é›ªä¸‹çš„æ­£ç´§ï¼Œæ—å†²å’Œå·®æ‹¨ä¸¤ä¸ªåœ¨è·¯ä¸Šåˆæ²¡ä¹°é…’åƒå¤„ã€‚æ—©æ¥åˆ°è‰æ–™åœºå¤–ï¼Œçœ‹æ—¶ï¼Œä¸€å‘¨é­æœ‰äº›é»„åœŸå¢™ï¼Œä¸¤æ‰‡å¤§é—¨ã€‚æ¨å¼€çœ‹é‡Œé¢æ—¶ï¼Œä¸ƒå…«é—´è‰æˆ¿åšç€ä»“å»’ï¼Œå››ä¸‹é‡Œéƒ½æ˜¯é©¬è‰å †ï¼Œä¸­é—´ä¸¤åº§è‰å…ã€‚åˆ°é‚£å…é‡Œï¼Œåªè§é‚£è€å†›åœ¨é‡Œé¢å‘ç«ã€‚å·®æ‹¨è¯´é“ï¼šâ€œç®¡è¥å·®è¿™ä¸ªæ—å†²æ¥æ›¿ä½ å›å¤©ç‹å ‚çœ‹å®ˆï¼Œä½ å¯å³ä¾¿äº¤å‰²ã€‚â€è€å†›æ‹¿äº†é’¥åŒ™ï¼Œå¼•ç€æ—å†²ï¼Œåˆ†ä»˜é“ï¼šâ€œä»“å»’å†…è‡ªæœ‰å®˜å¸å°è®°ï¼Œè¿™å‡ å †è‰ä¸€å †å †éƒ½æœ‰æ•°ç›®ã€‚â€è€å†›éƒ½ç‚¹è§äº†å †æ•°ï¼Œåˆå¼•æ—å†²åˆ°è‰å…ä¸Šã€‚è€å†›æ”¶æ‹¾è¡Œæï¼Œä¸´äº†è¯´é“ï¼šâ€œç«ç›†ã€é”…å­ã€ç¢—ç¢Ÿï¼Œéƒ½å€Ÿä¸ä½ ã€‚â€æ—å†²é“ï¼šâ€œå¤©ç‹å ‚å†…æˆ‘ä¹Ÿæœ‰åœ¨é‚£é‡Œï¼Œä½ è¦ä¾¿æ‹¿äº†å»ã€‚â€è€å†›æŒ‡å£ä¸ŠæŒ‚ä¸€ä¸ªå¤§è‘«èŠ¦ï¼Œè¯´é“ï¼šâ€œä½ è‹¥ä¹°é…’åƒæ—¶ï¼Œåªå‡ºè‰åœºï¼ŒæŠ•ä¸œå¤§è·¯å»ä¸‰äºŒé‡Œï¼Œä¾¿æœ‰å¸‚äº•ã€‚â€è€å†›è‡ªå’Œå·®æ‹¨å›è¥é‡Œæ¥ã€‚<br/> åªè¯´æ—å†²å°±åºŠä¸Šæ”¾äº†åŒ…è£¹è¢«å§ï¼Œå°±åä¸‹ç”Ÿäº›ç„°ç«èµ·æ¥ã€‚å±‹è¾¹æœ‰ä¸€å †æŸ´ç‚­ï¼Œæ‹¿å‡ å—æ¥ç”Ÿåœ¨åœ°ç‚‰é‡Œã€‚ä»°é¢çœ‹é‚£è‰å±‹æ—¶ï¼Œå››ä¸‹é‡Œå´©åäº†ï¼Œåˆè¢«æœ”é£å¹æ’¼ï¼Œæ‘‡æŒ¯å¾—åŠ¨ã€‚æ—å†²é“ï¼šâ€œè¿™å±‹å¦‚ä½•è¿‡å¾—ä¸€å†¬ï¼Ÿå¾…é›ªæ™´äº†ï¼Œå»åŸä¸­å”¤ä¸ªæ³¥æ°´åŒ æ¥ä¿®ç†ã€‚â€å‘äº†ä¸€å›ç«ï¼Œè§‰å¾—èº«ä¸Šå¯’å†·ï¼Œå¯»æ€ï¼šâ€œå´æ‰è€å†›æ‰€è¯´äº”é‡Œè·¯å¤–æœ‰é‚£å¸‚äº•ï¼Œä½•ä¸å»æ²½äº›é…’æ¥åƒï¼Ÿâ€ä¾¿å»åŒ…é‡Œå–äº›ç¢é“¶å­ï¼ŒæŠŠèŠ±æªæŒ‘äº†é…’è‘«èŠ¦ï¼Œå°†ç«ç‚­ç›–äº†ï¼Œå–æ¯¡ç¬ å­æˆ´ä¸Šï¼Œæ‹¿äº†é’¥åŒ™ï¼Œå‡ºæ¥æŠŠè‰å…é—¨æ‹½ä¸Šã€‚å‡ºåˆ°å¤§é—¨é¦–ï¼ŒæŠŠä¸¤æ‰‡è‰åœºé—¨åæ‹½ä¸Šï¼Œé”äº†ã€‚å¸¦äº†é’¥åŒ™ï¼Œä¿¡æ­¥æŠ•ä¸œã€‚é›ªåœ°é‡Œè¸ç€ç¢ç¼ä¹±ç‰ï¼Œè¿¤é€¦èƒŒç€åŒ—é£è€Œè¡Œã€‚é‚£é›ªæ­£ä¸‹å¾—ç´§ã€‚<br/> è¡Œä¸ä¸ŠåŠé‡Œå¤šè·¯ï¼Œçœ‹è§ä¸€æ‰€å¤åº™ã€‚æ—å†²é¡¶ç¤¼é“ï¼šâ€œç¥æ˜åº‡ä½‘ï¼Œæ”¹æ—¥æ¥çƒ§é’±çº¸ã€‚â€åˆè¡Œäº†ä¸€å›ï¼Œæœ›è§ä¸€ç°‡äººå®¶ã€‚æ—å†²ä½è„šçœ‹æ—¶ï¼Œè§ç¯±ç¬†ä¸­æŒ‘ç€ä¸€ä¸ªè‰å¸šå„¿åœ¨éœ²å¤©é‡Œã€‚æ—å†²å¾„åˆ°åº—é‡Œï¼Œä¸»äººé“ï¼šâ€œå®¢äººé‚£é‡Œæ¥ï¼Ÿâ€æ—å†²é“ï¼šâ€œä½ è®¤å¾—è¿™ä¸ªè‘«èŠ¦ä¹ˆï¼Ÿâ€ä¸»äººçœ‹äº†é“ï¼šâ€œè¿™è‘«èŠ¦æ˜¯è‰æ–™åœºè€å†›çš„ã€‚â€æ—å†²é“ï¼šâ€œå¦‚ä½•ä¾¿è®¤çš„ï¼Ÿâ€åº—ä¸»é“ï¼šâ€œæ—¢æ˜¯è‰æ–™åœºçœ‹å®ˆå¤§å“¥ï¼Œä¸”è¯·å°‘åã€‚å¤©æ°”å¯’å†·ï¼Œä¸”é…Œä¸‰æ¯æƒå½“æ¥é£ã€‚â€åº—å®¶åˆ‡ä¸€ç›˜ç†Ÿç‰›è‚‰ï¼Œçƒ«ä¸€å£¶çƒ­é…’ï¼Œè¯·æ—å†²åƒã€‚åˆè‡ªä¹°äº†äº›ç‰›è‚‰ï¼Œåˆåƒäº†æ•°æ¯ã€‚å°±åˆä¹°äº†ä¸€è‘«èŠ¦é…’ï¼ŒåŒ…äº†é‚£ä¸¤å—ç‰›è‚‰ï¼Œç•™ä¸‹ç¢é“¶å­ï¼ŒæŠŠèŠ±æªæŒ‘äº†é…’è‘«èŠ¦ï¼Œæ€€å†…æ£äº†ç‰›è‚‰ï¼Œå«å£°ç›¸æ‰°ï¼Œä¾¿å‡ºç¯±ç¬†é—¨ï¼Œä¾æ—§è¿ç€æœ”é£å›æ¥ã€‚çœ‹é‚£é›ªï¼Œåˆ°æ™šè¶Šä¸‹çš„ç´§äº†ã€‚å¤æ—¶æœ‰ä¸ªä¹¦ç”Ÿï¼Œåšäº†ä¸€ä¸ªè¯ï¼Œå•é¢˜é‚£è´«è‹¦çš„æ¨é›ªï¼š<br/> å¹¿è«ä¸¥é£åˆ®åœ°ï¼Œè¿™é›ªå„¿ä¸‹çš„æ­£å¥½ã€‚æ‰¯çµ®æŒ¦ç»µï¼Œè£å‡ ç‰‡å¤§å¦‚æ ²æ ³ã€‚è§æ—é—´ç«¹å±‹èŒ…èŒ¨ï¼Œäº‰äº›å„¿è¢«ä»–å‹å€’ã€‚å¯Œå®¤è±ªå®¶ï¼Œå´è¨€é“å‹ç˜´çŠ¹å«Œå°‘ã€‚å‘çš„æ˜¯å…½ç‚­çº¢ç‚‰ï¼Œç©¿çš„æ˜¯ç»µè¡£çµ®è¢„ã€‚æ‰‹æ»æ¢…èŠ±ï¼Œå”±é“å›½å®¶ç¥¥ç‘ï¼Œä¸å¿µè´«æ°‘äº›å°ã€‚é«˜å§æœ‰å¹½äººï¼ŒåŸå’å¤šè¯—è‰ã€‚</p>\n</blockquote>\n<p><font color=\"purple\" size=\"4\">å¯¹ä¸­æ–‡æå–æ–‡æœ¬ç‰¹å¾ï¼Œéœ€è¦å®‰è£…å¹¶ä½¿ç”¨åˆ°<code>jieba</code>åº“ã€‚ä½¿ç”¨è¯¥åº“å°†æ–‡æœ¬å¤„ç†æˆä¸ºç©ºæ ¼è¿æ¥è¯è¯­çš„æ ¼å¼ï¼Œå†ä½¿ç”¨CountVectorizeræ–‡æœ¬ç‰¹å¾æå–æ¨¡å‹è¿›è¡Œæå–å³å¯ã€‚</font></p>\n<p><font color=\"brown\" size=\"4\">ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token keyword\">import</span> jieba\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> CountVectorizer\n\n\n<span class=\"token comment\"># å°†æ–‡æœ¬è½¬ä¸ºä»¥ç©ºæ ¼ç›¸è¿çš„å­—ç¬¦ä¸²</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">cut_word</span><span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>jieba<span class=\"token punctuation\">.</span>cut<span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\"># å°†æ–‡æœ¬ä»¥è¡Œä¸ºå•ä½ï¼Œå»é™¤ç©ºæ ¼ï¼Œå¹¶ç½®äºåˆ—è¡¨ä¸­ã€‚æ ¼å¼å½¢å¦‚ï¼š[\"ç¬¬ä¸€è¡Œ\",\"ç¬¬äºŒè¡Œ\",...\"n\"]</span>\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"./è®ºæ–‡.txt\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"r\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    data <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>line<span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> f<span class=\"token punctuation\">.</span>readlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\nlis <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># å°†æ¯ä¸€è¡Œçš„è¯æ±‡ä»¥ç©ºæ ¼è¿æ¥ </span>\n<span class=\"token keyword\">for</span> temp <span class=\"token keyword\">in</span> data<span class=\"token punctuation\">:</span>\n    lis<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>cut_word<span class=\"token punctuation\">(</span>temp<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\ntransfer <span class=\"token operator\">=</span> CountVectorizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ntrans_data <span class=\"token operator\">=</span> transfer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>lis<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>transfer<span class=\"token punctuation\">.</span>get_feature_names<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># è¾“å‡ºsparseæ•°ç»„</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>trans_data<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># è½¬ä¸ºndarrayæ•°ç»„ï¼ˆå¦‚æœéœ€è¦ï¼‰</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>trans_data<span class=\"token punctuation\">.</span>toarray<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n</code></pre>\n<hr/>\n<p><font color=\"brown\" size=\"4\">ç¨‹åºæ‰§è¡Œæ•ˆæœå¦‚ä¸‹ï¼š<br/> <img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"image\\8a9add79abf84097889f581b0e196b9e.png\"/></font></p>\n<hr/>\n<p><font color=\"brown\" size=\"4\">è½¬æ¢å¾—åˆ°çš„ndarrayæ•°ç»„å½¢å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰å¦‚å›¾æ‰€ç¤ºï¼š<br/> â€ƒâ€ƒ<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"image\\a2848943c68e468699a8fd5e15b252ce.png\"/></font></p>\n<hr/>\n<h1><a id=\"4_TFIDF__TfidfVectorizer_136\"></a>4. TF-IDF æ–‡æœ¬ç‰¹å¾æå– TfidfVectorizer()</h1>\n<p><font color=\"red\" size=\"4\">TF-IDFæ–‡æœ¬æå–å™¨å¯ä»¥ç”¨æ¥è¯„ä¼°ä¸€å­—è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†æˆ–è€…ä¸€ä¸ªè¯­æ–™åº“ä¸­çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚<br/> ä»£ç å±•ç¤ºå¦‚ä¸‹ï¼š</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> TfidfVectorizer\n<span class=\"token keyword\">import</span> jieba\n\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">cut_word</span><span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>jieba<span class=\"token punctuation\">.</span>cut<span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"data.txt\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"r\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    data <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>line<span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> f<span class=\"token punctuation\">.</span>readlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\nlis <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> temp <span class=\"token keyword\">in</span> data<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># print(cut_word(temp))</span>\n    lis<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>cut_word<span class=\"token punctuation\">(</span>temp<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\ntransfer <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>transfer<span class=\"token punctuation\">.</span>get_feature_names<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>trans_data<span class=\"token punctuation\">)</span>\n</code></pre>\n<p><font color=\"purple\" size=\"4\">ç¨‹åºæ‰§è¡Œç»“æœå¦‚ä¸‹ï¼š<br/> â€ƒ<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" src=\"image\\8c20fe3c10564d4982a329b9f035cf3f.png\"/></font></p>\n<hr/>\n<p><font color=\"red\" size=\"5\">æœ¬æ¬¡åˆ†äº«å°±åˆ°è¿™é‡Œï¼Œå°å•¾æ„Ÿè°¢æ‚¨çš„å…³æ³¨ä¸æ”¯æŒï¼<br/> ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿ğŸŒ¹ê§”ê¦¿</font></p>\n<blockquote>\n<p><font color=\"purple\" size=\"4\">æœ¬ä¸“æ æ›´å¤šå¥½æ–‡æ¬¢è¿ç‚¹å‡»ä¸‹æ–¹è¿æ¥ï¼š<br/> â€ƒ<br/> 1.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/122967282\">åˆè¯†æœºå™¨å­¦ä¹ å‰å¯¼å†…å®¹_ä½ éœ€è¦çŸ¥é“çš„åŸºæœ¬æ¦‚å¿µç½—åˆ—_ä»¥PYä¸ºå·¥å…· ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆä¸€ï¼‰ã€‘</a><br/> â€ƒ<br/> 2.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/122921512\">sklearnåº“æ•°æ®æ ‡å‡†é¢„å¤„ç†åˆé›†_ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆäºŒï¼‰ã€‘</a><br/> â€ƒ<br/> 3.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/122896585\">K_è¿‘é‚»ç®—æ³•_åˆ†ç±»Ionosphereç”µç¦»å±‚æ•°æ®ã€pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆä¸‰ï¼‰ã€‘</a><br/> â€ƒ<br/> 4.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126194659\">pythonæœºå™¨å­¦ä¹  ä¸€å…ƒçº¿æ€§å›å½’ æ¢¯åº¦ä¸‹é™æ³•çš„å®ç° ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆå››ï¼‰ã€‘</a><br/> â€ƒ<br/> 5.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126217014\">sklearnå®ç°ä¸€å…ƒçº¿æ€§å›å½’ ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆäº”ï¼‰ã€‘</a><br/> â€ƒ<br/> 6.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126217379\">å¤šå…ƒçº¿æ€§å›å½’_æ¢¯åº¦ä¸‹é™æ³•å®ç°ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆå…­ï¼‰ã€‘</a><br/> â€ƒ<br/> 7.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126221430\">sklearnå®ç°å¤šå…ƒçº¿æ€§å›å½’ ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆä¸ƒï¼‰ã€‘</a><br/> â€ƒ<br/> 8.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126222929\">sklearnå®ç°å¤šé¡¹å¼çº¿æ€§å›å½’_ä¸€å…ƒ/å¤šå…ƒ ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆå…«ï¼‰ã€‘</a><br/> â€ƒ<br/> 9.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126229040?spm=1001.2014.3001.5502\">é€»è¾‘å›å½’åŸç†æ¢³ç†_ä»¥pythonä¸ºå·¥å…· ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆä¹ï¼‰ã€‘</a><br/> â€ƒ<br/> 10.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126227136\">sklearnå®ç°é€»è¾‘å›å½’_ä»¥pythonä¸ºå·¥å…·ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆåï¼‰ã€‘</a><br/> â€ƒ<br/> 11.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126329971\">å†³ç­–æ ‘ä¸“é¢˜_ä»¥pythonä¸ºå·¥å…·ã€Pythonæœºå™¨å­¦ä¹ ç³»åˆ—ï¼ˆåä¸€ï¼‰ã€‘</a></font></p>\n</blockquote>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>"}