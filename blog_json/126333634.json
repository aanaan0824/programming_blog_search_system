{"blogid": "126333634", "writerAge": "码龄2年", "writerBlogNum": "180", "writerCollect": "4030", "writerComment": "2963", "writerFan": "23586", "writerGrade": "7级", "writerIntegral": "11032", "writerName": "侯小啾", "writerProfileAdress": "writer_image\\profile_126333634.jpg", "writerRankTotal": "1009", "writerRankWeekly": "23", "writerThumb": "3361", "writerVisitNum": "332879", "blog_read_count": "1125", "blog_time": "已于 2022-09-02 22:28:50 修改", "blog_title": "文本特征提取专题_以python为工具【Python机器学习系列（十二）】", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-kimbie-light\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<p></p>\n<div class=\"toc\">\n<h3>文章目录</h3>\n<ul><li><a href=\"#1_DictVectorizer_13\">1.字典文本特征提取 DictVectorizer()</a></li><li><ul><li><a href=\"#11_onehot_14\">1.1 one-hot编码</a></li><li><a href=\"#12_sparse_36\">1.2 字典数据转sparse矩阵</a></li></ul>\n</li><li><a href=\"#2_51\">2.英文文本特征提取</a></li><li><a href=\"#3_89\">3.中文文本特征提取</a></li><li><a href=\"#4_TFIDF__TfidfVectorizer_136\">4. TF-IDF 文本特征提取 TfidfVectorizer()</a></li></ul>\n</div>\n<p></p>\n<hr/>\n<p><font size=\"5\">      ʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞ</font><br/>                  <img alt=\"在这里插入图片描述\" src=\"image\\7cc703968fc94dd7a64c66270f4a6193.png\"/><img alt=\"请添加图片描述\" src=\"https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif\"/><img alt=\"请添加图片描述\" src=\"https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif\"/><img alt=\"请添加图片描述\" src=\"https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif\"/><img alt=\"在这里插入图片描述\" src=\"image\\f9d8225c11fe42508a5bd938f8d99e24.png\"/><br/> <font size=\"5\">    ʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞ</font></p>\n<hr/>\n<p><font color=\"purple\" size=\"4\">大家好，我是侯小啾！</font><img alt=\"在这里插入图片描述\" src=\"image\\3ed5e82a513a4a41b019a3b3d747ef7f.png\"/></p>\n<p><font color=\"brown\" size=\"4\"> <img alt=\"在这里插入图片描述\" src=\"image\\4e3c21f29fd0442a8bf8b90dcf152434.png\"/>今天分享的话题是，文本特征的提取。具体内容请见下文：</font></p>\n<hr/>\n<h1><a id=\"1_DictVectorizer_13\"></a>1.字典文本特征提取 DictVectorizer()</h1>\n<h2><a id=\"11_onehot_14\"></a>1.1 one-hot编码</h2>\n<p><font color=\"purple\" size=\"4\">创建一个字典，观察如下数据形式的变化：</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction <span class=\"token keyword\">import</span> DictVectorizer\n\n\ndata <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">{<!-- --></span><span class=\"token string\">'city'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'洛阳'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'temperature'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">39</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">{<!-- --></span><span class=\"token string\">'city'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'成都'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'temperature'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">41</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">{<!-- --></span><span class=\"token string\">'city'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'宁波'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'temperature'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">42</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">{<!-- --></span><span class=\"token string\">'city'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'佛山'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'temperature'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">38</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span>\n\ndf1 <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>df1<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># one-hot编码 因为temperature是数值型的，所以会保留原始值，只有字符串类型的才会生成虚拟变量</span>\ndf2 <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>get_dummies<span class=\"token punctuation\">(</span>df1<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>df2<span class=\"token punctuation\">)</span>\n</code></pre>\n<p><font color=\"purple\" size=\"4\">输出如下：<br/>       <img alt=\"在这里插入图片描述\" src=\"image\\2606ffc3283e41e4afdcc2729b2d6742.png\"/></font></p>\n<hr/>\n<h2><a id=\"12_sparse_36\"></a>1.2 字典数据转sparse矩阵</h2>\n<p><font color=\"brown\" size=\"4\">使用DictVectorizer()创建字典特征提取模型</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token comment\"># 1.创建对象  默认sparse=True 返回的是sparse矩阵；  sparse=False  返回的是ndarray矩阵</span>\ntransfer <span class=\"token operator\">=</span> DictVectorizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 2.转化数据并训练</span>\ntrans_data <span class=\"token operator\">=</span> transfer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>transfer<span class=\"token punctuation\">.</span>get_feature_names_out<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> \n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>trans_data<span class=\"token punctuation\">)</span>\n</code></pre>\n<p>       <img alt=\"在这里插入图片描述\" src=\"image\\54722bb1cefa42949e619201346b3e3e.png\"/><br/>  <br/> <font color=\"brown\" size=\"4\">使用sparse矩阵没有显示0数据，节约了内存，更为简洁，这一点比ndarray矩阵更好。</font></p>\n<hr/>\n<h1><a id=\"2_51\"></a>2.英文文本特征提取</h1>\n<p><font color=\"brown\" size=\"4\">文本特征提取使用的是CountVectorizer文本特征提取模型，这里准备了一段英文文本（I have a dream）。统计词频并得到sparse矩阵，代码如下所示：<br/>  <br/> <font color=\"purple\" size=\"4\">CountVectorizer()没有sparse参数，默认采用sparse矩阵格式。且可以通过stop_words指定停用词。</font></font></p>\n<pre><code class=\"prism language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> CountVectorizer\n\n\ndata <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"I have a dream that one day this nation will rise up and live out the true meaning of its creed\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"We hold these truths to be self-evident, that all men are created equal\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"I have a dream that one day on the red hills of Georgia, \"</span>\n        <span class=\"token string\">\"the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"I have a dream that one day even the state of Mississippi\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\" a state sweltering with the heat of injustice\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"sweltering with the heat of oppression\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"will be transformed into an oasis of freedom and justice\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"I have a dream today\"</span><span class=\"token punctuation\">]</span>\n\n\n<span class=\"token comment\"># CountVectorizer文本特征提取模型</span>\n\n<span class=\"token comment\"># 1.实例化  将\"is\"标记为停用词</span>\nc_transfer <span class=\"token operator\">=</span> CountVectorizer<span class=\"token punctuation\">(</span>stop_words<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"is\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2.调用fit_transform</span>\nc_trans_data <span class=\"token operator\">=</span> c_transfer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\"># 打印特征名称</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>c_transfer<span class=\"token punctuation\">.</span>get_feature_names_out<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 打印sparse矩阵</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>c_trans_data<span class=\"token punctuation\">)</span>\n</code></pre>\n<p><font color=\"brown\" size=\"4\">输出结果如下图所示：<br/>     <img alt=\"在这里插入图片描述\" src=\"image\\a11093bd4a5a43b0a67bfc3dbafd00df.png\"/></font></p>\n<hr/>\n<h1><a id=\"3_89\"></a>3.中文文本特征提取</h1>\n<p><font color=\"brown\" size=\"4\">准备一段中文文本（data.txt），以水浒传中风雪山神庙情节为例：</font></p>\n<blockquote>\n<p>大雪下的正紧，林冲和差拨两个在路上又没买酒吃处。早来到草料场外，看时，一周遭有些黄土墙，两扇大门。推开看里面时，七八间草房做着仓廒，四下里都是马草堆，中间两座草厅。到那厅里，只见那老军在里面向火。差拨说道：“管营差这个林冲来替你回天王堂看守，你可即便交割。”老军拿了钥匙，引着林冲，分付道：“仓廒内自有官司封记，这几堆草一堆堆都有数目。”老军都点见了堆数，又引林冲到草厅上。老军收拾行李，临了说道：“火盆、锅子、碗碟，都借与你。”林冲道：“天王堂内我也有在那里，你要便拿了去。”老军指壁上挂一个大葫芦，说道：“你若买酒吃时，只出草场，投东大路去三二里，便有市井。”老军自和差拨回营里来。<br/> 只说林冲就床上放了包裹被卧，就坐下生些焰火起来。屋边有一堆柴炭，拿几块来生在地炉里。仰面看那草屋时，四下里崩坏了，又被朔风吹撼，摇振得动。林冲道：“这屋如何过得一冬？待雪晴了，去城中唤个泥水匠来修理。”向了一回火，觉得身上寒冷，寻思：“却才老军所说五里路外有那市井，何不去沽些酒来吃？”便去包里取些碎银子，把花枪挑了酒葫芦，将火炭盖了，取毡笠子戴上，拿了钥匙，出来把草厅门拽上。出到大门首，把两扇草场门反拽上，锁了。带了钥匙，信步投东。雪地里踏着碎琼乱玉，迤逦背着北风而行。那雪正下得紧。<br/> 行不上半里多路，看见一所古庙。林冲顶礼道：“神明庇佑，改日来烧钱纸。”又行了一回，望见一簇人家。林冲住脚看时，见篱笆中挑着一个草帚儿在露天里。林冲径到店里，主人道：“客人那里来？”林冲道：“你认得这个葫芦么？”主人看了道：“这葫芦是草料场老军的。”林冲道：“如何便认的？”店主道：“既是草料场看守大哥，且请少坐。天气寒冷，且酌三杯权当接风。”店家切一盘熟牛肉，烫一壶热酒，请林冲吃。又自买了些牛肉，又吃了数杯。就又买了一葫芦酒，包了那两块牛肉，留下碎银子，把花枪挑了酒葫芦，怀内揣了牛肉，叫声相扰，便出篱笆门，依旧迎着朔风回来。看那雪，到晚越下的紧了。古时有个书生，做了一个词，单题那贫苦的恨雪：<br/> 广莫严风刮地，这雪儿下的正好。扯絮挦绵，裁几片大如栲栳。见林间竹屋茅茨，争些儿被他压倒。富室豪家，却言道压瘴犹嫌少。向的是兽炭红炉，穿的是绵衣絮袄。手捻梅花，唱道国家祥瑞，不念贫民些小。高卧有幽人，吟咏多诗草。</p>\n</blockquote>\n<p><font color=\"purple\" size=\"4\">对中文提取文本特征，需要安装并使用到<code>jieba</code>库。使用该库将文本处理成为空格连接词语的格式，再使用CountVectorizer文本特征提取模型进行提取即可。</font></p>\n<p><font color=\"brown\" size=\"4\">代码示例如下：</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token keyword\">import</span> jieba\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> CountVectorizer\n\n\n<span class=\"token comment\"># 将文本转为以空格相连的字符串</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">cut_word</span><span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>jieba<span class=\"token punctuation\">.</span>cut<span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\"># 将文本以行为单位，去除空格，并置于列表中。格式形如：[\"第一行\",\"第二行\",...\"n\"]</span>\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"./论文.txt\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"r\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    data <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>line<span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> f<span class=\"token punctuation\">.</span>readlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\nlis <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># 将每一行的词汇以空格连接 </span>\n<span class=\"token keyword\">for</span> temp <span class=\"token keyword\">in</span> data<span class=\"token punctuation\">:</span>\n    lis<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>cut_word<span class=\"token punctuation\">(</span>temp<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\ntransfer <span class=\"token operator\">=</span> CountVectorizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ntrans_data <span class=\"token operator\">=</span> transfer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>lis<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>transfer<span class=\"token punctuation\">.</span>get_feature_names<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 输出sparse数组</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>trans_data<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 转为ndarray数组（如果需要）</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>trans_data<span class=\"token punctuation\">.</span>toarray<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n</code></pre>\n<hr/>\n<p><font color=\"brown\" size=\"4\">程序执行效果如下：<br/> <img alt=\"在这里插入图片描述\" src=\"image\\8a9add79abf84097889f581b0e196b9e.png\"/></font></p>\n<hr/>\n<p><font color=\"brown\" size=\"4\">转换得到的ndarray数组形式（如果需要）如图所示：<br/>   <img alt=\"在这里插入图片描述\" src=\"image\\a2848943c68e468699a8fd5e15b252ce.png\"/></font></p>\n<hr/>\n<h1><a id=\"4_TFIDF__TfidfVectorizer_136\"></a>4. TF-IDF 文本特征提取 TfidfVectorizer()</h1>\n<p><font color=\"red\" size=\"4\">TF-IDF文本提取器可以用来评估一字词对于一个文件集或者一个语料库中的其中一份文件的重要程度。<br/> 代码展示如下：</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> TfidfVectorizer\n<span class=\"token keyword\">import</span> jieba\n\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">cut_word</span><span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>jieba<span class=\"token punctuation\">.</span>cut<span class=\"token punctuation\">(</span>sent<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"data.txt\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"r\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    data <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>line<span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> f<span class=\"token punctuation\">.</span>readlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\nlis <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> temp <span class=\"token keyword\">in</span> data<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># print(cut_word(temp))</span>\n    lis<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>cut_word<span class=\"token punctuation\">(</span>temp<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\ntransfer <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>transfer<span class=\"token punctuation\">.</span>get_feature_names<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>trans_data<span class=\"token punctuation\">)</span>\n</code></pre>\n<p><font color=\"purple\" size=\"4\">程序执行结果如下：<br/>  <img alt=\"在这里插入图片描述\" src=\"image\\8c20fe3c10564d4982a329b9f035cf3f.png\"/></font></p>\n<hr/>\n<p><font color=\"red\" size=\"5\">本次分享就到这里，小啾感谢您的关注与支持！<br/> 🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ</font></p>\n<blockquote>\n<p><font color=\"purple\" size=\"4\">本专栏更多好文欢迎点击下方连接：<br/>  <br/> 1.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/122967282\">初识机器学习前导内容_你需要知道的基本概念罗列_以PY为工具 【Python机器学习系列（一）】</a><br/>  <br/> 2.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/122921512\">sklearn库数据标准预处理合集_【Python机器学习系列（二）】</a><br/>  <br/> 3.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/122896585\">K_近邻算法_分类Ionosphere电离层数据【python机器学习系列（三）】</a><br/>  <br/> 4.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126194659\">python机器学习 一元线性回归 梯度下降法的实现 【Python机器学习系列（四）】</a><br/>  <br/> 5.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126217014\">sklearn实现一元线性回归 【Python机器学习系列（五）】</a><br/>  <br/> 6.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126217379\">多元线性回归_梯度下降法实现【Python机器学习系列（六）】</a><br/>  <br/> 7.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126221430\">sklearn实现多元线性回归 【Python机器学习系列（七）】</a><br/>  <br/> 8.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126222929\">sklearn实现多项式线性回归_一元/多元 【Python机器学习系列（八）】</a><br/>  <br/> 9.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126229040?spm=1001.2014.3001.5502\">逻辑回归原理梳理_以python为工具 【Python机器学习系列（九）】</a><br/>  <br/> 10.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126227136\">sklearn实现逻辑回归_以python为工具【Python机器学习系列（十）】</a><br/>  <br/> 11.<a href=\"https://skylarkprogramming.blog.csdn.net/article/details/126329971\">决策树专题_以python为工具【Python机器学习系列（十一）】</a></font></p>\n</blockquote>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>"}