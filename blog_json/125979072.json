{"blogid": "125979072", "writerAge": "码龄7年", "writerBlogNum": "7", "writerCollect": "391", "writerComment": "245", "writerFan": "9065", "writerGrade": "3级", "writerIntegral": "485", "writerName": "爱晚乏客游", "writerProfileAdress": "writer_image\\profile_125979072.jpg", "writerRankTotal": "28618", "writerRankWeekly": "752", "writerThumb": "68", "writerVisitNum": "59932", "blog_read_count": "3201", "blog_time": "已于 2022-08-05 13:32:31 修改", "blog_title": "2022.07.25 C++下使用opencv部署yolov7模型（五）", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<h1>0.写在最前</h1>\n<p>此篇文字针对yolov7-1.0版本。</p>\n<p><img alt=\"\" height=\"891\" src=\"image\\7cdcbc7f94d9405cb171cffd3accf33c.png\" width=\"1200\"/></p>\n<p></p>\n<p>最近粗略的看了一遍yolov7的论文，关于yolov7和其他yolo系列的对比，咱就不多说了，大佬们的文章很多很详细。关于opencv部署方面，其实yolov7和yolov5的初期版本（5.0以前的版本）很像，分为三个输出口，yolov5-6.0之后的版本合并了三个输出口变成一个output输出【需要注意的是，<span style=\"color:#fe2c24;\">虽然</span>yolov可以在export的时候加上--grid参数将detect层加入之后变成和yolov5最新版本的输出一致（可以不用改yolov5代码直接跑yolov7的那种一致,当然，anchors数据还是得改的），<strong><span style=\"color:#fe2c24;\">但是</span></strong>我试过了，opencv包括onnxruntime推理加grid参数的onnx模型都有问题，暂时我也在探索一种适用于所有yolov7版本的修改方案，但是改了几种都是适用某几个模型，其他模型挂掉的情况】。使用<a href=\"https://netron.app/\" title=\"Netron\">Netron</a>打开两个模型对比下很明显，数据格式也和yolo的一致。所以基本上可以和yolov5的代码通用。只不过具体使用的时候还是有一点区别的。另外，yolov7目前可以直接通过其自身带的export.py导出onnx模型，并不需要像yolov5早期的代码修改。</p>\n<p><img alt=\"\" height=\"599\" src=\"image\\c167b571128d457cb23e9d8a6b4ae048.png\" width=\"994\"/></p>\n<h1>一.yolov5代码修改适用yolov7</h1>\n<p><img alt=\"\" height=\"876\" src=\"image\\22d2c27179be440b90779858675b8051.png\" width=\"971\"/></p>\n<h2>1.归一化框的读取类似yolov5的早期版本</h2>\n<p>上面说过，yolov7和yolov5的不同，实际上应该是一致的才对（实际上，如果yolov7导出的时候加上--grid参数，结果就和yolov5目前的版本一毛一样，但是加上之后opencv推理onnx的时候会报错，目前yolov7暂时未修复该bug，所以下面的yolov7代码导出的时候不要加--grid参数）。我没仔细debug，所以我们需要根据下面的红色框中的内容对网络的归一化anchors框进行变换变成正常的像素位置。也就是像yolov5之前古老的版本没优化之前一样（这就是我上面说的和yolov5-5.0以前的版本类似的原因）。可以看第三篇的代码中的读取归一化框的方式获取原始图像位置。<a href=\"https://blog.csdn.net/qq_34124780/article/details/116464727\" title=\"2021.09.02更新说明 c++下使用opencv部署yolov5模型 （三）_爱晚乏客游的博客-CSDN博客_c++ yolov5\">2021.09.02更新说明 c++下使用opencv部署yolov5模型 （三）_爱晚乏客游的博客-CSDN博客_c++ yolov5</a></p>\n<p><img alt=\"\" height=\"812\" src=\"image\\365abafe75c640e2843e8583a873e05f.png\" width=\"1005\"/></p>\n<h2>2.anchors数据不同</h2>\n<p><img alt=\"\" height=\"219\" src=\"image\\cb3553b21ebc461b8d7d8e20b29e99d6.png\" width=\"1032\"/></p>\n<p> 对比下两者的anchors数据，可以看到两个的anchors不一致了，修改这部分内容即可。</p>\n<p>所以综上所诉，对于yolov5-6.0的代码，修改一些地方即可马上应用到yolov7上面，可以说很方便了。</p>\n<p>具体修改有两处，一处是anchors，另外一处是推理程序，修改之后的链接我放最下面了，其实就是在第四篇的基础上面修改下：<a href=\"https://github.com/UNeedCryDear/yolov5-opencv-dnn-cpp\" title=\"GitHub - UNeedCryDear/yolov5-opencv-dnn-cpp: 使用opencv模块部署yolov5-6.0版本 \">GitHub - UNeedCryDear/yolov5-opencv-dnn-cpp: 使用opencv模块部署yolov5-6.0版本 </a></p>\n<pre><code>//yolo.h中改下anchors\nconst float netAnchors[3][6] = { {12, 16, 19, 36, 40, 28},{36, 75, 76, 55, 72, 146},{142, 110, 192, 243, 459, 401} }; //yolov7-P5 anchors\n\n//yolo.cpp中推理代码修改\nbool Yolo::Detect(Mat&amp; SrcImg, Net&amp; net, vector&lt;Output&gt;&amp; output) {\n\tMat blob;\n\tint col = SrcImg.cols;\n\tint row = SrcImg.rows;\n\tint maxLen = MAX(col, row);\n\tMat netInputImg = SrcImg.clone();\n\tif (maxLen &gt; 1.2 * col || maxLen &gt; 1.2 * row) {\n\t\tMat resizeImg = Mat::zeros(maxLen, maxLen, CV_8UC3);\n\t\tSrcImg.copyTo(resizeImg(Rect(0, 0, col, row)));\n\t\tnetInputImg = resizeImg;\n\t}\n\tvector&lt;Ptr&lt;Layer&gt; &gt; layer;\n\tvector&lt;string&gt; layer_names;\n\tlayer_names= net.getLayerNames();\n\tblobFromImage(netInputImg, blob, 1 / 255.0, cv::Size(netWidth, netHeight), cv::Scalar(0, 0, 0), true, false);\n\t//如果在其他设置没有问题的情况下但是结果偏差很大，可以尝试下用下面两句语句\n\t//blobFromImage(netInputImg, blob, 1 / 255.0, cv::Size(netWidth, netHeight), cv::Scalar(104, 117, 123), true, false);\n\t//blobFromImage(netInputImg, blob, 1 / 255.0, cv::Size(netWidth, netHeight), cv::Scalar(114, 114,114), true, false);\n\tnet.setInput(blob);\n\tstd::vector&lt;cv::Mat&gt; netOutputImg;\n\tnet.forward(netOutputImg, net.getUnconnectedOutLayersNames());\n\tstd::vector&lt;int&gt; classIds;//结果id数组\n\tstd::vector&lt;float&gt; confidences;//结果每个id对应置信度数组\n\tstd::vector&lt;cv::Rect&gt; boxes;//每个id矩形框\n\tfloat ratio_h = (float)netInputImg.rows / netHeight;\n\tfloat ratio_w = (float)netInputImg.cols / netWidth;\n\tint net_width = className.size() + 5;  //输出的网络宽度是类别数+5\n\tfor (int stride = 0; stride &lt; strideSize; stride++) {    //stride\n\t\tfloat* pdata = (float*)netOutputImg[stride].data;\n\t\tint grid_x = (int)(netWidth / netStride[stride]);\n\t\tint grid_y = (int)(netHeight / netStride[stride]);\n\t\tfor (int anchor = 0; anchor &lt; 3; anchor++) {\t//anchors\n\t\t\tconst float anchor_w = netAnchors[stride][anchor * 2];\n\t\t\tconst float anchor_h = netAnchors[stride][anchor * 2 + 1];\n\t\t\tfor (int i = 0; i &lt; grid_y; i++) {\n\t\t\t\tfor (int j = 0; j &lt; grid_x; j++) {\n\t\t\t\t\tfloat box_score = sigmoid_x(pdata[4]); ;//获取每一行的box框中含有某个物体的概率\n\t\t\t\t\tif (box_score &gt;= boxThreshold) {\n\t\t\t\t\t\tcv::Mat scores(1, className.size(), CV_32FC1, pdata + 5);\n\t\t\t\t\t\tPoint classIdPoint;\n\t\t\t\t\t\tdouble max_class_socre;\n\t\t\t\t\t\tminMaxLoc(scores, 0, &amp;max_class_socre, 0, &amp;classIdPoint);\n\t\t\t\t\t\tmax_class_socre = sigmoid_x(max_class_socre);\n\t\t\t\t\t\tif (max_class_socre &gt;= classThreshold) {\n\t\t\t\t\t\t\tfloat x = (sigmoid_x(pdata[0]) * 2.f - 0.5f + j) * netStride[stride];  //x\n\t\t\t\t\t\t\tfloat y = (sigmoid_x(pdata[1]) * 2.f - 0.5f + i) * netStride[stride];   //y\n\t\t\t\t\t\t\tfloat w = powf(sigmoid_x(pdata[2]) * 2.f, 2.f) * anchor_w;   //w\n\t\t\t\t\t\t\tfloat h = powf(sigmoid_x(pdata[3]) * 2.f, 2.f) * anchor_h;  //h\n\t\t\t\t\t\t\tint left = (int)(x - 0.5 * w) * ratio_w + 0.5;\n\t\t\t\t\t\t\tint top = (int)(y - 0.5 * h) * ratio_h + 0.5;\n\t\t\t\t\t\t\tclassIds.push_back(classIdPoint.x);\n\t\t\t\t\t\t\tconfidences.push_back(max_class_socre * box_score);\n\t\t\t\t\t\t\tboxes.push_back(Rect(left, top, int(w * ratio_w), int(h * ratio_h)));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpdata += net_width;//下一行\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t//执行非最大抑制以消除具有较低置信度的冗余重叠框（NMS）\n\tvector&lt;int&gt; nms_result;\n\tNMSBoxes(boxes, confidences, nmsScoreThreshold, nmsThreshold, nms_result);\n\tfor (int i = 0; i &lt; nms_result.size(); i++) {\n\t\tint idx = nms_result[i];\n\t\tOutput result;\n\t\tresult.id = classIds[idx];\n\t\tresult.confidence = confidences[idx];\n\t\tresult.box = boxes[idx];\n\t\toutput.push_back(result);\n\t}\n\tif (output.size())\n\t\treturn true;\n\telse\n\t\treturn false;\n}</code></pre>\n<p>最后贴个yolov7和yolov5的对比图，可以看到yolov7提升还是蛮明显的，结果的置信度高了一些，后面的自行车也检测出来了，就是领带这里误检了。 </p>\n<p> <img alt=\"\" height=\"912\" src=\"image\\dfbad7f6d00342368587a9639c6bdfa2.png\" width=\"1200\"/></p>\n<p>贴合github链接，里面包括了yolov7和yolov5，通过宏定义来控制：</p>\n<p><a href=\"https://github.com/UNeedCryDear/yolov7-opencv-dnn-cpp\" title=\"GitHub - UNeedCryDear/yolov7-opencv-dnn-cpp\">GitHub - UNeedCryDear/yolov7-opencv-dnn-cpp</a></p>\n<p></p>\n<p></p>\n</div>\n</div>"}