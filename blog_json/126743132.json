{"blogid": "126743132", "writerAge": "码龄39天", "writerBlogNum": "169", "writerCollect": "3", "writerComment": "1", "writerFan": "70", "writerGrade": "5级", "writerIntegral": "1860", "writerName": "幸福的小肥熊", "writerProfileAdress": "writer_image\\profile_126743132.jpg", "writerRankTotal": "14133", "writerRankWeekly": "9012", "writerThumb": "0", "writerVisitNum": "16330", "blog_read_count": "12", "blog_time": "于 2022-09-07 12:22:20 发布", "blog_title": "评估神经网络的计算模型,神经网络是参数模型吗", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<h2>神经网络BP模型</h2>\n<p>一、BP模型概述误差逆传播(ErrorBack-Propagation)神经网络模型简称为BP(Back-Propagation)网络模型。</p>\n<p>PallWerbas博士于1974年在他的博士论文中提出了误差逆传播学习算法。完整提出并被广泛接受误差逆传播学习算法的是以Rumelhart和McCelland为首的科学家小组。</p>\n<p>他们在1986年出版“ParallelDistributedProcessing，ExplorationsintheMicrostructureofCognition”(《并行分布信息处理》)一书中，对误差逆传播学习算法进行了详尽的分析与介绍，并对这一算法的潜在能力进行了深入探讨。</p>\n<p>BP网络是一种具有3层或3层以上的阶层型神经网络。上、下层之间各神经元实现全连接，即下层的每一个神经元与上层的每一个神经元都实现权连接，而每一层各神经元之间无连接。</p>\n<p>网络按有教师示教的方式进行学习，当一对学习模式提供给网络后，神经元的激活值从输入层经各隐含层向输出层传播，在输出层的各神经元获得网络的输入响应。</p>\n<p>在这之后，按减小期望输出与实际输出的误差的方向，从输入层经各隐含层逐层修正各连接权，最后回到输入层，故得名“误差逆传播学习算法”。</p>\n<p>随着这种误差逆传播修正的不断进行，网络对输入模式响应的正确率也不断提高。</p>\n<p>BP网络主要应用于以下几个方面：1)函数逼近：用输入模式与相应的期望输出模式学习一个网络逼近一个函数；2)模式识别：用一个特定的期望输出模式将它与输入模式联系起来；3)分类：把输入模式以所定义的合适方式进行分类；4)数据压缩：减少输出矢量的维数以便于传输或存储。</p>\n<p>在人工神经网络的实际应用中，80%～90%的人工神经网络模型采用BP网络或它的变化形式，它也是前向网络的核心部分，体现了人工神经网络最精华的部分。</p>\n<p>二、BP模型原理下面以三层BP网络为例，说明学习和应用的原理。</p>\n<p>1.数据定义P对学习模式(xp，dp)，p=1，2，…，P；输入模式矩阵X[N][P]=(x1，x2，…，xP)；目标模式矩阵d[M][P]=(d1，d2，…，dP)。</p>\n<p>三层BP网络结构输入层神经元节点数S0=N，i=1，2，…，S0；隐含层神经元节点数S1，j=1，2，…，S1；神经元激活函数f1[S1]；权值矩阵W1[S1][S0]；偏差向量b1[S1]。</p>\n<p>输出层神经元节点数S2=M，k=1，2，…，S2；神经元激活函数f2[S2]；权值矩阵W2[S2][S1]；偏差向量b2[S2]。</p>\n<p>学习参数目标误差ϵ；初始权更新值Δ0；最大权更新值Δmax；权更新值增大倍数η+；权更新值减小倍数η-。</p>\n<p>2.误差函数定义对第p个输入模式的误差的计算公式为中国矿产资源评价新技术与评价新模型y2kp为BP网的计算输出。</p>\n<p>3.BP网络学习公式推导BP网络学习公式推导的指导思想是，对网络的权值W、偏差b修正，使误差函数沿负梯度方向下降，直到网络输出误差精度达到目标精度要求，学习结束。</p>\n<p>各层输出计算公式输入层y0i=xi，i=1，2，…，S0；隐含层中国矿产资源评价新技术与评价新模型y1j=f1(z1j)，j=1，2，…，S1；输出层中国矿产资源评价新技术与评价新模型y2k=f2(z2k)，k=1，2，…，S2。</p>\n<p>输出节点的误差公式中国矿产资源评价新技术与评价新模型对输出层节点的梯度公式推导中国矿产资源评价新技术与评价新模型E是多个y2m的函数，但只有一个y2k与wkj有关，各y2m间相互独立。</p>\n<p>其中中国矿产资源评价新技术与评价新模型则中国矿产资源评价新技术与评价新模型设输出层节点误差为δ2k=(dk-y2k)·f2′(z2k)，则中国矿产资源评价新技术与评价新模型同理可得中国矿产资源评价新技术与评价新模型对隐含层节点的梯度公式推导中国矿产资源评价新技术与评价新模型E是多个y2k的函数，针对某一个w1ji，对应一个y1j，它与所有的y2k有关。</p>\n<p>因此，上式只存在对k的求和，其中中国矿产资源评价新技术与评价新模型则中国矿产资源评价新技术与评价新模型设隐含层节点误差为中国矿产资源评价新技术与评价新模型则中国矿产资源评价新技术与评价新模型同理可得中国矿产资源评价新技术与评价新模型4.采用弹性BP算法(RPROP)计算权值W、偏差b的修正值ΔW，Δb1993年德国MartinRiedmiller和HeinrichBraun在他们的论文“ADirectAdaptiveMethodforFasterBackpropagationLearning：TheRPROPAlgorithm”中，提出ResilientBackpropagation算法——弹性BP算法(RPROP)。</p>\n<p>这种方法试图消除梯度的大小对权步的有害影响，因此，只有梯度的符号被认为表示权更新的方向。</p>\n<p>权改变的大小仅仅由权专门的“更新值”确定中国矿产资源评价新技术与评价新模型其中表示在模式集的所有模式(批学习)上求和的梯度信息，(t)表示t时刻或第t次学习。</p>\n<p>权更新遵循规则：如果导数是正(增加误差)，这个权由它的更新值减少。如果导数是负，更新值增加。中国矿产资源评价新技术与评价新模型RPROP算法是根据局部梯度信息实现权步的直接修改。</p>\n<p>对于每个权，我们引入它的各自的更新值，它独自确定权更新值的大小。</p>\n<p>这是基于符号相关的自适应过程，它基于在误差函数E上的局部梯度信息，按照以下的学习规则更新中国矿产资源评价新技术与评价新模型其中0＜η-＜1＜η+。</p>\n<p>在每个时刻，如果目标函数的梯度改变它的符号，它表示最后的更新太大，更新值应由权更新值减小倍数因子η-得到减少；如果目标函数的梯度保持它的符号，更新值应由权更新值增大倍数因子η+得到增大。</p>\n<p>为了减少自由地可调参数的数目，增大倍数因子η+和减小倍数因子η–被设置到固定值η+=1.2，η-=0.5，这两个值在大量的实践中得到了很好的效果。</p>\n<p>RPROP算法采用了两个参数：初始权更新值Δ0和最大权更新值Δmax当学习开始时，所有的更新值被设置为初始值Δ0，因为它直接确定了前面权步的大小，它应该按照权自身的初值进行选择，例如，Δ0=0.1(默认设置)。</p>\n<p>为了使权不至于变得太大，设置最大权更新值限制Δmax，默认上界设置为Δmax=50.0。在很多实验中，发现通过设置最大权更新值Δmax到相当小的值，例如Δmax=1.0。</p>\n<p>我们可能达到误差减小的平滑性能。5.计算修正权值W、偏差b第t次学习，权值W、偏差b的的修正公式W(t)=W(t-1)+ΔW(t)，b(t)=b(t-1)+Δb(t)，其中，t为学习次数。</p>\n<p>6.BP网络学习成功结束条件每次学习累积误差平方和中国矿产资源评价新技术与评价新模型每次学习平均误差中国矿产资源评价新技术与评价新模型当平均误差MSE＜ε，BP网络学习成功结束。</p>\n<p>7.BP网络应用预测在应用BP网络时，提供网络输入给输入层，应用给定的BP网络及BP网络学习得到的权值W、偏差b，网络输入经过从输入层经各隐含层向输出层的“顺传播”过程，计算出BP网的预测输出。</p>\n<p>8.神经元激活函数f线性函数f(x)=x，f′(x)=1，f(x)的输入范围(-∞，+∞)，输出范围(-∞，+∞)。一般用于输出层，可使网络输出任何值。</p>\n<p>S型函数S(x)中国矿产资源评价新技术与评价新模型f(x)的输入范围(-∞，+∞)，输出范围(0，1)。f′(x)=f(x)[1-f(x)]，f′(x)的输入范围(-∞，+∞)，输出范围(0，]。</p>\n<p>一般用于隐含层，可使范围(-∞，+∞)的输入，变成(0，1)的网络输出，对较大的输入，放大系数较小；而对较小的输入，放大系数较大，所以可用来处理和逼近非线性的输入/输出关系。</p>\n<p>在用于模式识别时，可用于输出层，产生逼近于0或1的二值输出。双曲正切S型函数中国矿产资源评价新技术与评价新模型f(x)的输入范围(-∞，+∞)，输出范围(-1，1)。</p>\n<p>f′(x)=1-f(x)·f(x)，f′(x)的输入范围(-∞，+∞)，输出范围(0，1]。</p>\n<p>一般用于隐含层，可使范围(-∞，+∞)的输入，变成(-1，1)的网络输出，对较大的输入，放大系数较小；而对较小的输入，放大系数较大，所以可用来处理和逼近非线性的输入/输出关系。</p>\n<p>阶梯函数类型1中国矿产资源评价新技术与评价新模型f(x)的输入范围(-∞，+∞)，输出范围{0，1}。f′(x)=0。</p>\n<p>类型2中国矿产资源评价新技术与评价新模型f(x)的输入范围(-∞，+∞)，输出范围{-1，1}。f′(x)=0。</p>\n<p>斜坡函数类型1中国矿产资源评价新技术与评价新模型f(x)的输入范围(-∞，+∞)，输出范围[0，1]。中国矿产资源评价新技术与评价新模型f′(x)的输入范围(-∞，+∞)，输出范围{0，1}。</p>\n<p>类型2中国矿产资源评价新技术与评价新模型f(x)的输入范围(-∞，+∞)，输出范围[-1，1]。中国矿产资源评价新技术与评价新模型f′(x)的输入范围(-∞，+∞)，输出范围{0，1}。</p>\n<p>三、总体算法1.三层BP网络(含输入层，隐含层，输出层)权值W、偏差b初始化总体算法(1)输入参数X[N][P]，S0，S1，f1[S1]，S2，f2[S2]；(2)计算输入模式X[N][P]各个变量的最大值，最小值矩阵Xmax[N]，Xmin[N]；(3)隐含层的权值W1，偏差b1初始化。</p>\n<p>情形1：隐含层激活函数f()都是双曲正切S型函数1)计算输入模式X[N][P]的每个变量的范围向量Xrng[N]；2)计算输入模式X的每个变量的范围均值向量Xmid[N]；3)计算W，b的幅度因子Wmag；4)产生[-1，1]之间均匀分布的S0×1维随机数矩阵Rand[S1]；5)产生均值为0，方差为1的正态分布的S1×S0维随机数矩阵Randnr[S1][S0]，随机数范围大致在[-1，1]；6)计算W[S1][S0]，b[S1]；7)计算隐含层的初始化权值W1[S1][S0]；8)计算隐含层的初始化偏差b1[S1]；9))输出W1[S1][S0]，b1[S1]。</p>\n<p>情形2：隐含层激活函数f()都是S型函数1)计算输入模式X[N][P]的每个变量的范围向量Xrng[N]；2)计算输入模式X的每个变量的范围均值向量Xmid[N]；3)计算W，b的幅度因子Wmag；4)产生[-1，1]之间均匀分布的S0×1维随机数矩阵Rand[S1]；5)产生均值为0，方差为1的正态分布的S1×S0维随机数矩阵Randnr[S1][S0]，随机数范围大致在[-1，1]；6)计算W[S1][S0]，b[S1]；7)计算隐含层的初始化权值W1[S1][S0]；8)计算隐含层的初始化偏差b1[S1]；9)输出W1[S1][S0]，b1[S1]。</p>\n<p>情形3：隐含层激活函数f()为其他函数的情形1)计算输入模式X[N][P]的每个变量的范围向量Xrng[N]；2)计算输入模式X的每个变量的范围均值向量Xmid[N]；3)计算W，b的幅度因子Wmag；4)产生[-1，1]之间均匀分布的S0×1维随机数矩阵Rand[S1]；5)产生均值为0，方差为1的正态分布的S1×S0维随机数矩阵Randnr[S1][S0]，随机数范围大致在[-1，1]；6)计算W[S1][S0]，b[S1]；7)计算隐含层的初始化权值W1[S1][S0]；8)计算隐含层的初始化偏差b1[S1]；9)输出W1[S1][S0]，b1[S1]。</p>\n<p>(4)输出层的权值W2，偏差b2初始化1)产生[-1，1]之间均匀分布的S2×S1维随机数矩阵W2[S2][S1]；2)产生[-1，1]之间均匀分布的S2×1维随机数矩阵b2[S2]；3)输出W2[S2][S1]，b2[S2]。</p>\n<p>2.应用弹性BP算法(RPROP)学习三层BP网络(含输入层，隐含层，输出层)权值W、偏差b总体算法函数：Train3BP_RPROP(S0，X，P，S1，W1，b1，f1，S2，W2，b2，f2，d，TP)(1)输入参数P对模式(xp，dp)，p=1，2，…，P；三层BP网络结构；学习参数。</p>\n<p>(2)学习初始化1)；2)各层W，b的梯度值，初始化为零矩阵。</p>\n<p>(3)由输入模式X求第一次学习各层输出y0，y1，y2及第一次学习平均误差MSE(4)进入学习循环epoch=1(5)判断每次学习误差是否达到目标误差要求如果MSE＜ϵ，则，跳出epoch循环，转到(12)。</p>\n<p>(6)保存第epoch-1次学习产生的各层W，b的梯度值，(7)求第epoch次学习各层W，b的梯度值，1)求各层误差反向传播值δ；2)求第p次各层W，b的梯度值，；3)求p=1，2，…，P次模式产生的W，b的梯度值，的累加。</p>\n<p>(8)如果epoch=1，则将第epoch-1次学习的各层W，b的梯度值，设为第epoch次学习产生的各层W，b的梯度值，。</p>\n<p>(9)求各层W，b的更新1)求权更新值Δij更新；2)求W，b的权更新值，；3)求第epoch次学习修正后的各层W，b。</p>\n<p>(10)用修正后各层W、b，由X求第epoch次学习各层输出y0，y1，y2及第epoch次学习误差MSE(11)epoch=epoch+1，如果epoch≤MAX_EPOCH，转到(5)；否则，转到(12)。</p>\n<p>(12)输出处理1)如果MSE＜ε，则学习达到目标误差要求，输出W1，b1，W2，b2。2)如果MSE≥ε，则学习没有达到目标误差要求，再次学习。</p>\n<p>(13)结束3.三层BP网络(含输入层，隐含层，输出层)预测总体算法首先应用Train3lBP_RPROP()学习三层BP网络(含输入层，隐含层，输出层)权值W、偏差b，然后应用三层BP网络(含输入层，隐含层，输出层)预测。</p>\n<p>函数：Simu3lBP()。1)输入参数：P个需预测的输入数据向量xp，p=1，2，…，P；三层BP网络结构；学习得到的各层权值W、偏差b。</p>\n<p>2)计算P个需预测的输入数据向量xp(p=1，2，…，P)的网络输出y2[S2][P]，输出预测结果y2[S2][P]。四、总体算法流程图BP网络总体算法流程图见附图2。</p>\n<p>五、数据流图BP网数据流图见附图1。</p>\n<p>六、实例实例一全国铜矿化探异常数据BP模型分类1.全国铜矿化探异常数据准备在全国铜矿化探数据上用稳健统计学方法选取铜异常下限值33.1，生成全国铜矿化探异常数据。</p>\n<p>2.模型数据准备根据全国铜矿化探异常数据，选取7类33个矿点的化探数据作为模型数据。</p>\n<p>这7类分别是岩浆岩型铜矿、斑岩型铜矿、矽卡岩型、海相火山型铜矿、陆相火山型铜矿、受变质型铜矿、海相沉积型铜矿，另添加了一类没有铜异常的模型(表8-1)。3.测试数据准备全国化探数据作为测试数据集。</p>\n<p>4.BP网络结构隐层数2，输入层到输出层向量维数分别为14，9、5、1。学习率设置为0.9，系统误差1e-5。没有动量项。表8-1模型数据表续表5.计算结果图如图8-2、图8-3。</p>\n<p>图8-2图8-3全国铜矿矿床类型BP模型分类示意图实例二全国金矿矿石量品位数据BP模型分类1.模型数据准备根据全国金矿储量品位数据，选取4类34个矿床数据作为模型数据，这4类分别是绿岩型金矿、与中酸性浸入岩有关的热液型金矿、微细浸染型型金矿、火山热液型金矿(表8-2)。</p>\n<p>2.测试数据准备模型样本点和部分金矿点金属量、矿石量、品位数据作为测试数据集。3.BP网络结构输入层为三维，隐层1层，隐层为三维，输出层为四维，学习率设置为0.8，系统误差1e-4，迭代次数5000。</p>\n<p>表8-2模型数据4.计算结果结果见表8-3、8-4。表8-3训练学习结果表8-4预测结果(部分)续表。</p>\n<p><strong>谷歌人工智能写作项目：神经网络伪原创</strong></p>\n<p><img alt=\"\" src=\"image\\1343ebe46ee142b89e7d10fa2aee3b80.png\"/></p>\n<h2>BP神经网络模型各个参数的选取问题</h2>\n<p>样本变量不需要那么多，因为神经网络的信息存储能力有限，过多的样本会造成一些有用的信息被丢弃<strong><a href=\"http://www.wenangouai.com/\" title=\"文案狗\">文案狗</a></strong>。如果样本数量过多，应增加隐层节点数或隐层数目，才能增强学习能力。</p>\n<p>一、隐层数一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向。</p>\n<p>一般来讲应设计神经网络应优先考虑3层网络（即有1个隐层）。一般地，靠增加隐层节点数来获得较低的误差，其训练效果要比增加隐层数更容易实现。</p>\n<p>对于没有隐层的神经网络模型，实际上就是一个线性或非线性（取决于输出层采用线性或非线性转换函数型式）回归模型。</p>\n<p>因此，一般认为，应将不含隐层的网络模型归入回归分析中，技术已很成熟，没有必要在神经网络理论中再讨论之。</p>\n<p>二、隐层节点数在BP网络中，隐层节点数的选择非常重要，它不仅对建立的神经网络模型的性能影响很大，而且是训练时出现“过拟合”的直接原因，但是目前理论上还没有一种科学的和普遍的确定方法。</p>\n<p>目前多数文献中提出的确定隐层节点数的计算公式都是针对训练样本任意多的情况，而且多数是针对最不利的情况，一般工程实践中很难满足，不宜采用。事实上，各种计算公式得到的隐层节点数有时相差几倍甚至上百倍。</p>\n<p>为尽可能避免训练时出现“过拟合”现象，保证足够高的网络性能和泛化能力，确定隐层节点数的最基本原则是：在满足精度要求的前提下取尽可能紧凑的结构，即取尽可能少的隐层节点数。</p>\n<p>研究表明，隐层节点数不仅与输入/输出层的节点数有关，更与需解决的问题的复杂程度和转换函数的型式以及样本数据的特性等因素有关。</p>\n<h2>神经网络算法原理</h2>\n<p>4.2.1概述人工神经网络的研究与计算机的研究几乎是同步发展的。</p>\n<p>1943年心理学家McCulloch和数学家Pitts合作提出了形式神经元的数学模型，20世纪50年代末，Rosenblatt提出了感知器模型，1982年，Hopfiled引入了能量函数的概念提出了神经网络的一种数学模型，1986年，Rumelhart及LeCun等学者提出了多层感知器的反向传播算法等。</p>\n<p>神经网络技术在众多研究者的努力下，理论上日趋完善，算法种类不断增加。目前，有关神经网络的理论研究成果很多，出版了不少有关基础理论的著作，并且现在仍是全球非线性科学研究的热点之一。</p>\n<p>神经网络是一种通过模拟人的大脑神经结构去实现人脑智能活动功能的信息处理系统，它具有人脑的基本功能，但又不是人脑的真实写照。它是人脑的一种抽象、简化和模拟模型，故称之为人工神经网络（边肇祺，2000）。</p>\n<p>人工神经元是神经网络的节点，是神经网络的最重要组成部分之一。目前，有关神经元的模型种类繁多，最常用最简单的模型是由阈值函数、Sigmoid函数构成的模型（图4-3）。</p>\n<p>图4-3人工神经元与两种常见的输出函数神经网络学习及识别方法最初是借鉴人脑神经元的学习识别过程提出的。</p>\n<p>输入参数好比神经元接收信号，通过一定的权值（相当于刺激神经兴奋的强度）与神经元相连，这一过程有些类似于多元线性回归，但模拟的非线性特征是通过下一步骤体现的，即通过设定一阈值（神经元兴奋极限）来确定神经元的兴奋模式，经输出运算得到输出结果。</p>\n<p>经过大量样本进入网络系统学习训练之后，连接输入信号与神经元之间的权值达到稳定并可最大限度地符合已经经过训练的学习样本。</p>\n<p>在被确认网络结构的合理性和学习效果的高精度之后，将待预测样本输入参数代入网络，达到参数预测的目的。</p>\n<p>4.2.2反向传播算法（BP法）发展到目前为止，神经网络模型不下十几种，如前馈神经网络、感知器、Hopfiled网络、径向基函数网络、反向传播算法（BP法）等，但在储层参数反演方面，目前比较成熟比较流行的网络类型是误差反向传播神经网络（BP-ANN）。</p>\n<p>BP网络是在前馈神经网络的基础上发展起来的，始终有一个输入层（它包含的节点对应于每个输入变量）和一个输出层（它包含的节点对应于每个输出值），以及至少有一个具有任意节点数的隐含层（又称中间层）。</p>\n<p>在BP-ANN中，相邻层的节点通过一个任意初始权值全部相连，但同一层内各节点间互不相连。</p>\n<p>对于BP-ANN，隐含层和输出层节点的基函数必须是连续的、单调递增的，当输入趋于正或负无穷大时，它应该接近于某一固定值，也就是说，基函数为“S”型（Kosko，1992）。</p>\n<p>BP-ANN的训练是一个监督学习过程，涉及两个数据集，即训练数据集和监督数据集。</p>\n<p>给网络的输入层提供一组输入信息，使其通过网络而在输出层上产生逼近期望输出的过程，称之为网络的学习，或称对网络进行训练，实现这一步骤的方法则称为学习算法。</p>\n<p>BP网络的学习过程包括两个阶段：第一个阶段是正向过程，将输入变量通过输入层经隐层逐层计算各单元的输出值；第二阶段是反向传播过程，由输出误差逐层向前算出隐层各单元的误差，并用此误差修正前层权值。</p>\n<p>误差信息通过网络反向传播，遵循误差逐步降低的原则来调整权值，直到达到满意的输出为止。</p>\n<p>网络经过学习以后，一组合适的、稳定的权值连接权被固定下来，将待预测样本作为输入层参数，网络经过向前传播便可以得到输出结果，这就是网络的预测。</p>\n<p>反向传播算法主要步骤如下：首先选定权系数初始值，然后重复下述过程直至收敛（对各样本依次计算）。</p>\n<p>（1）从前向后各层计算各单元Oj储层特征研究与预测（2）对输出层计算δj储层特征研究与预测（3）从后向前计算各隐层δj储层特征研究与预测（4）计算并保存各权值修正量储层特征研究与预测（5）修正权值储层特征研究与预测以上算法是对每个样本作权值修正，也可以对各个样本计算δj后求和，按总误差修正权值。</p>\n<h2>神经网络ART1模型</h2>\n<p>一、ART1模型概述自适应共振理论(AdaptiveResonanceTheory)简称ART，是于1976年由美国Boston大学S.Grossberg提出来的。</p>\n<p>这一理论的显著特点是，充分利用了生物神经细胞之间自兴奋与侧抑制的动力学原理，让输入模式通过网络双向连接权的识别与比较，最后达到共振来完成对自身的记忆，并以同样的方法实现网络的回想。</p>\n<p>当提供给网络回想的是一个网络中记忆的、或是与已记忆的模式十分相似的模式时，网络将会把这个模式回想出来，提出正确的分类。</p>\n<p>如果提供给网络回想的是一个网络中不存在的模式，则网络将在不影响已有记忆的前提下，将这一模式记忆下来，并将分配一个新的分类单元作为这一记忆模式的分类标志。</p>\n<p>S.Grossberg和G.A.Carpenter经过多年研究和不断发展，至今已提出了ART1，ART2和ART3三种网络结构。</p>\n<p>ART1网络处理双极型(或二进制)数据，即观察矢量的分量是二值的，它只取0或1。二、ART1模型原理ART1网络是两层结构，分输入层(比较层)和输出层(识别层)。</p>\n<p>从输入层到输出层由前馈连接权连接，从输出层到输入层由反馈连接权连接。</p>\n<p>设网络输入层有N个神经元，网络输出层有M个神经元，二值输入模式和输出向量分别为：Xp=(，，…，)，Yp=(，，…，)，p=1，2，…，P，其中P为输入学习模式的个数。</p>\n<p>设前馈连接权和反馈连接权矩阵分别为W=(wnm)N×M，T=(tnm)N×M，n=1，2，…，N，m=1，2，…，M。</p>\n<p>ART1网络的学习及工作过程，是通过反复地将输入学习模式由输入层向输出层自下而上的识别和由输出层向输入层自上而下的比较过程来实现的。</p>\n<p>当这种自下而上的识别和自上而下的比较达到共振，即输出向量可以正确反映输入学习模式的分类，且网络原有记忆没有受到不良影响时，网络对一个输入学习模式的记忆分类则告完成。</p>\n<p>ART1网络的学习及工作过程，可以分为初始化阶段、识别阶段、比较阶段和探寻阶段。1.初始化阶段ART1网络需要初始化的参数主要有3个：即W=(wnm)N×M，T=(tnm)N×M和ρ。</p>\n<p>反馈连接权T=(tnm)N×M在网络的整个学习过程中取0或1二值形式。这一参数实际上反映了输入层和输出层之间反馈比较的范围或强度。由于网络在初始化前没有任何记忆，相当于一张白纸，即没有选择比较的余的。</p>\n<p>因此可将T的元素全部设置为1，即tnm=1，n=1，2，…，N，m=1，2，…，M。(1)这意味着网络在初始状态时，输入层和输出层之间将进行全范围比较，随着学习过程的深入，再按一定规则选择比较范围。</p>\n<p>前馈连接权W=(wnm)N×M在网络学习结束后，承担着对学习模式的记忆任务。在对W初始化时，应该给所有学习模式提供一个平等竞争的机会，然后通过对输入模式的竞争，按一定规则调整W。</p>\n<p>W的初始值按下式设置：中国矿产资源评价新技术与评价新模型ρ称为网络的警戒参数，其取值范围为0＜ρ≤1。2.识别阶段ART1网络的学习识别阶段发生在输入学习模式由输入层向输出层的传递过程中。</p>\n<p>在这一阶段，首先将一个输入学习模式Xp=(，，…，)提供给网络的输入层，然后把作为输入学习模式的存储媒介的前馈连接权W=(wnm)N×M与表示对这一输入学习模式分类结果的输出层的各个神经元进行比较，以寻找代表正确分类结果的神经元g。</p>\n<p>这一比较与寻找过程是通过寻找输出层神经元最大加权输入值，即神经元之间的竞争过程实现的，如下式所示：中国矿产资源评价新技术与评价新模型中国矿产资源评价新技术与评价新模型中国矿产资源评价新技术与评价新模型至此，网络的识别过程只是告一段落，并没有最后结束。</p>\n<p>此时，神经元m=g是否真正有资格代表对输入学习模式Xp的正确分类，还有待于下面的比较和寻找阶段来进一步确定。一般情况下需要对代表同一输入学习模式的分类结果的神经元进行反复识别。</p>\n<p>3.比较阶段ART1网络的比较阶段的主要职能是完成以下检查任务，每当给已学习结束的网络提供一个供识别的输入模式时，首先检查一下这个模式是否是已学习过的模式，如果是，则让网络回想出这个模式的分类结果；如果不是，则对这个模式加以记忆，并分配一个还没有利用过的输出层神经元来代表这个模式的分类结果。</p>\n<p>具体过程如下：把由输出层每个神经元反馈到输入层的各个神经元的反馈连接权向量Tm=(t1m，t2m，…，tNm)，m=1，2，…，M作为对已学习的输入模式的一条条记录，即让向量Tm=(t1m，t2m，…，tNm)与输出层第m个神经元所代表的某一学习输入模式Xp=(，，…，)完全相等。</p>\n<p>当需要网络对某个输入模式进行回想时，这个输入模式经过识别阶段，竞争到神经元g作为自己的分类结果后，要检查神经元g反馈回来的向量Tg是否与输入模式相等。</p>\n<p>如果相等，则说明这是一个已记忆过的模式，神经元g代表了这个模式的分类结果，识别与比较产生了共振，网络不需要再经过寻找阶段，直接进入下一个输入模式的识别阶段；如果不相符，则放弃神经元g的分类结果，进入寻找阶段。</p>\n<p>在比较阶段，当用向量Tg与输入模式XP进行比较时，允许二者之间有一定的差距，差距的大小由警戒参数ρ决定。首先计算中国矿产资源评价新技术与评价新模型Cg表示向量Tg与输入模式XP的拟合度。</p>\n<p>在式中，(tng*xn)表示向量Tg=(t1g，t2g，…，tNg)与输入模式Xp=(，，…，)的逻辑“与”。当Tg=XP时，Cg=1。</p>\n<p>当Cg≥ρ时，说明拟合度大于要求，没有超过警戒线。以上两种情况均可以承认识别结果。</p>\n<p>当Cg≠1且Cg＞ρ时，按式(6)式(7)将前馈连接权Wg=(w1g，w2g，…，wNg)和反馈连接权Tg=(t1g，t2g，…，tNg)向着与XP更接近的方向调整。</p>\n<p>中国矿产资源评价新技术与评价新模型tng(t+1)=tng(t)*xn，n=1，2，…，N。</p>\n<p>(7)当Cg＜ρ时，说明拟合度小于要求，超过警戒线，则拒绝识别结果，将神经元g重新复位为0，并将这个神经元排除在下次识别范围之外，网络转入寻找阶段。</p>\n<p>4.寻找阶段寻找阶段是网络在比较阶段拒绝识别结果之后转入的一个反复探寻的阶段，在这一阶段中，网络将在余下的输出层神经元中搜索输入模式Xp的恰当分类。</p>\n<p>只要在输出向量Yp=(，，…)中含有与这一输入模式Xp相对应、或在警戒线以内相对应的分类单元，则网络可以得到与记忆模式相符的分类结果。</p>\n<p>如果在已记忆的分类结果中找不到与现在输入的模式相对应的分类，但在输出向量中还有未曾使用过的单元，则可以给这个输入模式分配一个新的分类单元。</p>\n<p>在以上两种情况下，网络的寻找过程总能获得成功，也就是说共振终将发生。</p>\n<p>三、总体算法设网络输入层有N个神经元，网络输出层有M个神经元，二值输入模式和输出向量分别为：Xp=(，，…，)，Yp=(，，…，)p=1，2，…，p，其中p为输入学习模式的个数。</p>\n<p>设前馈连接权和反馈连接权矩阵分别为W=(wnm)N×M，T=(tnm)N×M，n=1，2，…，N，m=1，2，…，M。</p>\n<p>(1)网络初始化tnm(0)=1，中国矿产资源评价新技术与评价新模型n=1，2，…，N，m=1，2，…，M。0＜ρ≤1。</p>\n<p>(2)将输入模式Xp=(，，…，)提供给网络的输入层(3)计算输出层各神经元输入加权和中国矿产资源评价新技术与评价新模型(4)选择XP的最佳分类结果中国矿产资源评价新技术与评价新模型令神经元g的输出为1。</p>\n<p>(5)计算中国矿产资源评价新技术与评价新模型中国矿产资源评价新技术与评价新模型判断中国矿产资源评价新技术与评价新模型当式(8)成立，转到(7)，否则，转到(6)。</p>\n<p>(6)取消识别结果，将输出层神经元g的输出值复位为0，并将这一神经元排除在下一次识别的范围之外，返回步骤(4)。</p>\n<p>当所有已利用过的神经元都无法满足式(8)，则选择一个新的神经元作为分类结果，转到步骤(7)。</p>\n<p>(7)承认识别结果，并按下式调整连接权中国矿产资源评价新技术与评价新模型tng(t+1)=tng(t)*xn，n=1，2，…，N。</p>\n<p>(8)将步骤(6)复位的所有神经元重新加入识别范围之内，返回步骤(2)对下一模式进行识别。(9)输出分类识别结果。(10)结束。四、实例实例为ART1神经网络模型在柴北缘-东昆仑造山型金矿预测的应用。</p>\n<p>1.建立综合预测模型柴北缘—东昆仑地区位于青海省的西部，是中央造山带的西部成员——秦祁昆褶皱系的一部分，是典型的复合造山带(殷鸿福等，1998)。</p>\n<p>根据柴北缘—东昆仑地区地质概括以及造山型金矿成矿特点，选择与成矿相关密切的专题数据，建立柴北缘—东昆仑地区的综合信息找矿模型：1)金矿重砂异常数据是金矿的重要找矿标志。</p>\n<p>2)金矿水化异常数据是金矿的重要找矿标志。3)金矿的化探异常数据控制金矿床的分布。4)金矿的空间分布与通过该区的深大断裂有关。5)研究区内断裂密集程度控制金矿的产出。</p>\n<p>6)重力构造的存在与否是金矿存在的一个标志。7)磁力构造线的存在也是金矿存在的一个重要标志。8)研究区地质复杂程度也对金矿的产出具有重要的作用。9)研究区存在的矿(化)点是一个重要的标志。</p>\n<p>2.划分预测单元预测工作是在单元上进行的，预测工作的结果是与单元有着较为直接的联系，在找矿模型指导下，以最大限度地反映成矿信息和预测单元面积最小为原则，通过对研究区内地质、地球物理、地球化学等的综合资料分析，对可能的成矿地段圈定了预测单元。</p>\n<p>采用网格化单元作为本次研究的预测单元，网格单元的大小是，40×40，将研究区划分成774个预测单元。</p>\n<p>3.变量选择(表8-6)4.ART1模型预测结果ART1神经网络模型算法中，给定不同的阈值，将改变预测分类的结果。</p>\n<p>本次实验选取得阈值为ρ=0.41，系统根据此阈值进行计算获得计算结果，并通过将不同的分类结果赋予不同的颜色，最终获得ART模型预测单元的分类结果。分类的结果是形成29个类别。</p>\n<p>分类结果用不同的颜色表示，其具体结果地显示见图8-5。图形中颜色只代表类别号，不代表分类的好坏。将矿点专题图层叠加以后，可以看出，颜色为灰色的单元与矿的关系更为密切。</p>\n<p>表8-6预测变量标志的选择表图8-5东昆仑—柴北缘地区基于ARTL模型的金矿分类结果图。</p>\n<h2>bp神经网络用啥算法？</h2>\n<p>自己找个例子算一下，推导一下，这个回答起来比较复杂神经网络对模型的表达能力依赖于优化算法，优化是一个不断计算梯度并调整可学习参数的过程，Fluid中的优化算法可参考 优化器 。</p>\n<p>在网络的训练过程中，梯度计算分为两个步骤：前向计算与 反向传播 。前向计算会根据您搭建的网络结构，将输入单元的状态传递到输出单元。</p>\n<p>反向传播借助 链式法则 ，计算两个或两个以上复合函数的导数，将输出单元的梯度反向传播回输入单元，根据计算出的梯度，调整网络的可学习参数。BP算法隐层的引入使网络具有很大的潜力。</p>\n<p>但正像Minskey和Papert当时所指出的．虽然对所有那些能用简单(无隐层)网结解决的问题有非常简单的学习规则，即简单感知器的收敛程序(主要归功于Widrow和HMf于1960年提出的Delta规刚)，BP算法但当时并没有找到同样有技的含隐层的同培的学习规则。</p>\n<p>对此问题的研究有三个基本的结果。一种是使用简单无监督学习规则的竞争学习方法．但它缺乏外部信息．难以确定适台映射的隐层结构。第二条途径是假设一十内部(隐层)的表示方法，这在一些先约条件下是台理的。</p>\n<p>另一种方法是利用统计手段设计一个学习过程使之能有技地实现适当的内部表示法，Hinton等人(1984年)提出的Bolzmann机是这种方法的典型例子．它要求网络在两个不同的状态下达到平衡，并且只局限于对称网络。</p>\n<p>Barto和他的同事(1985年)提出了另一条利用统计手段的学习方法。</p>\n<p>但迄今为止最有教和最实用的方瑶是Rumelhart、Hinton和Williams(1986年)提出的一般Delta法则，即反向传播(BP)算法。</p>\n<p>Parter(1985年)也独立地得出过相似的算法,他称之为学习逻辑。此外，Lecun(1985年)也研究出大致相似的学习法则。</p>\n<h2>用MATLAB建立bp神经网络模型，求高手，在线等</h2>\n<p>Matlab神经网络工具箱提供了一系列用于建立和训练bp神经网络模型的函数命令，很难一时讲全。下面仅以一个例子列举部分函数的部分用法。</p>\n<p>更多的函数和用法请仔细查阅NeuralNetworkToolbox的帮助文档。例子：利用bp神经网络模型建立z=sin(x+y)的模型并检验效果%第1步。</p>\n<p>随机生成200个采样点用于训练x=unifrnd(-5,5,1,200);y=unifrnd(-5,5,1,200);z=sin(x+y);%第2步。建立神经网络模型。</p>\n<p>其中参数一是输入数据的范围，参数二是各层神经元数量，参数三是各层传递函数类型。</p>\n<p>N=newff([-55;-55],[5,5,1],{'tansig','tansig','purelin'});%第3步。训练。这里用批训练函数train。也可用adapt函数进行增长训练。</p>\n<p>N=train(N,[x;y],z);%第4步。检验训练成果。</p>\n<p>[X,Y]=meshgrid(linspace(-5,5));Z=sim(N,[X(:),Y(:)]');figuremesh(X,Y,reshape(Z,100,100));holdon;plot3(x,y,z,'.')。</p>\n<p> </p>\n</div>\n</div>"}