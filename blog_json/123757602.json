{"blogid": "123757602", "writerAge": "码龄4年", "writerBlogNum": "116", "writerCollect": "743", "writerComment": "150", "writerFan": "5202", "writerGrade": "4级", "writerIntegral": "1595", "writerName": "小馨馨的小翟", "writerProfileAdress": "writer_image\\profile_123757602.jpg", "writerRankTotal": "10854", "writerRankWeekly": "1762", "writerThumb": "199", "writerVisitNum": "120169", "blog_read_count": "9926", "blog_time": "已于 2022-05-03 09:28:58 修改", "blog_title": "手撕Resnet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<p><strong> Alexnet网络详解代码：</strong><a href=\"https://blog.csdn.net/qq_43215597/article/details/123495329?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165104333916782388030041%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165104333916782388030041&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-123495329.nonecase&amp;utm_term=alexnet&amp;spm=1018.2226.3001.4450\" title=\"手撕Alexnet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客_alexnet神经网络代码\">手撕Alexnet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客_alexnet神经网络代码</a></p>\n<p><strong>VGG网络详解代码：</strong> <a href=\"https://blog.csdn.net/qq_43215597/article/details/123578789?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165104342016781685397692%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165104342016781685397692&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-4-123578789.nonecase&amp;utm_term=VGG&amp;spm=1018.2226.3001.4450\" title=\"手撕VGG卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客\">手撕VGG卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客</a></p>\n<p><strong>Resnet网络详解代码：</strong> <a href=\"https://blog.csdn.net/qq_43215597/article/details/123757602?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165104344616780366521059%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165104344616780366521059&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-123757602.nonecase&amp;utm_term=rESNET&amp;spm=1018.2226.3001.4450\" title=\"手撕Resnet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客\">手撕Resnet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客</a></p>\n<p><strong>Googlenet网络详解代码：</strong><a href=\"https://blog.csdn.net/qq_43215597/article/details/124061070?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165104343616782246433545%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165104343616782246433545&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-124061070.nonecase&amp;utm_term=GOOGLENET&amp;spm=1018.2226.3001.4450\" title=\"手撕Googlenet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客_cnn测试集准确率低\">手撕Googlenet卷积神经网络-pytorch-详细注释版（可以直接替换自己数据集）-直接放置自己的数据集就能直接跑。跑的代码有问题的可以在评论区指出，看到了会回复。训练代码和预测代码均有。_小馨馨的小翟的博客-CSDN博客_cnn测试集准确率低</a></p>\n<p><strong>集成学习模型融合网络详解代码： </strong></p>\n<p><a href=\"https://blog.csdn.net/qq_43215597/article/details/123907732?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165104348416782388059672%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=165104348416782388059672&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-3-123907732.nonecase&amp;utm_term=ALEXNET&amp;spm=1018.2226.3001.4450\" title=\"集成学习-模型融合（Lenet，Alexnet，Vgg）三个模型进行融合-附源代码-宇宙的尽头一定是融合模型而不是单个模型。_小馨馨的小翟的博客-CSDN博客_torch模型融合\">集成学习-模型融合（Lenet，Alexnet，Vgg）三个模型进行融合-附源代码-宇宙的尽头一定是融合模型而不是单个模型。_小馨馨的小翟的博客-CSDN博客_torch模型融合</a></p>\n<p><strong>深度学习常用数据增强，数据扩充代码数据缩放代码： </strong></p>\n<p><a href=\"https://blog.csdn.net/qq_43215597/article/details/123905879?spm=1001.2014.3001.5501\" title=\"深度学习数据增强方法-内含（亮度增强，对比度增强，旋转图图像，翻转图像，仿射变化扩充图像，错切变化扩充图像，HSV数据增强）七种方式进行增强-每种扩充一张实现7倍扩）+ 图像缩放代码-批量_小馨馨的小翟的博客-CSDN博客_训练数据增强\">深度学习数据增强方法-内含（亮度增强，对比度增强，旋转图图像，翻转图像，仿射变化扩充图像，错切变化扩充图像，HSV数据增强）七种方式进行增强-每种扩充一张实现7倍扩）+ 图像缩放代码-批量_小馨馨的小翟的博客-CSDN博客_训练数据增强</a></p>\n<p></p>\n<p></p>\n<p><strong>Resnet（Deep residual network, ResNet），深度残差神经网络，卷积神经网络历史在具有划时代意义的神经网络。与Alexnet和VGG不同的是，网络结构上就有很大的改变，在大家为了提升卷积神经网络的性能在不断提升网络深度的时候，大家发现随着网络深度的提升，网络的效果变得越来越差，甚至出现了网络的退化问题，80层的网络比30层的效果还差，深度网络存在的梯度消失和爆炸问题越来越严重，这使得训练一个优异的深度学习模型变得更加艰难，在这种情况下，网络结构图</strong></p>\n<p>                                                                            <img alt=\"\" src=\"https://img-blog.csdn.net/20161006214122425?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\"/></p>\n<p></p>\n<p><strong>何恺明提出了残差神经网络，提出残差学习来解决这个问题，他设计了如下图所示的神经网络结构，并在VGG19的基础上进行了修改。简单来说就是类似与加网络之前的结果，拉出来拉到后面进行拼接，组成新的输出。</strong></p>\n<p>                                             <img alt=\"\" height=\"227\" src=\"image\\ee6b72d889474d96993b88bc4850f4f0.png\" width=\"419\"/></p>\n<p><strong> </strong></p>\n<p><strong>事实上在一些目标检测算法例如yolo系列的算法当中也用到了Resnet的结构，而且在很多多尺度特征融合的算法里 也是采用类似的方法，多层不同的输出组合在一起，因此学好Resnet网络对于以后学习目标检测算法具有深刻的意义。</strong></p>\n<p></p>\n<p><strong>导入库：</strong></p>\n<pre><code>import torch\nimport torchvision\nimport torchvision.models\nimport os\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms</code></pre>\n<p><strong>图像预处理：</strong></p>\n<p><strong>将图像放缩成120*120进行处理，如果需要放缩成其他比例的，直接修改代码中的120成其他数即可。</strong></p>\n<pre><code>data_transform = {\n    \"train\": transforms.Compose([transforms.RandomResizedCrop(120),\n                                 transforms.RandomHorizontalFlip(),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n    \"val\": transforms.Compose([transforms.Resize((120, 120)),  # cannot 224, must (224, 224)\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}</code></pre>\n<p><strong>导入数据：</strong></p>\n<p><strong>将数据像挤牙膏似的一点一点的抽出去，设置相应的batc_size</strong></p>\n<p><strong>自己的数据放在跟代码相同的文件夹下新建一个data文件夹，data文件夹里的新建一个train文件夹用于放置训练集的图片。同理新建一个val文件夹用于放置测试集的图片。</strong></p>\n<pre><code>train_data = torchvision.datasets.ImageFolder(root = \"./data/train\" ,   transform = data_transform[\"train\"])\n\ntraindata = DataLoader(dataset=train_data, batch_size=128, shuffle=True, num_workers=0)  # 将训练数据以每次32张图片的形式抽出进行训练\n\ntest_data = torchvision.datasets.ImageFolder(root = \"./data/val\" , transform = data_transform[\"val\"])\n\ntrain_size = len(train_data)  # 训练集的长度\ntest_size = len(test_data)  # 测试集的长度\nprint(train_size)   #输出训练集长度看一下，相当于看看有几张图片\nprint(test_size)    #输出测试集长度看一下，相当于看看有几张图片\ntestdata = DataLoader(dataset=test_data, batch_size=128, shuffle=True, num_workers=0)  # 将训练数据以每次32张图片的形式抽出进行测试\n</code></pre>\n<p><strong>设置GPU 和 CPU的使用：</strong></p>\n<p><strong>有GPU则调用GPU，没有的话就调用CPU</strong></p>\n<pre><code>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"using {} device.\".format(device))\n</code></pre>\n<p><strong>构建Resnet网络：</strong></p>\n<p><strong>因为Resnet网络太深，所以一般采用迁移学习的方法进行搭建</strong></p>\n<pre><code>class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n                               kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channel)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n                               kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n\n    expansion = 4\n\n    def __init__(self, in_channel, out_channel, stride=1, downsample=None,\n                 groups=1, width_per_group=64):\n        super(Bottleneck, self).__init__()\n\n        width = int(out_channel * (width_per_group / 64.)) * groups\n\n        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,\n                               kernel_size=1, stride=1, bias=False)  # squeeze channels\n        self.bn1 = nn.BatchNorm2d(width)\n        # -----------------------------------------\n        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,\n                               kernel_size=3, stride=stride, bias=False, padding=1)\n        self.bn2 = nn.BatchNorm2d(width)\n        # -----------------------------------------\n        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion,\n                               kernel_size=1, stride=1, bias=False)  # unsqueeze channels\n        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self,\n                 block,\n                 blocks_num,\n                 num_classes=7,#种类修改的地方，你是几种，就把这个改成几，我是7中所以这里是7\n                 include_top=True,\n                 groups=1,\n                 width_per_group=64):\n        super(ResNet, self).__init__()\n        self.include_top = include_top\n        self.in_channel = 64\n\n        self.groups = groups\n        self.width_per_group = width_per_group\n\n        self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2,\n                               padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channel)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, blocks_num[0])\n        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\n        if self.include_top:\n            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)\n            self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\n    def _make_layer(self, block, channel, block_num, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channel != channel * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(channel * block.expansion))\n\n        layers = []\n        layers.append(block(self.in_channel,\n                            channel,\n                            downsample=downsample,\n                            stride=stride,\n                            groups=self.groups,\n                            width_per_group=self.width_per_group))\n        self.in_channel = channel * block.expansion\n\n        for _ in range(1, block_num):\n            layers.append(block(self.in_channel,\n                                channel,\n                                groups=self.groups,\n                                width_per_group=self.width_per_group))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        if self.include_top:\n            x = self.avgpool(x)\n            x = torch.flatten(x, 1)\n            x = self.fc(x)\n\n        return x\n\n\ndef resnet34(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet34-333f7ec4.pth\n    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n\n\ndef resnet50(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n\n\ndef resnet101(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)\n\n\ndef resnext50_32x4d(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\n    groups = 32\n    width_per_group = 4\n    return ResNet(Bottleneck, [3, 4, 6, 3],\n                  num_classes=num_classes,\n                  include_top=include_top,\n                  groups=groups,\n                  width_per_group=width_per_group)\n\n\ndef resnext101_32x8d(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\n    groups = 32\n    width_per_group = 8\n    return ResNet(Bottleneck, [3, 4, 23, 3],\n                  num_classes=num_classes,\n                  include_top=include_top,\n                  groups=groups,\n                  width_per_group=width_per_group)\n\n\nnet = resnet34()\n# load pretrain weights\n# download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth\nmodel_weight_path = \"./resnet34-pre.pth\"  #加载resnet的预训练模型\nassert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\nnet.load_state_dict(torch.load(model_weight_path, map_location=device))\nnet.to(device)\nprint(net.to(device))  #输出模型结构\n</code></pre>\n<p> <strong>启动模型，测试模型输出： </strong></p>\n<pre><code>\ntest1 = torch.ones(64, 3, 120, 120)  # 测试一下输出的形状大小 输入一个64,3,120,120的向量\n\ntest1 = net(test1.to(device))    #将向量打入神经网络进行测试\nprint(test1.shape)  #查看输出的结果\n</code></pre>\n<p> <strong>设置训练需要的参数，epoch，学习率learning 优化器。损失函数。</strong></p>\n<pre><code>epoch = 10  # 迭代次数即训练次数\nlearning = 0.001  # 学习率\noptimizer = torch.optim.Adam(net.parameters(), lr=learning)  # 使用Adam优化器-写论文的话可以具体查一下这个优化器的原理\nloss = nn.CrossEntropyLoss()  # 损失计算方式，交叉熵损失函数</code></pre>\n<p><strong>设置四个空数组，用来存放训练集的loss和accuracy    测试集的loss和 accuracy</strong></p>\n<pre><code>train_loss_all = []\ntrain_accur_all = []\ntest_loss_all = []\ntest_accur_all = []</code></pre>\n<p><strong>开始训练：</strong></p>\n<pre><code>for i in range(epoch):  #开始迭代\n    train_loss = 0   #训练集的损失初始设为0\n    train_num = 0.0   #\n    train_accuracy = 0.0  #训练集的准确率初始设为0\n    net.train()   #将模型设置成 训练模式\n    train_bar = tqdm(traindata)  #用于进度条显示，没啥实际用处\n    for step, data in enumerate(train_bar):  #开始迭代跑， enumerate这个函数不懂可以查查，将训练集分为 data是序号，data是数据\n        img, target = data    #将data 分位 img图片，target标签\n        optimizer.zero_grad()  # 清空历史梯度\n        outputs = net(img.to(device))  # 将图片打入网络进行训练,outputs是输出的结果\n\n        loss1 = loss(outputs, target.to(device))  # 计算神经网络输出的结果outputs与图片真实标签target的差别-这就是我们通常情况下称为的损失\n        outputs = torch.argmax(outputs, 1)   #会输出10个值，最大的值就是我们预测的结果 求最大值\n        loss1.backward()   #神经网络反向传播\n        optimizer.step()  #梯度优化 用上面的abam优化\n        train_loss += abs(loss1.item()) * img.size(0)  #将所有损失的绝对值加起来\n        accuracy = torch.sum(outputs == target.to(device))   #outputs == target的 即使预测正确的，统计预测正确的个数,从而计算准确率\n        train_accuracy = train_accuracy + accuracy   #求训练集的准确率\n        train_num += img.size(0)  #\n\n    print(\"epoch：{} ， train-Loss：{} , train-accuracy：{}\".format(i + 1, train_loss / train_num,   #输出训练情况\n                                                                train_accuracy / train_num))\n    train_loss_all.append(train_loss / train_num)   #将训练的损失放到一个列表里 方便后续画图\n    train_accur_all.append(train_accuracy.double().item() / train_num)#训练集的准确率</code></pre>\n<p><strong>开始测试：</strong></p>\n<pre><code>    test_loss = 0   #同上 测试损失\n    test_accuracy = 0.0  #测试准确率\n    test_num = 0\n    net.eval()   #将模型调整为测试模型\n    with torch.no_grad():  #清空历史梯度，进行测试  与训练最大的区别是测试过程中取消了反向传播\n        test_bar = tqdm(testdata)\n        for data in test_bar:\n            img, target = data\n\n            outputs = net(img.to(device))\n            loss2 = loss(outputs, target.to(device))\n            outputs = torch.argmax(outputs, 1)\n            test_loss = test_loss + abs(loss2.item()) * img.size(0)\n            accuracy = torch.sum(outputs == target.to(device))\n            test_accuracy = test_accuracy + accuracy\n            test_num += img.size(0)\n\n    print(\"test-Loss：{} , test-accuracy：{}\".format(test_loss / test_num, test_accuracy / test_num))\n    test_loss_all.append(test_loss / test_num)\n    test_accur_all.append(test_accuracy.double().item() / test_num)\n</code></pre>\n<p><strong>绘制训练集loss和accuracy图 和测试集的loss和accuracy图：</strong></p>\n<pre><code>#下面的是画图过程，将上述存放的列表  画出来即可\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(range(epoch), train_loss_all,\n         \"ro-\", label=\"Train loss\")\nplt.plot(range(epoch), test_loss_all,\n         \"bs-\", label=\"test loss\")\nplt.legend()\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss\")\nplt.subplot(1, 2, 2)\nplt.plot(range(epoch), train_accur_all,\n         \"ro-\", label=\"Train accur\")\nplt.plot(range(epoch), test_accur_all,\n         \"bs-\", label=\"test accur\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"acc\")\nplt.legend()\nplt.show()\n\ntorch.save(net.state_dict(), \"Resnet.pth\")\nprint(\"模型已保存\")\n\n</code></pre>\n<p><strong>全部train训练代码：</strong></p>\n<pre><code>import torch\nimport torchvision\nimport torchvision.models\nimport os\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndata_transform = {\n    \"train\": transforms.Compose([transforms.RandomResizedCrop(120),\n                                 transforms.RandomHorizontalFlip(),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n    \"val\": transforms.Compose([transforms.Resize((120, 120)),  # cannot 224, must (224, 224)\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n\ntrain_data = torchvision.datasets.ImageFolder(root = \"./data/train\" ,   transform = data_transform[\"train\"])\n\ntraindata = DataLoader(dataset=train_data, batch_size=128, shuffle=True, num_workers=0)  # 将训练数据以每次32张图片的形式抽出进行训练\n\ntest_data = torchvision.datasets.ImageFolder(root = \"./data/val\" , transform = data_transform[\"val\"])\n\ntrain_size = len(train_data)  # 训练集的长度\ntest_size = len(test_data)  # 测试集的长度\nprint(train_size)   #输出训练集长度看一下，相当于看看有几张图片\nprint(test_size)    #输出测试集长度看一下，相当于看看有几张图片\ntestdata = DataLoader(dataset=test_data, batch_size=128, shuffle=True, num_workers=0)  # 将训练数据以每次32张图片的形式抽出进行测试\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"using {} device.\".format(device))\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n                               kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channel)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n                               kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n  \n    expansion = 4\n\n    def __init__(self, in_channel, out_channel, stride=1, downsample=None,\n                 groups=1, width_per_group=64):\n        super(Bottleneck, self).__init__()\n\n        width = int(out_channel * (width_per_group / 64.)) * groups\n\n        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,\n                               kernel_size=1, stride=1, bias=False)  # squeeze channels\n        self.bn1 = nn.BatchNorm2d(width)\n        # -----------------------------------------\n        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,\n                               kernel_size=3, stride=stride, bias=False, padding=1)\n        self.bn2 = nn.BatchNorm2d(width)\n        # -----------------------------------------\n        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion,\n                               kernel_size=1, stride=1, bias=False)  # unsqueeze channels\n        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self,\n                 block,\n                 blocks_num,\n                 num_classes=7,\n                 include_top=True,\n                 groups=1,\n                 width_per_group=64):\n        super(ResNet, self).__init__()\n        self.include_top = include_top\n        self.in_channel = 64\n\n        self.groups = groups\n        self.width_per_group = width_per_group\n\n        self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2,\n                               padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channel)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, blocks_num[0])\n        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\n        if self.include_top:\n            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)\n            self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\n    def _make_layer(self, block, channel, block_num, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channel != channel * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(channel * block.expansion))\n\n        layers = []\n        layers.append(block(self.in_channel,\n                            channel,\n                            downsample=downsample,\n                            stride=stride,\n                            groups=self.groups,\n                            width_per_group=self.width_per_group))\n        self.in_channel = channel * block.expansion\n\n        for _ in range(1, block_num):\n            layers.append(block(self.in_channel,\n                                channel,\n                                groups=self.groups,\n                                width_per_group=self.width_per_group))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        if self.include_top:\n            x = self.avgpool(x)\n            x = torch.flatten(x, 1)\n            x = self.fc(x)\n\n        return x\n\n\ndef resnet34(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet34-333f7ec4.pth\n    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n\n\ndef resnet50(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n\n\ndef resnet101(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)\n\n\ndef resnext50_32x4d(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\n    groups = 32\n    width_per_group = 4\n    return ResNet(Bottleneck, [3, 4, 6, 3],\n                  num_classes=num_classes,\n                  include_top=include_top,\n                  groups=groups,\n                  width_per_group=width_per_group)\n\n\ndef resnext101_32x8d(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\n    groups = 32\n    width_per_group = 8\n    return ResNet(Bottleneck, [3, 4, 23, 3],\n                  num_classes=num_classes,\n                  include_top=include_top,\n                  groups=groups,\n                  width_per_group=width_per_group)\n\n\nnet = resnet34()\n# load pretrain weights\n# download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth\nmodel_weight_path = \"./resnet34-pre.pth\"\nassert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\nnet.load_state_dict(torch.load(model_weight_path, map_location=device))\nnet.to(device)\nprint(net.to(device))  #输出模型结构\n\n\ntest1 = torch.ones(64, 3, 120, 120)  # 测试一下输出的形状大小 输入一个64,3,120,120的向量\n\ntest1 = net(test1.to(device))    #将向量打入神经网络进行测试\nprint(test1.shape)  #查看输出的结果\n\nepoch = 10  # 迭代次数即训练次数\nlearning = 0.001  # 学习率\noptimizer = torch.optim.Adam(net.parameters(), lr=learning)  # 使用Adam优化器-写论文的话可以具体查一下这个优化器的原理\nloss = nn.CrossEntropyLoss()  # 损失计算方式，交叉熵损失函数\n\ntrain_loss_all = []  # 存放训练集损失的数组\ntrain_accur_all = []  # 存放训练集准确率的数组\ntest_loss_all = []  # 存放测试集损失的数组\ntest_accur_all = []  # 存放测试集准确率的数组\nfor i in range(epoch):  #开始迭代\n    train_loss = 0   #训练集的损失初始设为0\n    train_num = 0.0   #\n    train_accuracy = 0.0  #训练集的准确率初始设为0\n    net.train()   #将模型设置成 训练模式\n    train_bar = tqdm(traindata)  #用于进度条显示，没啥实际用处\n    for step, data in enumerate(train_bar):  #开始迭代跑， enumerate这个函数不懂可以查查，将训练集分为 data是序号，data是数据\n        img, target = data    #将data 分位 img图片，target标签\n        optimizer.zero_grad()  # 清空历史梯度\n        outputs = net(img.to(device))  # 将图片打入网络进行训练,outputs是输出的结果\n\n        loss1 = loss(outputs, target.to(device))  # 计算神经网络输出的结果outputs与图片真实标签target的差别-这就是我们通常情况下称为的损失\n        outputs = torch.argmax(outputs, 1)   #会输出10个值，最大的值就是我们预测的结果 求最大值\n        loss1.backward()   #神经网络反向传播\n        optimizer.step()  #梯度优化 用上面的abam优化\n        train_loss += abs(loss1.item()) * img.size(0)  #将所有损失的绝对值加起来\n        accuracy = torch.sum(outputs == target.to(device))   #outputs == target的 即使预测正确的，统计预测正确的个数,从而计算准确率\n        train_accuracy = train_accuracy + accuracy   #求训练集的准确率\n        train_num += img.size(0)  #\n\n    print(\"epoch：{} ， train-Loss：{} , train-accuracy：{}\".format(i + 1, train_loss / train_num,   #输出训练情况\n                                                                train_accuracy / train_num))\n    train_loss_all.append(train_loss / train_num)   #将训练的损失放到一个列表里 方便后续画图\n    train_accur_all.append(train_accuracy.double().item() / train_num)#训练集的准确率\n    test_loss = 0   #同上 测试损失\n    test_accuracy = 0.0  #测试准确率\n    test_num = 0\n    net.eval()   #将模型调整为测试模型\n    with torch.no_grad():  #清空历史梯度，进行测试  与训练最大的区别是测试过程中取消了反向传播\n        test_bar = tqdm(testdata)\n        for data in test_bar:\n            img, target = data\n\n            outputs = net(img.to(device))\n            loss2 = loss(outputs, target.to(device))\n            outputs = torch.argmax(outputs, 1)\n            test_loss = test_loss + abs(loss2.item()) * img.size(0)\n            accuracy = torch.sum(outputs == target.to(device))\n            test_accuracy = test_accuracy + accuracy\n            test_num += img.size(0)\n\n    print(\"test-Loss：{} , test-accuracy：{}\".format(test_loss / test_num, test_accuracy / test_num))\n    test_loss_all.append(test_loss / test_num)\n    test_accur_all.append(test_accuracy.double().item() / test_num)\n\n#下面的是画图过程，将上述存放的列表  画出来即可\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(range(epoch), train_loss_all,\n         \"ro-\", label=\"Train loss\")\nplt.plot(range(epoch), test_loss_all,\n         \"bs-\", label=\"test loss\")\nplt.legend()\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss\")\nplt.subplot(1, 2, 2)\nplt.plot(range(epoch), train_accur_all,\n         \"ro-\", label=\"Train accur\")\nplt.plot(range(epoch), test_accur_all,\n         \"bs-\", label=\"test accur\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"acc\")\nplt.legend()\nplt.show()\n\ntorch.save(net.state_dict(), \"Resnet.pth\")\nprint(\"模型已保存\")\n\n</code></pre>\n<p><strong>全部predict代码： </strong></p>\n<pre><code>import torch\nfrom PIL import Image\nfrom torch import nn\nfrom torchvision.transforms import transforms\n\nimage_path = \"1.JPG\"  # 相对路径 导入图片\ntrans = transforms.Compose([transforms.Resize((120, 120)),\n                            transforms.ToTensor()])  # 将图片缩放为跟训练集图片的大小一样 方便预测，且将图片转换为张量\nimage = Image.open(image_path)  # 打开图片\nprint(image)  # 输出图片 看看图片格式\nimage = image.convert(\"RGB\")  # 将图片转换为RGB格式\nimage = trans(image)  # 上述的缩放和转张量操作在这里实现\nprint(image)  # 查看转换后的样子\nimage = torch.unsqueeze(image, dim=0)  # 将图片维度扩展一维\n\nclasses = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]  # 预测种类#自己是几种，这里就改成自己种类的字符数组\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n                               kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channel)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n                               kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。\n    但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2，\n    这么做的好处是能够在top1上提升大概0.5%的准确率。\n    可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, in_channel, out_channel, stride=1, downsample=None,\n                 groups=1, width_per_group=64):\n        super(Bottleneck, self).__init__()\n\n        width = int(out_channel * (width_per_group / 64.)) * groups\n\n        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,\n                               kernel_size=1, stride=1, bias=False)  # squeeze channels\n        self.bn1 = nn.BatchNorm2d(width)\n        # -----------------------------------------\n        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,\n                               kernel_size=3, stride=stride, bias=False, padding=1)\n        self.bn2 = nn.BatchNorm2d(width)\n        # -----------------------------------------\n        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion,\n                               kernel_size=1, stride=1, bias=False)  # unsqueeze channels\n        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self,\n                 block,\n                 blocks_num,\n                 num_classes=7,\n                 include_top=True,\n                 groups=1,\n                 width_per_group=64):\n        super(ResNet, self).__init__()\n        self.include_top = include_top\n        self.in_channel = 64\n\n        self.groups = groups\n        self.width_per_group = width_per_group\n\n        self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2,\n                               padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channel)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, blocks_num[0])\n        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\n        if self.include_top:\n            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)\n            self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\n    def _make_layer(self, block, channel, block_num, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channel != channel * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(channel * block.expansion))\n\n        layers = []\n        layers.append(block(self.in_channel,\n                            channel,\n                            downsample=downsample,\n                            stride=stride,\n                            groups=self.groups,\n                            width_per_group=self.width_per_group))\n        self.in_channel = channel * block.expansion\n\n        for _ in range(1, block_num):\n            layers.append(block(self.in_channel,\n                                channel,\n                                groups=self.groups,\n                                width_per_group=self.width_per_group))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        if self.include_top:\n            x = self.avgpool(x)\n            x = torch.flatten(x, 1)\n            x = self.fc(x)\n\n        return x\n\n\ndef resnet34(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet34-333f7ec4.pth\n    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n\n\ndef resnet50(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n\n\ndef resnet101(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)\n\n\ndef resnext50_32x4d(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\n    groups = 32\n    width_per_group = 4\n    return ResNet(Bottleneck, [3, 4, 6, 3],\n                  num_classes=num_classes,\n                  include_top=include_top,\n                  groups=groups,\n                  width_per_group=width_per_group)\n\n\ndef resnext101_32x8d(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\n    groups = 32\n    width_per_group = 8\n    return ResNet(Bottleneck, [3, 4, 23, 3],\n                  num_classes=num_classes,\n                  include_top=include_top,\n                  groups=groups,\n                  width_per_group=width_per_group)\n\n\n\n\n# 以上是神经网络结构，因为读取了模型之后代码还得知道神经网络的结构才能进行预测\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # 将代码放入GPU进行训练\nprint(\"using {} device.\".format(device))\n\nnet = resnet34()\nnet.load_state_dict(torch.load(\"Resnet.pth\", map_location=device))\nnet.to(device)\nnet.eval()  # 关闭梯度，将模型调整为测试模式\nwith torch.no_grad():  # 梯度清零\n    outputs = net(image.to(device))  # 将图片打入神经网络进行测试\n    print(net)  # 输出模型结构\n    print(outputs)  # 输出预测的张量数组\n    ans = (outputs.argmax(1)).item()  # 最大的值即为预测结果，找出最大值在数组中的序号，\n    # 对应找其在种类中的序号即可然后输出即为其种类\n    print(classes[ans])##输出的是那种即为预测结果</code></pre>\n<p><strong>代码下载链接：链接：https://pan.baidu.com/s/1el2kss9x1qiSLd4B4H3Iyw <br/> 提取码：m3zq</strong></p>\n<p><strong>有用的话麻烦点一下关注，博主后续会开源更多代码，非常感谢支持！</strong></p>\n</div>\n</div>"}