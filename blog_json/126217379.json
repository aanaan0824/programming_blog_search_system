{"blogid": "126217379", "writerAge": "码龄2年", "writerBlogNum": "179", "writerCollect": "4011", "writerComment": "3021", "writerFan": "23546", "writerGrade": "7级", "writerIntegral": "11061", "writerName": "侯小啾", "writerProfileAdress": "writer_image\\profile_126217379.jpg", "writerRankTotal": "1009", "writerRankWeekly": "23", "writerThumb": "3342", "writerVisitNum": "332466", "blog_read_count": "1871", "blog_time": "已于 2022-08-11 09:16:29 修改", "blog_title": "多元线性回归_梯度下降法实现【Python机器学习系列（六）】", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-kimbie-light\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<p><font color=\"brown\" size=\"5\">多元线性回归_梯度下降法实现【Python机器学习系列（六）】<br/> </font></p>\n<div class=\"toc\">\n<h3>文章目录</h3>\n<ul><li><a href=\"#1__14\">1. 读取数据</a></li><li><a href=\"#2_29\">2.定义代价函数</a></li><li><a href=\"#3__60\">3. 梯度下降</a></li><li><a href=\"#4_98\">4.可视化展示</a></li></ul>\n</div>\n<p></p>\n<hr/>\n<p><font size=\"5\">      ʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞ</font><br/>                  <img alt=\"在这里插入图片描述\" src=\"image\\7cc703968fc94dd7a64c66270f4a6193.png\"/><img alt=\"请添加图片描述\" src=\"https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif\"/><img alt=\"请添加图片描述\" src=\"https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif\"/><img alt=\"请添加图片描述\" src=\"https://img-blog.csdnimg.cn/cb71c1556ec1478ea93e34098bdfbfac.gif\"/><img alt=\"在这里插入图片描述\" src=\"image\\f9d8225c11fe42508a5bd938f8d99e24.png\"/><br/> <font size=\"5\">    ʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞʚʕ̯•͡˔•̯᷅ʔɞ</font></p>\n<hr/>\n<p><font color=\"purple\" size=\"4\">大家好，我是侯小啾！</font><img alt=\"在这里插入图片描述\" src=\"image\\3ed5e82a513a4a41b019a3b3d747ef7f.png\"/></p>\n<p><font color=\"brown\" size=\"4\"> <img alt=\"在这里插入图片描述\" src=\"image\\4e3c21f29fd0442a8bf8b90dcf152434.png\"/>本期blog分享的是，python实现多元线性回归的梯度下降法。</font></p>\n<hr/>\n<h1><a id=\"1__14\"></a>1. 读取数据</h1>\n<p><font color=\"purple\" size=\"4\">首先要做的就是读取数据，请自行准备一组适合做多元回归的数据即可。这里以<code>data.csv</code>为例，这里做的是二元回归。导入相关库，及相关代码如下。</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n<span class=\"token keyword\">from</span> mpl_toolkits<span class=\"token punctuation\">.</span>mplot3d <span class=\"token keyword\">import</span> Axes3D\n\n\ndata <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>loadtxt<span class=\"token punctuation\">(</span><span class=\"token string\">\"data.csv\"</span><span class=\"token punctuation\">,</span> delimiter<span class=\"token operator\">=</span><span class=\"token string\">\",\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 提取特征数据与标签</span>\nx_data <span class=\"token operator\">=</span> data<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">:</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\ny_data <span class=\"token operator\">=</span> data<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n</code></pre>\n<hr/>\n<h1><a id=\"2_29\"></a>2.定义代价函数</h1>\n<p><font size=\"4\">回归模型形如：<br/>  <br/> <font size=\"5\">      <span class=\"katex--inline\"><span class=\"katex\"><span class=\"katex-mathml\">\n      \n       \n        \n         \n          h\n         \n         \n          (\n         \n         \n          x\n         \n         \n          )\n         \n         \n          =\n         \n         \n          t\n         \n         \n          h\n         \n         \n          e\n         \n         \n          t\n         \n         \n          a\n         \n         \n          0\n         \n         \n          +\n         \n         \n          t\n         \n         \n          h\n         \n         \n          e\n         \n         \n          t\n         \n         \n          a\n         \n         \n          1\n         \n         \n          ∗\n         \n         \n          x\n         \n         \n          1\n         \n         \n          +\n         \n         \n          t\n         \n         \n          h\n         \n         \n          e\n         \n         \n          t\n         \n         \n          a\n         \n         \n          2\n         \n         \n          ∗\n         \n         \n          x\n         \n         \n          2\n         \n        \n        \n         h(x)=theta0 + theta1*x1 + theta2*x2\n        \n       \n      </span><span class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mord mathnormal\">h</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right: 0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.7778em; vertical-align: -0.0833em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">a</span><span class=\"mord\">0</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">a</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.7278em; vertical-align: -0.0833em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">a</span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.6444em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mord\">2</span></span></span></span></span></font></font></p>\n<p><font color=\"brown\" size=\"4\">接下来我们需要初始化相关参数，并定义出代价函数。因为存在多个系数参数，这里代价函数的写法与一元回归时的情况略有不同，稍微有所调整。具体如下：</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token comment\"># 初始化一系列参数</span>\n<span class=\"token comment\"># 截距</span>\ntheta0 <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n<span class=\"token comment\"># 系数</span>\ntheta1 <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\ntheta2 <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n<span class=\"token comment\"># 学习率</span>\nlearning_rate <span class=\"token operator\">=</span> <span class=\"token number\">0.0001</span>\n<span class=\"token comment\"># 初始化迭代次数</span>\nn_iterables <span class=\"token operator\">=</span> <span class=\"token number\">1000</span>\n\n\n<span class=\"token comment\"># 定义代价函数（损失函数）</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">compute_mse</span><span class=\"token punctuation\">(</span>theta0<span class=\"token punctuation\">,</span> theta1<span class=\"token punctuation\">,</span> theta2<span class=\"token punctuation\">,</span> x_data<span class=\"token punctuation\">,</span> y_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    total_error <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># 计算损失 真实值:y_data  预测值h(x)=theta0 + theta1*x1 + theta2*x2</span>\n        total_error <span class=\"token operator\">+=</span> <span class=\"token punctuation\">(</span>y_data<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">-</span> <span class=\"token punctuation\">(</span>theta0 <span class=\"token operator\">+</span> theta1 <span class=\"token operator\">*</span> x_data<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> theta2 <span class=\"token operator\">*</span> x_data<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">**</span> <span class=\"token number\">2</span>\n\n    mse_ <span class=\"token operator\">=</span> total_error <span class=\"token operator\">/</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> <span class=\"token number\">2</span>\n    <span class=\"token keyword\">return</span> mse_\n</code></pre>\n<hr/>\n<h1><a id=\"3__60\"></a>3. 梯度下降</h1>\n<p><font color=\"brown\" size=\"4\"> 多元回归的梯度下降与一元回归的差不多，在一元回归中只需要求一个导数，而现在求多个偏导数。代码过程如下：</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">gradient_descent</span><span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">,</span> y_data<span class=\"token punctuation\">,</span> theta0<span class=\"token punctuation\">,</span> theta1<span class=\"token punctuation\">,</span> theta2<span class=\"token punctuation\">,</span> learning_rate<span class=\"token punctuation\">,</span> n_iterables<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    m <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># 循环 --&gt; 迭代次数</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>n_iterables<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># 初始化 theta0 theta1 theta2 的偏导值</span>\n        theta0_grad <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n        theta1_grad <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n        theta2_grad <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n        <span class=\"token comment\"># 计算偏导的总和再平均</span>\n        <span class=\"token comment\"># 遍历m次</span>\n        <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            theta0_grad <span class=\"token operator\">+=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">/</span> m<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>theta1 <span class=\"token operator\">*</span> x_data<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> theta2 <span class=\"token operator\">*</span> x_data<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> theta0<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> y_data<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            theta1_grad <span class=\"token operator\">+=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">/</span> m<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>theta1 <span class=\"token operator\">*</span> x_data<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> theta2 <span class=\"token operator\">*</span> x_data<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> theta0<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> y_data<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> x_data<span class=\"token punctuation\">[</span>\n                j<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n            theta2_grad <span class=\"token operator\">+=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">/</span> m<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>theta1 <span class=\"token operator\">*</span> x_data<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> theta2 <span class=\"token operator\">*</span> x_data<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> theta0<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> y_data<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> x_data<span class=\"token punctuation\">[</span>\n                j<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n\n        <span class=\"token comment\"># 更新theta</span>\n        theta0 <span class=\"token operator\">=</span> theta0 <span class=\"token operator\">-</span> <span class=\"token punctuation\">(</span>learning_rate <span class=\"token operator\">*</span> theta0_grad<span class=\"token punctuation\">)</span>\n        theta1 <span class=\"token operator\">=</span> theta1 <span class=\"token operator\">-</span> <span class=\"token punctuation\">(</span>learning_rate <span class=\"token operator\">*</span> theta1_grad<span class=\"token punctuation\">)</span>\n        theta2 <span class=\"token operator\">=</span> theta2 <span class=\"token operator\">-</span> <span class=\"token punctuation\">(</span>learning_rate <span class=\"token operator\">*</span> theta2_grad<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> theta0<span class=\"token punctuation\">,</span> theta1<span class=\"token punctuation\">,</span> theta2\n\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"开始：截距theta0=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{<!-- --></span>theta0<span class=\"token punctuation\">}</span></span><span class=\"token string\">,theta1=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{<!-- --></span>theta1<span class=\"token punctuation\">}</span></span><span class=\"token string\">,theta2=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{<!-- --></span>theta2<span class=\"token punctuation\">}</span></span><span class=\"token string\">,损失=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{<!-- --></span>compute_mse<span class=\"token punctuation\">(</span>theta0<span class=\"token punctuation\">,</span>theta1<span class=\"token punctuation\">,</span>theta2<span class=\"token punctuation\">,</span>x_data<span class=\"token punctuation\">,</span>y_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"开始运行\"</span><span class=\"token punctuation\">)</span>\ntheta0<span class=\"token punctuation\">,</span>theta1<span class=\"token punctuation\">,</span>theta2 <span class=\"token operator\">=</span> gradient_descent<span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">,</span>y_data<span class=\"token punctuation\">,</span>theta0<span class=\"token punctuation\">,</span>theta1<span class=\"token punctuation\">,</span>theta2<span class=\"token punctuation\">,</span>learning_rate<span class=\"token punctuation\">,</span>n_iterables<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"迭代</span><span class=\"token interpolation\"><span class=\"token punctuation\">{<!-- --></span>n_iterables<span class=\"token punctuation\">}</span></span><span class=\"token string\">次后：截距theta0=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{<!-- --></span>theta0<span class=\"token punctuation\">}</span></span><span class=\"token string\">,theta1=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{<!-- --></span>theta1<span class=\"token punctuation\">}</span></span><span class=\"token string\">,theta2=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{<!-- --></span>theta2<span class=\"token punctuation\">}</span></span><span class=\"token string\">,损失=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{<!-- --></span>compute_mse<span class=\"token punctuation\">(</span>theta0<span class=\"token punctuation\">,</span>theta1<span class=\"token punctuation\">,</span>theta2<span class=\"token punctuation\">,</span>x_data<span class=\"token punctuation\">,</span>y_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n</code></pre>\n<p><font color=\"brown\" size=\"4\">执行结果输出如下：<br/> <img alt=\"在这里插入图片描述\" src=\"image\\0dc1d8e37df7420a9f870ecd377e21ed.png\"/><br/> <font color=\"brown\" size=\"4\">1000次迭代之后，损失值由23.64变为0.3865。</font></font></p>\n<hr/>\n<h1><a id=\"4_98\"></a>4.可视化展示</h1>\n<p><font color=\"brown\" size=\"4\">可视化展示常常作为机器学习过程的补充，可以使得机器学习的效果更为生动，直观。</font></p>\n<pre><code class=\"prism language-python\"><span class=\"token comment\"># 可视化散点分布</span>\nfig <span class=\"token operator\">=</span> plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nax <span class=\"token operator\">=</span> Axes3D<span class=\"token punctuation\">(</span>fig<span class=\"token punctuation\">)</span>\nax<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>x_data<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>y_data<span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\"># 可视化散点分布</span>\nfig <span class=\"token operator\">=</span> plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nax <span class=\"token operator\">=</span> Axes3D<span class=\"token punctuation\">(</span>fig<span class=\"token punctuation\">)</span>\nax<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>x_data<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>y_data<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 绘制预期平面</span>\n<span class=\"token comment\"># 构建x</span>\nx_0 <span class=\"token operator\">=</span> x_data<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\nx_1 <span class=\"token operator\">=</span> x_data<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># 生成网格矩阵</span>\nx_0<span class=\"token punctuation\">,</span>x_1 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>meshgrid<span class=\"token punctuation\">(</span>x_0<span class=\"token punctuation\">,</span>x_1<span class=\"token punctuation\">)</span>\n\ny_hat <span class=\"token operator\">=</span> theta0 <span class=\"token operator\">+</span> theta1<span class=\"token operator\">*</span>x_0 <span class=\"token operator\">+</span> theta2<span class=\"token operator\">*</span>x_1\n\n<span class=\"token comment\"># 绘制3D图</span>\nax<span class=\"token punctuation\">.</span>plot_surface<span class=\"token punctuation\">(</span>x_0<span class=\"token punctuation\">,</span>x_1<span class=\"token punctuation\">,</span>y_hat<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 设置标签</span>\nax<span class=\"token punctuation\">.</span>set_xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">\"Miles\"</span><span class=\"token punctuation\">)</span>\nax<span class=\"token punctuation\">.</span>set_ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">\"nums\"</span><span class=\"token punctuation\">)</span>\nax<span class=\"token punctuation\">.</span>set_zlabel<span class=\"token punctuation\">(</span><span class=\"token string\">\"Time\"</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n</code></pre>\n<p>散点图输出如下：<br/>         <img alt=\"在这里插入图片描述\" src=\"image\\902a21b8100345bba324e88d78aa86b4.png\"/></p>\n<p><font size=\"4\">加上拟合回归面后如图所示：<br/>         <img alt=\"在这里插入图片描述\" src=\"image\\29133f314cd54942b89825dbec7eff32.png\"/></font></p>\n<hr/>\n<p><font color=\"red\" size=\"5\">本次分享就到这里，小啾感谢您的关注与支持！<br/> 🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ🌹꧔ꦿ</font></p>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>"}