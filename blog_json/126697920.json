{"blogid": "126697920", "writerAge": "码龄5年", "writerBlogNum": "39", "writerCollect": "19", "writerComment": "9", "writerFan": "12", "writerGrade": "3级", "writerIntegral": "641", "writerName": "李甜甜~", "writerProfileAdress": "writer_image\\profile_126697920.jpg", "writerRankTotal": "1678641", "writerRankWeekly": "241940", "writerThumb": "12", "writerVisitNum": "31265", "blog_read_count": "205", "blog_time": "已于 2022-09-05 10:17:52 修改", "blog_title": "Scrapy + selenium + 超级鹰验证码识别爬取网站", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<h1><a id=\"_0\"></a></h1>\n<p></p>\n<div>\n<p id=\"main-toc\"><strong>目录</strong></p>\n<p id=\"%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85Scrapy-toc\" style=\"margin-left:0px;\"><a href=\"#%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85Scrapy\">一、安装Scrapy</a></p>\n<p id=\"%E4%BA%8C%E3%80%81%E4%BD%BF%E7%94%A8%E6%AD%A5%E9%AA%A4-toc\" style=\"margin-left:0px;\"><a href=\"#%E4%BA%8C%E3%80%81%E4%BD%BF%E7%94%A8%E6%AD%A5%E9%AA%A4\">二、Scrapy项目生成</a></p>\n<p id=\"%E4%B8%89%E3%80%81%E7%88%AC%E5%8F%96%E6%9F%90%E4%B8%AA%E7%BD%91%E7%AB%99%EF%BC%88%E4%BB%A5%E4%B8%8B%E6%88%91%E7%94%A8%E4%B9%8B%E5%89%8D%E7%9A%84%E5%88%9B%E5%BB%BA%E7%9A%84%E9%A1%B9%E7%9B%AE%EF%BC%8C%E4%B8%8D%E6%98%AF%E5%88%9A%E5%88%9A%E6%96%B0%E5%88%9B%E7%9A%84%EF%BC%89-toc\" style=\"margin-left:0px;\"><a href=\"#%E4%B8%89%E3%80%81%E7%88%AC%E5%8F%96%E6%9F%90%E4%B8%AA%E7%BD%91%E7%AB%99%EF%BC%88%E4%BB%A5%E4%B8%8B%E6%88%91%E7%94%A8%E4%B9%8B%E5%89%8D%E7%9A%84%E5%88%9B%E5%BB%BA%E7%9A%84%E9%A1%B9%E7%9B%AE%EF%BC%8C%E4%B8%8D%E6%98%AF%E5%88%9A%E5%88%9A%E6%96%B0%E5%88%9B%E7%9A%84%EF%BC%89\">三、爬取某个网站（以下我用之前的创建的项目，不是刚刚新创的）</a></p>\n<p id=\"1.%E5%BC%95%E5%85%A5%E5%BA%93-toc\" style=\"margin-left:40px;\"></p>\n<p id=\"%E6%80%BB%E7%BB%93-toc\" style=\"margin-left:0px;\"></p>\n<hr id=\"hr-toc\"/>\n<p></p>\n</div>\n<h1 id=\"%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85Scrapy\">一、安装Scrapy</h1>\n<h3>1，window安装</h3>\n<pre><code>pip install Scrapy</code></pre>\n<h3>2，安装selenium</h3>\n<pre><code>pip install selenium</code></pre>\n<h3>3，下载Chrome驱动</h3>\n<p>         a，查看Google Chrome浏览器版本    </p>\n<p>                Chrome驱动下载地址http://chromedriver.storage.googleapis.com/index.html</p>\n<p><img alt=\"\" height=\"205\" src=\"image\\ca4ce92a451c47a8940468169b07e0d3.png\" width=\"1012\"/></p>\n<p>         b，找到和你版本最接近的哪个安装包</p>\n<p> <img alt=\"\" height=\"84\" src=\"image\\3f81e44431494bb28c7a4015d8923930.png\" width=\"486\"/></p>\n<p>         c，下载好之后将我们的chromedriver放到和我们python安装路径相同的目录下</p>\n<p><img alt=\"\" height=\"641\" src=\"image\\1c972796ad7f409e8ca57696d2bd8272.png\" width=\"1200\"/></p>\n<p>         d，配置环境变量</p>\n<p><img alt=\"\" height=\"287\" src=\"image\\d064b629bebf47739fadb6a562e61bab.png\" width=\"597\"/></p>\n<h3> 4，超级鹰验证码识别</h3>\n<p>        a，超级鹰官网 <a href=\"https://www.chaojiying.com/\" title=\"https://www.chaojiying.com/\">https://www.chaojiying.com/</a></p>\n<p>        b，注册，登入</p>\n<p>        c，生成软件id</p>\n<p>        d，下载，放置到爬虫工程目录下</p>\n<p><img alt=\"\" height=\"1042\" src=\"image\\443bdf35e3f040fe8e22f95c478c659a.png\" width=\"1200\"/></p>\n<p><img alt=\"\" height=\"686\" src=\"image\\a7a45e7c63b94e4f8075c6187031b790.png\" width=\"1200\"/></p>\n<p><img alt=\"\" height=\"377\" src=\"image\\ee5096b8e5c844ca83a145e9888852d0.png\" width=\"796\"/></p>\n<p></p>\n<p></p>\n<p></p>\n<h1 id=\"%E4%BA%8C%E3%80%81%E4%BD%BF%E7%94%A8%E6%AD%A5%E9%AA%A4\"><a id=\"_26\"></a>二、Scrapy项目生成</h1>\n<h3>1，win + R</h3>\n<p><img alt=\"\" height=\"324\" src=\"image\\c0a2886c101242b6b73b6d028d30b95e.png\" width=\"628\"/></p>\n<h3>2，输入命令</h3>\n<pre><code>​​# 切换到自己想要的路径 cd C:\\Users\\（用户名）\\Desktop\\spider\n# 创建工程  scrapy startproject （项目名）\n# 切换到新创建的文件夹 cd hellospider\n# 创建爬虫项目 scrapy genspider (爬虫名) (爬取网址的域名)\n</code></pre>\n<p><img alt=\"\" height=\"82\" src=\"image\\a752ec9072434e5ea1696e9fed41d3c2.png\" width=\"864\"/></p>\n<p><img alt=\"\" height=\"273\" src=\"image\\ceff7000074d4a35b70e55f0f14207ba.png\" width=\"1200\"/><img alt=\"\" height=\"72\" src=\"image\\e4333f4bbc4448b786c43e7345c844d0.png\" width=\"1154\"/></p>\n<h3> 3，使用pycharm打开​​​​​​​<img alt=\"\" height=\"809\" src=\"image\\91a385bbce454f5490f1b92540ca5147.png\" width=\"1200\"/></h3>\n<h3>4， 修改为虚拟环境()</h3>\n<p>file-&gt;setting</p>\n<p><img alt=\"\" height=\"688\" src=\"image\\dadc7fc896354bb2a6d6a28d875cd65b.png\" width=\"1200\"/></p>\n<p> pycharm里面的命令行，再次安装scrapy，selenium<img alt=\"\" height=\"401\" src=\"image\\ac20be12b7bc4084ae64f7a62193b2b0.png\" width=\"1090\"/></p>\n<p></p>\n<h1 id=\"%E4%B8%89%E3%80%81%E7%88%AC%E5%8F%96%E6%9F%90%E4%B8%AA%E7%BD%91%E7%AB%99%EF%BC%88%E4%BB%A5%E4%B8%8B%E6%88%91%E7%94%A8%E4%B9%8B%E5%89%8D%E7%9A%84%E5%88%9B%E5%BB%BA%E7%9A%84%E9%A1%B9%E7%9B%AE%EF%BC%8C%E4%B8%8D%E6%98%AF%E5%88%9A%E5%88%9A%E6%96%B0%E5%88%9B%E7%9A%84%EF%BC%89\">三、爬取某个网站（以下我用之前的创建的项目，不是刚刚新创的）</h1>\n<h3>1，修改setting</h3>\n<pre><code># 修改机器人协议 \nROBOTSTXT_OBEY = False\n# 下载时间间隙\nDOWNLOAD_DELAY = 1\n# 启用后，当从相同的网站获取数据时，Scrapy将会等待一个随机的值，延迟时间为0.5到1.5之间的一个随机值乘以DOWNLOAD_DELAY\nRANDOMIZE_DOWNLOAD_DELAY=True\n# 若是请求时间超过60秒，就会报异常，异常机制是会再次发起请求的\nDOWNLOAD_TIMEOUT = 60\n# 设置请求头\nDEFAULT_REQUEST_HEADERS = {\n  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n  'Accept-Language': 'en',\n  'User-Agent':'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'\n}\n# 打开一个管道\nITEM_PIPELINES = {\n   # '项目名称.pipelines.管道名': 300,\n     'chuxiongfilespider.pipelines.ChuxiongfilespiderPipeline': 300,\n}</code></pre>\n<h3> 2，items.py文件</h3>\n<p>定义需要的字段</p>\n<p><img alt=\"\" height=\"341\" src=\"image\\a8332d9196db4dda9a7efd36a6c64424.png\" width=\"621\"/></p>\n<h3>3，写爬虫文件 chuxiongfile.py</h3>\n<pre><code class=\"language-python\">import copy\nfrom datetime import time\n\nimport scrapy\nfrom pymysql.converters import escape_string\nfrom scrapy.http import HtmlResponse\nfrom selenium.common import NoSuchElementException\nfrom selenium.webdriver import Chrome\nfrom selenium.webdriver.common.by import By\n\nfrom chuxiongfilespider.items import ChuxiongfilespiderItem\nfrom chuxiongfilespider.spiders.chaojiying import Chaojiying_Client\n\nimport uuid\n\n\nclass ChuxiongfileSpider(scrapy.Spider):\n    name = 'chuxiongfile'\n    allowed_domains = ['网址']\n    start_urls = [\n        '爬取的网址']\n    page = 1\n\n    def start_requests(self):\n        web = Chrome()\n        web.get(self.start_urls[0])\n        try:\n            # selenium版本更新，原find_element_by_xpath需要改写，并导By包\n            web.find_element(By.XPATH, '/html/body/form/div/img')\n            # screenshot_as_png当前窗口的屏幕快照保存为二进制数据\n            img = web.find_element(By.XPATH, '/html/body/form/div/img').screenshot_as_png\n\n            # 超级鹰处理验证码\n            chaojiying = Chaojiying_Client('超级鹰登入账号', '超级鹰登入密码', '软件id')\n            # 1902处理验证码类型\n            dic = chaojiying.PostPic(img, 1902)\n            verify_code = dic['pic_str']\n            # 填写验证码\n            web.find_element(By.XPATH, '//*[@id=\"visitcode\"]').send_keys(verify_code)\n            # 点击确定\n            time.sleep(2)\n            web.find_element(By.XPATH, '/html/body/form/div/input[4]').click()\n            # 获取验证码输入后的cookie\n            cookies_dict = {cookie['name']: cookie['value'] for cookie in web.get_cookies()}\n            web.close()\n            yield scrapy.Request(url=self.start_urls[0], cookies=cookies_dict, callback=self.parse)\n        except NoSuchElementException:\n            yield scrapy.Request(url=self.start_urls[0], callback=self.parse)\n\n    def parse(self, response: HtmlResponse, **kwargs):\n        items = ChuxiongfilespiderItem()\n        for item in response.css('.tml'):\n            items['name'] = item.css('.tcc a::text').extract()[0]\n            items['policy_id'] = ''.join(str(uuid.uuid5(uuid.NAMESPACE_DNS, items['name'])).split('-'))\n            items['attachment_id'] = '123'\n            items['url'] = response.urljoin(item.css('.tcc a::attr(href)').extract_first())\n            if item.css('.d a::attr(href)').extract_first() == '':\n                items['attachment_url'] = '无下载选项'\n            else:\n                items['attachment_url'] = response.urljoin(item.css('.d a::attr(href)').extract_first())\n            items['netloc'] = '网址'\n            yield scrapy.Request(url=items['url'], callback=self.get_details, meta={\"items\": copy.deepcopy(items)})\n\n    def get_details(self, response):\n        items = response.meta['items']\n        items['content'] =escape_string(\" \".join(response.css('.xzgfwrap').getall()))\n        yield items\n        if self.page &lt; 2:\n            self.page += 1\n            url = f'http://（网址）?totalpage=3&amp;PAGENUM={str(self.page)}&amp;urltype' \\\n                  f'=tree.TreeTempUrl&amp;wbtreeid=3494'\n            yield scrapy.Request(url=url, callback=self.parse)  # 使用callback进行回调\n</code></pre>\n<h3> 4，存储到数据库 pipelines.py</h3>\n<pre><code class=\"language-python\"># Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n\n\n# useful for handling different item types with a single interface\nfrom itemadapter import ItemAdapter\nimport pymysql\n\nclass ChuxiongfilespiderPipeline(object):\n    mysql = None\n    cursor = None  # 执行SQL语句返回游标接口\n\n    def open_spider(self, spider):\n        self.mysql = pymysql.Connect(host='localhost', user='数据库用户名', password='数据库用户密码', port=3306, charset='utf8',\n                                     database='库名')\n        self.cursor = self.mysql.cursor()\n\n    def process_item(self, items, spider):\n        # 创建表\n        table = 'create table if not exists cx_other(' \\\n                'id int not null primary key auto_increment' \\\n                ',policy_id varchar(100)' \\\n                ',url varchar(1000)' \\\n                ',attachment_id varchar(100)' \\\n                ',attachment_url varchar(100)' \\\n                ',name varchar(150)' \\\n                ',netloc varchar(50)' \\\n                ');'\n        table_1 = 'create table if not exists cx_other_content(' \\\n                  'id int not null primary key auto_increment' \\\n                  ',policy_id varchar(100)' \\\n                  ',content MEDIUMTEXT NOT NULL' \\\n                  ');'\n\n        insert = 'insert into cx_other(policy_id,url,attachment_id,attachment_url,name,netloc) ' \\\n                 'values(\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\")' \\\n                 % (items['policy_id'], items['url'], items['attachment_id'], items['attachment_url'], items['name'], items['netloc'])\n        insert_1 = 'insert into cx_other_content(policy_id,content) values(\"%s\",\"%s\")' % (\n            items['policy_id'], items['content'])\n\n        try:\n            # 数据库断开后重连\n            self.mysql.ping(reconnect=True)\n            # 创建表\n            self.cursor.execute(table)\n            self.cursor.execute(table_1)\n            # 插入数据\n            self.cursor.execute(insert)\n            self.cursor.execute(insert_1)\n            self.mysql.commit()\n            print('===============插入数据成功===============')\n        except Exception as e:\n            print('===============插入数据失败===============', e)\n            self.mysql.rollback()\n        return items\n\n    def close_spider(self, spider):\n        self.cursor.close()\n        self.mysql.close()</code></pre>\n<h2 id=\"1.%E5%BC%95%E5%85%A5%E5%BA%93\"><a id=\"1_27\"></a></h2>\n<h1 id=\"%E6%80%BB%E7%BB%93\"><a id=\"_55\"></a></h1>\n</div>\n</div>"}