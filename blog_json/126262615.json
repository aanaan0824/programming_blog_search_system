{"blogid": "126262615", "writerAge": "None", "writerBlogNum": "856", "writerCollect": "1402", "writerComment": "288", "writerFan": "792", "writerGrade": "7级", "writerIntegral": "10008", "writerName": "格格巫 MMQ!!", "writerProfileAdress": "writer_image\\profile_126262615.jpg", "writerRankTotal": "1427", "writerRankWeekly": "1993", "writerThumb": "319", "writerVisitNum": "867764", "blog_read_count": "1543", "blog_time": "于 2022-08-10 11:16:55 发布", "blog_title": "教你自己搭建一个IP池(绝对超好用！！！！)", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-atom-one-light\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<p>随着我们爬虫的速度越来越快，很多时候，有人发现，数据爬不了啦，打印出来一看。</p>\n<p>不返回数据，而且还甩一句话<br/> <img alt=\"在这里插入图片描述\" src=\"image\\c8ec302659cb494896ed736d7be6b79b.png\"/></p>\n<p>是不是很熟悉啊？</p>\n<p>要想想看，人是怎么访问网站的？ 发请求，对，那么就会带有</p>\n<p>request.headers，</p>\n<p>那么当你疯狂请求别人的网站时候，人家网站的管理人员就会 觉得有点不对劲了，</p>\n<p>他看看请求的 header 信息，一看吓一跳，结果看到的 headers 信息是这样的：</p>\n<p>Host: 127.0.0.1:3369<br/> User-Agent: python-requests/3.21.0<br/> Accept-Encoding: gzip, deflate<br/> Accept: <em>/</em><br/> Connection: keep-alive</p>\n<p>看到:</p>\n<p>User-Agent: python-requests/3.21.0</p>\n<p>居然使用 python 的库来请求，说明你已经暴露了，人家不封你才怪呢？</p>\n<p>那么怎么办呢？伪装自己呗。</p>\n<p>python 不可以伪装，浏览器可以伪装，所以可以修改浏览器的请求头。</p>\n<p>简单来说，就是让自己的 python 爬虫假装是浏览器。</p>\n<p>伪装 Header的哪个地方？<br/> 要让自己的 python 爬虫假装是浏览器，我们要伪装headers，那么headers里面有很多字段，我们主要注意那几个呢？<br/> headers数据通常用这两个即可，强烈推荐在爬虫中为每个request都配个user-agent，而’Referer’如果需要就加，不需要就不用。（Referer是什么？后面补充知识点）</p>\n<p>图示：<br/> <img alt=\"在这里插入图片描述\" src=\"image\\c38a5f881dd8417bb3dc2517bab85bab.png\"/></p>\n<p>上面几个重要点解释如下：</p>\n<p>Requests Headers：<br/> • “吾是人！”——修改user-agent：里面储存的是系统和浏览器的型号版本，通过修改它来假装自己是人。</p>\n<p>• “我从台湾省来”——修改referer：告诉服务器你是通过哪个网址点进来的而不是凭空出现的，有些网站会检查。</p>\n<p>• “饼干！”：——带上cookie，有时带不带饼干得到的结果是不同的，试着带饼干去“贿赂”服务器让她给你完整的信息。<br/> 3.headers的伪装—随机User-Agent<br/> 爬虫机制：很多网站都会对Headers的User-Agent进行检测，还有一部分网站会对Referer进行检测（一些资源网站的防盗链就是检测Referer）</p>\n<p>随机User-Agent生成 ：生成一个随机的User-Agent，这样你就可以是很多不同的浏览器模样。</p>\n<p>（代码现成的，复制拿去用即可）</p>\n<p>#!/usr/bin/python3</p>\n<p>#@Readme : 反爬之headers的伪装</p>\n<h1><a id=\"Headers_59\"></a>对于检测Headers的反爬虫</h1>\n<p>from fake_useragent import UserAgent # 下载：pip install fake-useragent</p>\n<p>ua = UserAgent() # 实例化，需要联网但是网站不太稳定-可能耗时会长一些</p>\n<h1><a id=\"1_64\"></a>1.生成指定浏览器的请求头</h1>\n<p>print(ua.ie)<br/> print(ua.opera)<br/> print(ua.chrome)<br/> print(ua.google)<br/> print(ua.firefox)<br/> print(ua.safari)</p>\n<h1><a id=\"UserAgent_71\"></a>随机打印一个浏览器的User-Agent</h1>\n<p>print(ua.random)<br/> print(‘完毕。’)</p>\n<h1><a id=\"2uarandom_75\"></a>2.在工作中常用的则是ua.random方式</h1>\n<p>import requests<br/> ua = UserAgent()<br/> print(ua.random) # 随机产生</p>\n<p>headers = {<!-- --><br/> ‘User-Agent’: ua.random # 伪装<br/> }</p>\n<h1><a id=\"_84\"></a>请求</h1>\n<p>url = ‘https://www.baidu.com/’<br/> response = requests.get(url, headers=headers)<br/> print(response.status_code)</p>\n<p>Referer的伪装：</p>\n<p>如果想爬图片，图片反盗链的话就要用到Referer了。<br/> headers = {‘User-Agent’:ua.random,‘Referer’:‘这里放入图片的主页面’}<br/> 如果遇到防盗链的图片，一般思路就是先爬到所有图片的地址.jpg —–&gt;将它们储存在列表中 —–&gt;遍历访问图片地址，然后用 ‘wb’的格式打开文件写入，文件名根据图片地址动态改变。</p>\n<p>这个基本上如果你的爬虫对象不是很严肃的图片网站，都不会用到。</p>\n<p>4.2.1 自建的IP代理池<br/> 多线程爬虫<br/> 就是自己去收集网上公开的免费ip，自建起 自己的ip代理池。<br/> 就是通过 python 程序去抓取网上大量免费的代理 ip ， 然后定时的去检测这些 ip 可不可以用，那么下次你要使用代理 ip 的时候，你只需要去自己的 ip 代理池里面拿就行了。<br/> 简单来说：访问免费代理的网站 —&gt; 正则/xpath提取 ip和端口—&gt; 测试ip是否可用 》》可用则保存 》》使用ip爬虫 &gt; 过期，抛弃ip。</p>\n<p>这个过程可以使用多线程或异步的方式，因为检测代理是个很慢的过程。</p>\n<p>这是来源于网络的一个西刺代理的多线程ip代理爬虫：（我不用）</p>\n<p>#!/usr/bin/python3</p>\n<p>#@Readme : IP代理==模拟一个ip地址去访问某个网站（爬的次数太多，ip被屏蔽）</p>\n<h1><a id=\"ip_111\"></a>多线程的方式构造ip代理池。</h1>\n<p>from bs4 import BeautifulSoup<br/> import requests<br/> from urllib import request, error<br/> import threading</p>\n<p>import os<br/> from fake_useragent import UserAgent</p>\n<p>inFile = open(‘proxy.txt’) # 存放爬虫下来的ip<br/> verifiedtxt = open(‘verified.txt’) # 存放已证实的可用的ip</p>\n<p>lock = threading.Lock()</p>\n<p>def getProxy(url):<br/> # 打开我们创建的txt文件<br/> proxyFile = open(‘proxy.txt’, ‘a’)</p>\n<pre><code># 伪装\nua = UserAgent()\nheaders = {\n    'User-Agent': ua.random\n}\n\n# page是我们需要获取多少页的ip，这里我们获取到第９页\nfor page in range(1, 10):\n    # 通过观察ＵＲＬ，我们发现原网址+页码就是我们需要的网址了，这里的page需要转换成str类型\n    urls = url + str(page)\n    # 通过requests来获取网页源码\n    rsp = requests.get(urls, headers=headers)\n    html = rsp.text\n    # 通过BeautifulSoup，来解析html页面\n    soup = BeautifulSoup(html,'html.parser')\n    # 通过分析我们发现数据在　id为ip_list的table标签中的tr标签中\n    trs = soup.find('table', id='ip_list').find_all('tr')  # 这里获得的是一个list列表\n    # 我们循环这个列表\n    for item in trs[1:]:\n        # 并至少出每个tr中的所有td标签\n        tds = item.find_all('td')\n        # 我们会发现有些img标签里面是空的，所以这里我们需要加一个判断\n        if tds[0].find('img') is None:\n            nation = '未知'\n            locate = '未知'\n        else:\n            nation = tds[0].find('img')['alt'].strip()\n            locate = tds[3].text.strip()\n        # 通过td列表里面的数据，我们分别把它们提取出来\n        ip = tds[1].text.strip()\n        port = tds[2].text.strip()\n        anony = tds[4].text.strip()\n        protocol = tds[5].text.strip()\n        speed = tds[6].find('div')['title'].strip()\n        time = tds[8].text.strip()\n        # 将获取到的数据按照规定格式写入txt文本中，这样方便我们获取\n        proxyFile.write('%s|%s|%s|%s|%s|%s|%s|%s\\n' % (nation, ip, port, locate, anony, protocol, speed, time))\n</code></pre>\n<p>def verifyProxyList():<br/> verifiedFile = open(‘verified.txt’, ‘a’)</p>\n<pre><code>while True:\n    lock.acquire()\n    ll = inFile.readline().strip()\n    lock.release()\n    if len(ll) == 0: break\n    line = ll.strip().split('|')\n    ip = line[1]\n    port = line[2]\n    realip = ip + ':' + port\n    code = verifyProxy(realip)\n    if code == 200:\n        lock.acquire()\n        print(\"---Success成功:\" + ip + \":\" + port)\n        verifiedFile.write(ll + \"\\n\")\n        lock.release()\n    else:\n        print(\"---Failure失败:\" + ip + \":\" + port)\n</code></pre>\n<p>def verifyProxy(ip):<br/> ‘’’<br/> 验证代理的有效性<br/> ‘’’<br/> ua = UserAgent()<br/> requestHeader = {<!-- --><br/> ‘User-Agent’: ua.random<br/> }<br/> url = “http://www.baidu.com”<br/> # 填写代理地址<br/> proxy = {‘http’: ip}<br/> # 创建proxyHandler<br/> proxy_handler = request.ProxyHandler(proxy)<br/> # 创建ｏｐｅｎｅｒ<br/> proxy_opener = request.build_opener(proxy_handler)<br/> # 安装opener<br/> request.install_opener(proxy_opener)</p>\n<pre><code>try:\n    req = request.Request(url, headers=requestHeader)\n    rsq = request.urlopen(req, timeout=5.0)\n    code = rsq.getcode()\n    return code\nexcept error.URLError as e:\n    return e\n</code></pre>\n<p>if <strong>name</strong> == ‘<strong>main</strong>’:<br/> # 手动新建两个文件<br/> filename = ‘proxy.txt’<br/> filename2 = ‘verified.txt’<br/> if not os.path.isfile(filename):<br/> inFile = open(filename, mode=“w”, encoding=“utf-8”)<br/> if not os.path.isfile(filename2):<br/> verifiedtxt = open(filename2, mode=“w”, encoding=“utf-8”)<br/> tmp = open(‘proxy.txt’, ‘w’)<br/> tmp.write(“”)<br/> tmp.close()<br/> tmp1 = open(‘verified.txt’, ‘w’)<br/> tmp1.write(“”)<br/> tmp1.close()<br/> # 多线程爬虫西刺代理网，找可用ip<br/> getProxy(“http://www.xicidaili.com/nn/”)<br/> getProxy(“http://www.xicidaili.com/nt/”)<br/> getProxy(“http://www.xicidaili.com/wn/”)<br/> getProxy(“http://www.xicidaili.com/wt/”)</p>\n<pre><code>all_thread = []\nfor i in range(30):\n    t = threading.Thread(target=verifyProxyList)\n    all_thread.append(t)\n    t.start()\n\nfor t in all_thread:\n    t.join()\n\ninFile.close()\nverifiedtxt.close()\n</code></pre>\n<p>运行一下，效果：<br/> <img alt=\"在这里插入图片描述\" src=\"image\\6d72f0c2c19e42ddb9f52d48147b4369.png\"/></p>\n<p>爬出来的可用的很少或者很短：</p>\n<p>重点来了！！！！！！！！！<br/> 4.2.3 开源 ip代理池—ProxyPool（吐血推荐）<br/> 类比线程池，进程池，懂了吧？<br/> 这是俺发现的一个不错的开源 ip 代理池ProxyPool，可以用windows系统的，至少Python3.5以上环境哟，还需要将Redis服务开启。</p>\n<p>现成的代理池，还不用起来？</p>\n<p>ProxyPool下载地址：</p>\n<p>https://github.com/Python3WebSpider/ProxyPool.git</p>\n<p>（可以手动下载也可以使用git下来。）</p>\n<p>1.ProxyPool的使用：</p>\n<p>首先使用 git clone 将源代码拉到你本地，<br/> <img alt=\"在这里插入图片描述\" src=\"image\\2c1d43be0ffa40a0851ff6dca5e44be8.png\"/></p>\n<p>3.进入proxypool目录，修改settings.py文件，PASSWORD为Redis密码，如果为空，则设置为None。（新装的redis一般没有密码。）</p>\n<p>(如果你没 redis 的话，可以先去下载了安装了再来看吧。)</p>\n<p>（假设你的redis已经安装完成。）</p>\n<p>4.接着在你 clone 下来的文件目录中（就是这个ProxyPool存的电脑路径 ）</p>\n<p>5.安装相关所需的 依赖包：<br/> （pip或pip3）</p>\n<p>pip install -r requirements.txt</p>\n<p>（如果你把ProxyPool导入在pycharm里面，那就一切都在pycharm里面搞就可以了。<br/> <img alt=\"在这里插入图片描述\" src=\"image\\838be7cce654407ca87fbf957b9c0d7e.png\"/></p>\n<p>6.接下来开启你的 redis服务，</p>\n<p>直接cmd 打开dos窗口，运行：redis-server.exe<br/> 即可开启redis服务器。redis 的默认端口就是 6379<br/> <img alt=\"在这里插入图片描述\" src=\"image\\99032eec440c44b8b29115abf2c90161.png\"/></p>\n<p>7.接着就可以运行 run.py 了。</p>\n<p>可以在cmd里面命令方式运行，也可以导入pycharm里面运行。</p>\n<p>图示：<br/> <img alt=\"在这里插入图片描述\" src=\"image\\f2ca576e135a4130b666958dc6ba1193.png\"/></p>\n<p>8.运行 run.py 以后，你可以打开你的redis管理工具，或者进入redis里面查看，这时候在你的 redis 中就会存入很多已经爬取到的代理 ip 了：<br/> <img alt=\"在这里插入图片描述\" src=\"image\\559a4095768d4cc7acc6a75ef6158c2d.png\"/></p>\n<p>9.项目跑起来之后，【不要停止】，此时redis里面存了ip，就可以访问这个代理池了。</p>\n<p>在上面的图中，可以看到有这么一句话</p>\n<p>Running on http://0.0.0.0:5555/ (Press CTRL+C to quit)<br/> 这就是告诉我们随机访问地址URL是多少。<br/> 10.在浏览器中随机获取一个代理 ip 地址：</p>\n<p>你就浏览器输入：</p>\n<p>http://0.0.0.0:5555/random<br/> 1<br/> <img alt=\"在这里插入图片描述\" src=\"image\\8f8e26e525d7403e9fcf90ffad4a7755.png\"/></p>\n<p>11.在代码中随机获取一个ip代理</p>\n<p>就这样：</p>\n<p>import requests</p>\n<h1><a id=\"ip_326\"></a>随机ip代理获取</h1>\n<p>PROXY_POOL_URL = ‘http://localhost:5555/random’<br/> def get_proxy():<br/> try:<br/> response = requests.get(PROXY_POOL_URL)<br/> if response.status_code == 200:<br/> return response.text<br/> except ConnectionError:<br/> return None</p>\n<p>if <strong>name</strong> == ‘<strong>main</strong>’:<br/> print(get_proxy())</p>\n<p><img alt=\"在这里插入图片描述\" src=\"image\\92c3c0a6a9e14d649f405e7f5f346416.png\"/></p>\n<p>好了。到此结束了。</p>\n<p>使用这个 ip代理池，目前来说是最好的了，又免费又高效唉~~~</p>\n<p>5.报错解决<br/> 安装的时候，如果报错类似于如下：</p>\n<p>AttributeError: ‘int’ object has no attribute 'items</p>\n<p>更新一下 对应的xxx软件版本，比如redis 版本：</p>\n<p>pip install redis==3.33.1</p>\n<p>最后给一个REDIS DESKTOP MANAGER安装教程：<br/> Redis 服务安装与配置</p>\n<p>1.下载解压</p>\n<p>先下载Redis：</p>\n<p>https://github.com/ServiceStack/redis-windows/tree/master/downloads<br/> <img alt=\"在这里插入图片描述\" src=\"image\\b39b616000ad4e0ba578e9bdd1a9e84e.png\"/><br/> <img alt=\"在这里插入图片描述\" src=\"image\\ac63728071354663926953f40e787cae.png\"/></p>\n<p>下载完后解压到任意路径下（我的是D:\\SoftWare\\Redis-3.0）</p>\n<p>2.启动redis服务器</p>\n<p>进入解压后的文件夹，然后运行redis-server.exe文件。<br/> 　<img alt=\"在这里插入图片描述\" src=\"image\\ddb2d4c8122a47edbee3002a88b8b89f.png\"/><br/> <img alt=\"在这里插入图片描述\" src=\"image\\55247e262e864e0b9b2e62b7324a87c8.png\"/></p>\n<p>注意：该窗口不可关闭，否则，Redis服务不可用！</p>\n<p>现在再从Redis Desktop Manager进行连接就可以成功了！<br/> <img alt=\"在这里插入图片描述\" src=\"image\\e15d8281f9d241ffb85d1cbf276d7a1a.png\"/></p>\n<p>3.启动redis客户端</p>\n<p>直接双击D:\\SoftWare\\Redis-3.0目录下的redis-cli.exe文件（redis客户端），如果显示127.0.0.1:6379&gt; ，就说明客户端运行成功。<br/> 　<img alt=\"在这里插入图片描述\" src=\"image\\3140c5a0acbd4017a0afc024244eaa69.png\"/></p>\n<p>4.修改密码</p>\n<p>redis默认是空密码，但是这样在项目上线后是不安全的，容易被入侵，所以要设置密码。</p>\n<p>1）打开redis.windows.conf文件，找到# requirepass foobared 这行，在此行下增加一行requirepass 所设置的密码 ，保存。</p>\n<p><img alt=\"在这里插入图片描述\" src=\"image\\4a14ff9283b84e4e904913da5dc436fd.png\"/></p>\n<p>//此处注意，密码自定义就行，并且行前不能有空格！</p>\n<p>2）打开cmd(windows命令窗口)，切换到redis-server.exe目录下。</p>\n<p>3）输入命令：redis-server.exe redis.windows.conf启动redis，即可使用密码了。<br/> <img alt=\"在这里插入图片描述\" src=\"image\\27f956dc843f4973807d2ab623cbe1ff.png\"/></p>\n<p>4）客户端测试：</p>\n<p>127.0.0.1:6379&gt; keys *<br/> (empty list or set)<br/> 127.0.0.1:6379&gt; auth redis //会报以下错误<br/> (error) ERR Client sent AUTH, but no password is set<br/> 127.0.0.1:6379&gt; CONFIG SET requirepass “redis” //执行此行指令即可解决错误<br/> OK<br/> 127.0.0.1:6379&gt; auth redis<br/> OK</p>\n<p>5.将Redis服务安装到本地服务</p>\n<p>由于上述启动Redis服务器的方式有点复杂，且redis服务窗口不可关闭。故这里介绍如何将Redis服务安装到Windows系统的本地服务。</p>\n<p>在cmd下输入以下命令：</p>\n<p>redis-server --service-install redis.conf --loglevel verbose //安装redis本地服务，指定配置文件redis.windows.conf</p>\n<p><img alt=\"在这里插入图片描述\" src=\"image\\22cd85e57e794e5880e3958021371632.png\"/></p>\n<p>6.如何卸载Redis本地服务</p>\n<p>打开win系统命令行，依次输入下列命令：</p>\n<p>C:\\Users\\lenovo&gt;cd /d D:</p>\n<p>D:&gt;cd D:\\SoftWare\\Redis-3.0</p>\n<p>D:\\SoftWare\\Redis-3.0&gt; redis-server --service-uninstall</p>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>"}