{"blogid": "124748898", "writerAge": "码龄14年", "writerBlogNum": "179", "writerCollect": "752", "writerComment": "226", "writerFan": "7243", "writerGrade": "5级", "writerIntegral": "2656", "writerName": "扫地的小何尚", "writerProfileAdress": "writer_image\\profile_124748898.jpg", "writerRankTotal": "6784", "writerRankWeekly": "749", "writerThumb": "474", "writerVisitNum": "211519", "blog_read_count": "6271", "blog_time": "已于 2022-06-12 03:10:54 修改", "blog_title": "人脸口罩检测(含运行代码+数据集)Pytorch+TensorRT+Xavier NX", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-atom-one-light\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<h1><a id=\"_0\"></a>人脸口罩检测(含运行代码+数据集)</h1>\n<p><img alt=\"在这里插入图片描述\" src=\"image\\c2c827c93db84141a31d6a4b7bc130e9.jpeg\"/></p>\n<ul><li><strong>本教程目的为让开发者了解深度学习中的完整流程，这包括：</strong><br/> 1.数据集导入及预处理流程<br/> 2.网络模型选择及参数设置流程<br/> 3.模型训练及导出流程<br/> 4.模型加载/优化并得出推断结果</li></ul>\n<p>项目源码以及数据集下载:<br/> https://download.csdn.net/download/kunhe0512/85360655</p>\n<p><img alt=\"在这里插入图片描述\" src=\"image\\e0004de2f0c54e5795625ecc6c92c259.png\"/></p>\n<ul><li> <p><strong>本教程采用了以下主要的软硬件环境：</strong><br/> 1.NVIDIA Xavier NX<br/> 2.Jetpack 4.6<br/> 3.TensorRT 8.0.1<br/> 4.Pytorch 1.10.0<br/> 5.Python 3.6.9<br/> 6.Opencv 4.1.1</p> </li><li> <p><strong>实验内容：</strong></p>\n<ul><li>本教程的实验内容是利用深度学习的方法，完成口罩检测的任务。</li><li>检测目标类别为：Background，face，mask，mask_weared_incorrect</li><li>在实验过程中，采用了OpenImages CVS格式的数据集和SSD-mobilenet的模型。</li><li>本实验利用Pytorch进行模型训练，将训练好的模型转化为ONNX格式，最后利用TensorRT进行推理</li><li>更多精彩内容，请扫描下方二维码来<a href=\"https://developer.nvidia.com/zh-cn/developer-program\">加入NVIDIA开发者计划</a></li></ul> </li></ul>\n<p><img alt=\"在这里插入图片描述\" src=\"image\\f599cf90509b4c55887bc8ba614d85f5.jpeg\"/></p>\n<h1><a id=\"_35\"></a>开始实验</h1>\n<h2><a id=\"1_36\"></a>1.导入需要的工具库</h2>\n<pre><code class=\"prism language-Python\">#1\nimport os\nimport sys\nsys.executable\nimport logging\nimport argparse\nimport itertools\nimport torch\n\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n\nfrom vision.utils.misc import str2bool, Timer, freeze_net_layers, store_labels\nfrom vision.ssd.ssd import MatchPrior\nfrom vision.ssd.vgg_ssd import create_vgg_ssd\nfrom vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd\nfrom vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite\nfrom vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite\nfrom vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite\nfrom vision.datasets.voc_dataset import VOCDataset\nfrom vision.datasets.open_images import OpenImagesDataset\nfrom vision.nn.multibox_loss import MultiboxLoss\nfrom vision.ssd.config import vgg_ssd_config\nfrom vision.ssd.config import mobilenetv1_ssd_config\nfrom vision.ssd.config import squeezenet_ssd_config\nfrom vision.ssd.data_preprocessing import TrainAugmentation, TestTransform\n</code></pre>\n<h2><a id=\"2GPU_67\"></a>2.使用GPU完成训练</h2>\n<pre><code class=\"prism language-Python\">#2\nDEVICE = torch.device(\"cuda:0\")\ntorch.backends.cudnn.benchmark = True\n</code></pre>\n<h2><a id=\"3_75\"></a>3.设定训练方法</h2>\n<pre><code class=\"prism language-Python\">#3\ndef train(loader, net, criterion, optimizer, device, debug_steps=100, epoch=-1):\n    net.train(True)\n    running_loss = 0.0\n    running_regression_loss = 0.0\n    running_classification_loss = 0.0\n    for i, data in enumerate(loader):\n        images, boxes, labels = data\n        images = images.to(device)\n        boxes = boxes.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        confidence, locations = net(images)\n        regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)  # TODO CHANGE BOXES\n        loss = regression_loss + classification_loss\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        running_regression_loss += regression_loss.item()\n        running_classification_loss += classification_loss.item()\n        if i and i % debug_steps == 0:\n            avg_loss = running_loss / debug_steps\n            avg_reg_loss = running_regression_loss / debug_steps\n            avg_clf_loss = running_classification_loss / debug_steps\n            print(\n                f\"Epoch: {epoch}, Step: {i}/{len(loader)}, \" +\n                f\"Avg Loss: {avg_loss:.4f}, \" +\n                f\"Avg Regression Loss {avg_reg_loss:.4f}, \" +\n                f\"Avg Classification Loss: {avg_clf_loss:.4f}\"\n            )\n            running_loss = 0.0\n            running_regression_loss = 0.0\n            running_classification_loss = 0.0\n</code></pre>\n<h2><a id=\"4_114\"></a>4.设定测试方法</h2>\n<pre><code class=\"prism language-Python\">#4\ndef test(loader, net, criterion, device):\n    net.eval()\n    running_loss = 0.0\n    running_regression_loss = 0.0\n    running_classification_loss = 0.0\n    num = 0\n    for _, data in enumerate(loader):\n        images, boxes, labels = data\n        images = images.to(device)\n        boxes = boxes.to(device)\n        labels = labels.to(device)\n        num += 1\n\n        with torch.no_grad():\n            confidence, locations = net(images)\n            regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)\n            loss = regression_loss + classification_loss\n\n        running_loss += loss.item()\n        running_regression_loss += regression_loss.item()\n        running_classification_loss += classification_loss.item()\n    return running_loss / num, running_regression_loss / num, running_classification_loss / num\n</code></pre>\n<h2><a id=\"5_142\"></a>5.设定训练参数</h2>\n<pre><code class=\"prism language-Python\">#5\nnet_name = \"mb1-ssd\"\ndatasets = []\ndatasets_path = [\"data/mask\"]\nmodel_dir = \"models/mask/\" \nvoc_or_open_images = \"open_images\"\nbatch_size = 4\nnum_epochs = 6\nvalidation_epochs = 2\nnum_workers = 2\nlr = 0.01\nbase_net_lr = 0.001\nextra_layers_lr = 0.01\nmomentum=0.9\nweight_decay=5e-4\n</code></pre>\n<h2><a id=\"6_162\"></a>6.加载数据集</h2>\n<p><img alt=\"在这里插入图片描述\" src=\"image\\ec785e15561b4e21badd7a029345199c.png\"/></p>\n<pre><code class=\"prism language-Python\">#6\ntimer = Timer()\ncreate_net = create_mobilenetv1_ssd\nconfig = mobilenetv1_ssd_config\n\n        \n# create data transforms for train/test/val\ntrain_transform = TrainAugmentation(config.image_size, config.image_mean, config.image_std)\ntarget_transform = MatchPrior(config.priors, config.center_variance,\n                                  config.size_variance, 0.5)\n\ntest_transform = TestTransform(config.image_size, config.image_mean, config.image_std)\n\n# load datasets (could be multiple)\nprint(\"Prepare training datasets.\")\nfor dataset_path in datasets_path:\n    if voc_or_open_images == 'voc':\n        dataset = VOCDataset(dataset_path, transform=train_transform,target_transform=target_transform)\n        label_file = os.path.join(model_dir, \"labels.txt\")\n        store_labels(label_file, dataset.class_names)\n        num_classes = len(dataset.class_names)\n    elif voc_or_open_images == 'open_images':\n        dataset = OpenImagesDataset(dataset_path,transform=train_transform, target_transform=target_transform,dataset_type=\"train\", balance_data=False)\n        label_file = os.path.join(model_dir, \"labels.txt\")\n        store_labels(label_file, dataset.class_names)\n        print(dataset)\n        num_classes = len(dataset.class_names)\n\n    else:\n        raise ValueError(f\"Dataset type is not supported.\")\n    datasets.append(dataset)\n</code></pre>\n<h2><a id=\"7_199\"></a>7.将加载好的数据集分割为训练集和验证集</h2>\n<pre><code class=\"prism language-Python\">#7\n# create training dataset\nprint(f\"Stored labels into file {label_file}.\")\ntrain_dataset = ConcatDataset(datasets)\nprint(\"Train dataset size: {}\".format(len(train_dataset)))\ntrain_loader = DataLoader(train_dataset, batch_size,num_workers=num_workers,shuffle=True)\n                           \n# create validation dataset                           \nprint(\"Prepare Validation datasets.\")\nif voc_or_open_images == \"voc\":\n    val_dataset = VOCDataset(dataset_path, transform=test_transform,target_transform=target_transform, is_test=True)\nelif voc_or_open_images == 'open_images':\n    val_dataset = OpenImagesDataset(dataset_path,transform=test_transform, target_transform=target_transform,dataset_type=\"test\")\n    print(val_dataset)\nprint(\"Validation dataset size: {}\".format(len(val_dataset)))\n\nval_loader = DataLoader(val_dataset, batch_size,num_workers = num_workers,shuffle=False)\n</code></pre>\n<h2><a id=\"8_221\"></a>8.创建网络模型</h2>\n<pre><code class=\"prism language-Python\">#8\n# create the network\nprint(\"Build network.\")\nnet = create_net(num_classes)\nmin_loss = -10000.0\nlast_epoch = -1\n\nparams = [\n    {'params': net.base_net.parameters(), 'lr': base_net_lr},\n    {'params': itertools.chain(\n        net.source_layer_add_ons.parameters(),\n        net.extras.parameters()\n    ), 'lr': extra_layers_lr},\n    {'params': itertools.chain(\n        net.regression_headers.parameters(),\n        net.classification_headers.parameters()\n    )}\n]\n</code></pre>\n<h2><a id=\"9_244\"></a>9.定义是否使用预训练模型或者</h2>\n<ul><li><strong>我们这里设计了三种模式:</strong><br/> <strong>1.重头开始训练，只需将你的模型路径赋值给base_net： base_net = “path/to/the/basic/model”</strong><br/> <strong>2.使用之前训练一半中间断开没训练完的模型继续训练，只需将模型路径赋值给resume：resume = “path/to/the/resume/model”</strong><br/> <strong>3.利用我们已经准好的预训练模型，只需将模型路径赋值给pretrained_ssd: pretrained_ssd = “path/to/the/pretrained_ssd/model”</strong></li><li><strong>如果不太明白想选择什么模型，可以将resume，base_net和pretrained_ssd都赋值None，将会自动从头开始训练</strong></li></ul>\n<pre><code class=\"prism language-Python\">#9\n# load a previous model checkpoint (if requested)\ntimer.start(\"Load Model\")\nresume=None\nbase_net = None\npretrained_ssd = \"models/face-mask-pretrain-model.pth\"\nif resume:\n    print(f\"Resume from the model {resume}\")\n    net.load(resume)\nelif base_net:\n    print(f\"Init from base net {base_net}\")\n    net.init_from_base_net(base_net)\nelif pretrained_ssd:\n    print(f\"Init from pretrained ssd {pretrained_ssd}\")\n    net.init_from_pretrained_ssd(pretrained_ssd)\nprint(f'Took {timer.end(\"Load Model\"):.2f} seconds to load the model.')\n</code></pre>\n<h2><a id=\"10_270\"></a>10.开始训练模型</h2>\n<pre><code class=\"prism language-Python\">#10\n# move the model to GPU\nnet.to(DEVICE)\n\n# define loss function and optimizer\ncriterion = MultiboxLoss(config.priors, iou_threshold=0.5, neg_pos_ratio=3,center_variance=0.1, size_variance=0.2, device=DEVICE)\noptimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\nprint(f\"Learning rate: {lr}, Base net learning rate: {base_net_lr}, \"+ f\"Extra Layers learning rate: {extra_layers_lr}.\")\n\n# set learning rate policy\nprint(\"Uses CosineAnnealingLR scheduler.\")\nscheduler = CosineAnnealingLR(optimizer, 100, last_epoch=last_epoch)\n\n\n# train for the desired number of epochs\nprint(f\"Start training from epoch {last_epoch + 1}.\")\n    \nfor epoch in range(last_epoch + 1, num_epochs):\n    scheduler.step()\n    train(train_loader, net, criterion, optimizer,device=DEVICE, debug_steps=10, epoch=epoch)\n        \n    if epoch % validation_epochs == 0 or epoch == num_epochs - 1:\n        val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)\n        print(\n            f\"Epoch: {epoch}, \" +\n            f\"Validation Loss: {val_loss:.4f}, \" +\n            f\"Validation Regression Loss {val_regression_loss:.4f}, \" +\n            f\"Validation Classification Loss: {val_classification_loss:.4f}\"\n        )\n        model_path = os.path.join(model_dir, f\"{net_name}-Epoch-{epoch}-Loss-{val_loss}.pth\")\n        net.save(model_path)\n        print(f\"Saved model {model_path}\")\n\nprint(\"Task done, exiting program.\")\n    \n</code></pre>\n<h2><a id=\"11ONNX_310\"></a>11.将训练好的模型转化成ONNX格式</h2>\n<pre><code class=\"prism language-Python\">#11\n!python3 onnx_export.py --model-dir=models/mask\n</code></pre>\n<h2><a id=\"12ONNXTensorRTTensorRT_317\"></a>12.将转化好的ONNX格式利用TensorRT进行优化，生成TensorRT推理引擎</h2>\n<p>这里注意,需要安装Onnx2TensorRT</p>\n<pre><code class=\"prism language-Python\">#12\n!onnx2trt models/mask/ssd-mobilenet.onnx -o models/TRT_ssd_mobilenet_v1_face2.bin\n</code></pre>\n<h2><a id=\"13_326\"></a>13.加载引擎推理时所需要的工具库</h2>\n<pre><code class=\"prism language-Python\">#13\nimport sys\nimport time\nimport argparse\nimport cv2\nimport pycuda.autoinit \nimport numpy as np\nfrom utils.ssd_classes import get_cls_dict\nfrom utils.camera import add_camera_args, Camera\nfrom utils.display import open_window, set_display, show_fps\nfrom utils.visualization import BBoxVisualization\nimport ctypes\nimport tensorrt as trt\nimport pycuda.driver as cuda\n</code></pre>\n<h2><a id=\"14_345\"></a>14.设计引擎输入输出的预处理方法和后处理方法</h2>\n<pre><code class=\"prism language-Python\">#14\ndef do_nms(det, boxes, confs, clss):\n    drop = False\n    if len(boxes) &lt;= 0:\n        boxes.append((det[0],det[1],det[2],det[3]))\n        confs.append(det[4])\n        clss.append(det[5])\n        return boxes, confs, clss\n    for i in range(0,len(boxes)):\n        bbox = boxes[i]\n        xx1 = np.maximum(det[0], bbox[0])\n        yy1 = np.maximum(det[1], bbox[1])\n        xx2 = np.minimum(det[2], bbox[2])\n        yy2 = np.minimum(det[3], bbox[3])\n        \n        w = np.maximum(0.0, xx2-xx1+1)\n        h = np.maximum(0.0, yy2-yy1+1)\n        area_det = (det[2]-det[0]+1)*(det[3]-det[1]+1)\n        area_bbox = (bbox[2]-bbox[0]+1)*(bbox[3]-bbox[1]+1)\n        inter = w*h\n        ovr = inter / (area_det + area_bbox - inter)\n        if ovr &gt; 0.6 and not drop:\n            if det[4] &gt; confs[i]:\n                boxes[i] = ((det[0],det[1],det[2],det[3]))\n                confs[i] = det[4]\n                clss[i] = det[5]\n            drop = True\n    if not drop:\n        boxes.append((det[0],det[1],det[2],det[3]))\n        confs.append(det[4])\n        clss.append(det[5])\n    return boxes, confs, clss\n\ndef _preprocess_trt(img, shape=(300, 300)):\n    \"\"\"Preprocess an image before TRT SSD inferencing.\"\"\"\n    img = cv2.resize(img, shape)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.transpose((2, 0, 1)).astype(np.float32)\n    img *= (2.0/255.0)\n    img -= 1.0\n    return img\n\n\n\n\ndef _postprocess_trt(img, output, conf_th, output_layout):\n    \"\"\"Postprocess TRT SSD output.\"\"\"\n    img_h, img_w, _ = img.shape\n    boxes, confs, clss, results = [], [], [],[]\n    #print(((len(output[1]))/4+1))\n    #print(\"len(outputs[0]): \"+str(len(output[0]))+\" len(outputs[1]): \"+str(len(output[1])))\n    for n in range(0, int((len(output[1]))/4)):\n        maxScore = -1000.0000\n        maxClass = 0\n        for m in range(0, 4):\n            score = output[0][n*4+m]\n            #print(score)\n            if score &lt; conf_th:\n                continue\n            if m &lt;= 0:\n                continue\n            if( score &gt; maxScore):\n                maxScore = score\n                maxClass = m\n        #if(maxClass &lt; 0):\n        #    continue\n        index = int(n)\n        if maxScore &lt; conf_th:\n            continue\n        #print(str(output[1][n*4+0])+\" \"+str(output[1][n*4+1])+\" \"+str(output[1][n*4+2])+\" \"+str(output[1][n*4+3]))\n        x1 = int(output[1][n*4+0] * img_w)\n        y1 = int(output[1][n*4+1] * img_h)\n        x2 = int(output[1][n*4+2] * img_w)\n        y2 = int(output[1][n*4+3] * img_h)\n        det = [x1,y1,x2,y2,maxScore,maxClass,n]\n        boxes, confs, clss = do_nms(det, boxes, confs, clss)\n    return boxes, confs, clss\n</code></pre>\n<h2><a id=\"15SSDmobilenet_v1_426\"></a>15.定义SSD-mobilenet v1模型的推理引擎的加载</h2>\n<ul><li>当我们已经优化好了引擎的时候，我们可以将优化好的引擎以文件的形式写到硬盘上，我们称之为序列化文件（serialized file）或PLAN文件</li><li>我们下次想直接使用优化好的引擎的时候，我们可以通过读取硬盘上的序列化文件，并利用 <font color=\"#008000\" size=\"4\">deserialize_cuda_engine() </font>方法进行反序列化，生成可执行的引擎</li><li>利用序列化文件生成可执行引擎可以为我们节省大量的时间</li><li>不同平台（软件或硬件平台）上生成的引擎的序列化文件不能直接通用，相同平台（软件且硬件平台）或同一台设备上生成的引擎序列化文件可以直接用</li></ul>\n<pre><code class=\"prism language-Python\">#15\nclass TrtSSD(object):\n    \"\"\"TrtSSD class encapsulates things needed to run TRT SSD.\"\"\"\n    #加载自定义组建，这里如果TensorRT版本小于7.0需要额外生成flattenconcat的自定义组件库\n    def _load_plugins(self):\n        trt.init_libnvinfer_plugins(self.trt_logger, '')\n    #加载通过Transfer Learning Toolkit生成的推理引擎\n    def _load_engine(self):\n        TRTbin = 'models/TRT_%s.bin' % self.model\n        with open(TRTbin, 'rb') as f, trt.Runtime(self.trt_logger) as runtime:\n            return runtime.deserialize_cuda_engine(f.read())\n    #通过加载的引擎，生成可执行的上下文\n    def _create_context(self):\n        for binding in self.engine:\n            size = trt.volume(self.engine.get_binding_shape(binding)) * \\\n                   self.engine.max_batch_size\n            ##注意：这里的host_mem需要时用pagelocked memory，以免内存被释放\n            host_mem = cuda.pagelocked_empty(size, np.float32)\n            cuda_mem = cuda.mem_alloc(host_mem.nbytes)\n            self.bindings.append(int(cuda_mem))\n            if self.engine.binding_is_input(binding):\n                self.host_inputs.append(host_mem)\n                self.cuda_inputs.append(cuda_mem)\n            else:\n                self.host_outputs.append(host_mem)\n                self.cuda_outputs.append(cuda_mem)\n        return self.engine.create_execution_context()\n    #初始化引擎\n    def __init__(self, model, input_shape, output_layout=7):\n        \"\"\"Initialize TensorRT plugins, engine and conetxt.\"\"\"\n        self.model = model\n        self.input_shape = input_shape\n        self.output_layout = output_layout\n        self.trt_logger = trt.Logger(trt.Logger.INFO)\n        self._load_plugins()\n        self.engine = self._load_engine()\n\n        self.host_inputs = []\n        self.cuda_inputs = []\n        self.host_outputs = []\n        self.cuda_outputs = []\n        self.bindings = []\n        self.stream = cuda.Stream()\n        self.context = self._create_context()\n    #释放引擎，释放GPU显存，释放CUDA流\n    def __del__(self):\n        \"\"\"Free CUDA memories.\"\"\"\n        del self.stream\n        del self.cuda_outputs\n        del self.cuda_inputs\n    #利用生成的可执行上下文执行推理\n    def detect(self, img, conf_th=0.3):\n        \"\"\"Detect objects in the input image.\"\"\"\n        img_resized = _preprocess_trt(img, self.input_shape)\n        np.copyto(self.host_inputs[0], img_resized.ravel())\n        #将处理好的图片从CPU内存中复制到GPU显存\n        cuda.memcpy_htod_async(\n            self.cuda_inputs[0], self.host_inputs[0], self.stream)\n        #开始执行推理任务\n        self.context.execute_async(\n            batch_size=1,\n            bindings=self.bindings,\n            stream_handle=self.stream.handle)\n        #将推理结果输出从GPU显存复制到CPU内存\n        cuda.memcpy_dtoh_async(\n            self.host_outputs[1], self.cuda_outputs[1], self.stream)\n        cuda.memcpy_dtoh_async(\n            self.host_outputs[0], self.cuda_outputs[0], self.stream)\n        self.stream.synchronize()\n\n\n        output = self.host_outputs\n        #print(\"len(outputs[0]): \"+str(len(self.host_outputs[0]))+\" len(outputs[1]): \"+str(len(self.host_outputs[1])))\n        #for x in self.host_outputs[0]:\n        #    print(str(x),end=' ')\n        #for x in self.host_outputs[1]:\n        #    print(str(x),end=' ')\n        return _postprocess_trt(img, output, conf_th, self.output_layout)\n</code></pre>\n<h2><a id=\"16_513\"></a>16.设置模型库</h2>\n<ul><li>1.这里定义了多个模型库，我们选用的是人脸口罩检测，也就是最后一个<strong>ssd_mobilenet_v1_face2</strong></li><li>2.这里还定义了我们模型的输入（300,300）</li></ul>\n<pre><code class=\"prism language-Python\">#16\nINPUT_HW = (300, 300)\nSUPPORTED_MODELS = [\n    'ssd_mobilenet_v1_coco',\n    'ssd_mobilenet_v1_egohands',\n    'ssd_mobilenet_v2_coco',\n    'ssd_mobilenet_v2_egohands',\n    'ssd_mobilenet_v2_face',\n    'ssd_resnet18_5th',\n    'ssd_mobilenet_v1_face2',\n    'ssd_mobilenet_v1_fruit'\n]\n</code></pre>\n<h2><a id=\"17_531\"></a>17.开始定义方法来读取数据并将输出可视化的画到图像上</h2>\n<ul><li>detect_one()方法是检测单张图片，detect_video()方法是检测视频</li><li>注意：这里打印的fps值是包括将图像写到结果视频中的时间，如果取消将视频写到结果视频的功能，速度会有大幅度提升</li></ul>\n<pre><code class=\"prism language-Python\">#17-1\ndef detect_video(video, trt_ssd, conf_th, vis,result_file_name):\n    full_scrn = False\n    fps = 0.0\n    tic = time.time()\n    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = video.get(cv2.CAP_PROP_FPS)\n    #print(str(frame_width)+str(frame_height))\n    ##定义输入编码\n    fourcc = cv2.VideoWriter_fourcc('M', 'P', '4', 'V')\n    videoWriter = cv2.VideoWriter('result.AVI', fourcc, fps, (frame_width,frame_height))\n    ##开始循环检测，并将结果写到result.mp4中\n    while True:\n        ret,img = video.read()\n        if img is not None:\n            boxes, confs, clss = trt_ssd.detect(img, conf_th)\n            #print(\"boxes,confs,clss: \"+ str(boxes)+\" \"+ str(confs)+\" \"+str(clss))\n            img = vis.draw_bboxes(img, boxes, confs, clss)\n            videoWriter.write(img)\n            toc = time.time()\n            curr_fps = 1.0 / (toc - tic)\n            fps = curr_fps if fps == 0.0 else (fps*0.95 + curr_fps*0.05)\n            tic = toc\n            print(\"\\rfps: \"+str(fps),end=\"\")\n        else:\n            break\n\n#17-2\ndef detect_one(img, trt_ssd, conf_th, vis):\n    full_scrn = False\n    tic = time.clock()\n    ##开始检测，并将结果写到result.jpg中\n    boxes, confs, clss = trt_ssd.detect(img, conf_th)\n    toc = time.clock()\n    curr_fps = (toc - tic)\n    #print(\"boxes: \"+str(boxes))\n    #print(\"clss: \"+str(clss))\n    #print(\"confs: \"+str(confs))\n    img = vis.draw_bboxes(img, boxes, confs, clss)\n    cv2.imwrite(\"result.jpg\",img)        \n    print(\"time: \"+str(curr_fps)+\"(sec)\")\n</code></pre>\n<h2><a id=\"18main_578\"></a>18.定义main()函数，检测单张图片**</h2>\n<ul><li>您可以自行上传图像到当前文件夹，并将filename请改成您要测试的图片的名字</li><li>face指的是没有戴口罩的人脸，face_mask指的是带了口罩的人脸，mask_weared_incorrect指的是带了口罩但是带的不规范的人脸</li></ul>\n<pre><code class=\"prism language-Python\">#18-1\ndef main_one():    \n    filename = \"mask.jpg\"\n    result_file_name = str(filename)\n    img = cv2.imread(filename)\n    cls_dict = get_cls_dict(\"ssd_mobilenet_v1_face2\".split('_')[-1])\n    model_name =\"ssd_mobilenet_v1_face2\"\n    trt_ssd = TrtSSD(model_name, INPUT_HW)\n    vis = BBoxVisualization(cls_dict)\n    print(\"start detection!\")\n    detect_one(img, trt_ssd, conf_th=0.5, vis=vis)\n    cv2.destroyAllWindows()\n    print(\"finish!\")\n</code></pre>\n<pre><code class=\"prism language-Python\">#18-2\nfrom IPython.display import Image\nmain_one()\nImage(\"result.jpg\")\n</code></pre>\n<p><img alt=\"在这里插入图片描述\" src=\"image\\c56e7c15c513413b99c58be761fa94d9.jpeg\"/></p>\n<h2><a id=\"19main_606\"></a>19.定义main()函数，检测视频</h2>\n<ul><li>您可以自行上传视频到当前文件夹，并将filename请改成您要测试的视频的名字</li><li>检测视频部分由于要将检测的结果写到硬盘上，所以时间会加倍，如果要得到和单张检测相似的数据，可以将读写的语句注释掉</li><li>face指的是没有戴口罩的人脸，face_mask指的是带了口罩的人脸，mask_weared_incorrect指的是带了口罩但是带的不规范的人脸）</li></ul>\n<pre><code class=\"prism language-Python\">#19-1\ndef main_loop():   \n    filename = \"face_mask_test_video.mp4\"\n    result_file_name = str(filename)\n    video = cv2.VideoCapture(filename)\n    cls_dict = get_cls_dict(\"ssd_mobilenet_v1_face2\".split('_')[-1])\n    model_name =\"ssd_mobilenet_v1_face2\"\n    trt_ssd = TrtSSD(model_name, INPUT_HW)\n    vis = BBoxVisualization(cls_dict)\n    print(\"start detection!\")\n    detect_video(video, trt_ssd, conf_th=0.8, vis=vis, result_file_name=result_file_name)\n    video.release()\n    cv2.destroyAllWindows()\n    print(\"\\nfinish!\")\n</code></pre>\n<pre><code class=\"prism language-Python\">#19-2\nmain_loop()\n</code></pre>\n<h2><a id=\"20Jupyter_Notebook_631\"></a>20.将生成的视频转码，以便能够在Jupyter Notebook中查看</h2>\n<ul><li>这里采用的是利用GPU加速的转码技术，将输出的视频转换到MP4格式，比单纯使用CPU进行转码的速度有大幅度提升</li></ul>\n<pre><code class=\"prism language-Python\">#20\n!rm result-ffmpeg4.mp4\n!ffmpeg -i result.AVI -vcodec libx264 -f mp4 result-ffmpeg4.mp4 \n</code></pre>\n<h2><a id=\"21_639\"></a>21.查看结果视频</h2>\n<pre><code class=\"prism language-Python\">#21\nfrom IPython.display import Video\n\nVideo(\"result-ffmpeg4.mp4\")\n</code></pre>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>"}