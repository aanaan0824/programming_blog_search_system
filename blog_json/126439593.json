{"blogid": "126439593", "writerAge": "码龄1年", "writerBlogNum": "29", "writerCollect": "220", "writerComment": "406", "writerFan": "330", "writerGrade": "4级", "writerIntegral": "1025", "writerName": "发量不足", "writerProfileAdress": "writer_image\\profile_126439593.jpg", "writerRankTotal": "18548", "writerRankWeekly": "343", "writerThumb": "297", "writerVisitNum": "24478", "blog_read_count": "724", "blog_time": "于 2022-08-20 14:39:23 发布", "blog_title": "Spark SQL 结构化数据文件处理", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<blockquote>\n<p id=\"main-toc\"><strong>目录📑</strong></p>\n<p id=\"4.1%20Spark%20SQL%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-toc\" style=\"margin-left:40px;\"><a href=\"#4.1%20Spark%20SQL%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86\">Spark SQL的基础知识</a></p>\n<p id=\"%E4%B8%80%E3%80%81Spark%20SQL%E7%9A%84%E7%AE%80%E4%BB%8B-toc\" style=\"margin-left:80px;\"><a href=\"#%E4%B8%80%E3%80%81Spark%20SQL%E7%9A%84%E7%AE%80%E4%BB%8B\">一、Spark SQL的简介</a></p>\n<p id=\"%E4%BA%8C%EF%BC%8ESpark%20SQL%20%E6%9E%B6%E6%9E%84-toc\" style=\"margin-left:80px;\"><a href=\"#%E4%BA%8C%EF%BC%8ESpark%20SQL%20%E6%9E%B6%E6%9E%84\">二．Spark SQL 架构</a></p>\n<hr id=\"hr-toc\"/>\n<p class=\"img-center\"><img alt=\"\" height=\"383\" src=\"image\\56f4389ca26b41039257ac7616d1af5e.jpeg\" width=\"669\"/></p>\n<p> </p>\n</blockquote>\n<h2 id=\"4.1%20Spark%20SQL%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86\" style=\"margin-left:0;text-align:justify;\">Spark SQL的基础知识🎈🎈</h2>\n<p style=\"margin-left:0;text-align:left;\">   Spark SQL是Spark用来处理结构化数据的一个模块，<strong><span style=\"color:#2f5496;\">它提供了一个叫作DataFrame的编程模型（带有Schema信息的RDD）</span></strong><span style=\"color:#2f5496;\">。</span>SparkSQL作为分布式SQL查询引擎，让用户可以通过SQL、DataFrameAPI和DatasetAPI三种方法实现对结构化数据的处理。</p>\n<h3 id=\"%E4%B8%80%E3%80%81Spark%20SQL%E7%9A%84%E7%AE%80%E4%BB%8B\" style=\"text-align:left;\"><strong>一、Spark SQL的简介</strong></h3>\n<p style=\"margin-left:0;text-align:justify;\"><strong><span style=\"color:#2f5496;\">Spurk SQL</span></strong><strong><span style=\"color:#2f5496;\">的前身是Shark</span></strong>，Shark最初是美国加州大学伯克利分校的实验室开发的Spark生态系统的组件之一。</p>\n<p style=\"margin-left:0;text-align:justify;\">Shark<strong><span style=\"color:#2f5496;\">过于依赖</span></strong>Hive,因此在版本这代时很难添加新的优化策略。</p>\n<p style=\"margin-left:0;text-align:left;\">   Spark SQL主要提供三个功能</p>\n<p style=\"margin-left:0;text-align:left;\">   1.Spark SQL可以从各个结构数字化数据源（如JSON，Hive，Parquet等）中读取数据，进行缝隙</p>\n<p style=\"margin-left:0;text-align:left;\">   2.SparkSQL包含行业标准JDBC和ODBC连接方式，因此他不限于在Spark程序中使用SQL语句进行查询。</p>\n<p style=\"margin-left:0;text-align:left;\">   3.Spark SQL可以无缝的将SQL查询与Spark程序进行结合，他能将结构化数据作为Spark中的分布式数据集（RDD）进行查询，再Python，Scala和Java中均继承了相关的API。</p>\n<p style=\"margin-left:0;text-align:left;\">总结：<strong><span style=\"color:#2f5496;\">SparkSQL</span><span style=\"color:#2f5496;\">支持多种数据源的查询和加载，兼容Hive,可以使用JDBC/ODBC的连接方式来执行SQL语句，它为Spark框架在结构化数据分析方面提供重要的技术支持。</span></strong></p>\n<p style=\"margin-left:0;text-align:left;\"></p>\n<h3 id=\"%E4%BA%8C%EF%BC%8ESpark%20SQL%20%E6%9E%B6%E6%9E%84\" style=\"margin-left:0px;text-align:left;\"><strong>二．Spark SQL 架构</strong></h3>\n<p style=\"margin-left:0;text-align:left;\">1 .<strong><span style=\"color:#2f5496;\">Spark SQL</span></strong><strong><span style=\"color:#2f5496;\">兼容Hive</span></strong>,Spark SQL 架构与Hive底层结构相似，Spark SQL复用Hive提供的元数据仓库（Metastore）、HiveQL、用户自定义函数（UDF）以及序列化和反序列工具（SerDes）。</p>\n<p style=\"margin-left:0;text-align:left;\"><img alt=\"\" height=\"449\" src=\"image\\c18d2571bb57447b86c966621feec824.png\" width=\"795\"/></p>\n<p style=\"margin-left:0;text-align:left;\"></p>\n<p style=\"margin-left:0;text-align:left;\"><strong><span style=\"color:#2f5496;\">Spark SQL</span></strong><strong><span style=\"color:#2f5496;\">快速的计算效率得益于 Catalyst优化器</span></strong>。(HiveQL 被解析成语法抽象树起，执行计划生成和优化的工作全部交 给(Spark sQD的Catalyst优化器负责和管理。</p>\n<p style=\"margin-left:0;text-align:left;\">Spark的三大过程：解析（Parser）、优化（optimizer）、执行(execution)</p>\n<p style=\"margin-left:0;text-align:left;\">Catalyst优化器执行生产和优化的<strong><span style=\"color:#2f5496;\">五大组件</span></strong>：</p>\n<p style=\"margin-left:0;text-align:left;\"><strong>Parse</strong><strong>组件:</strong>该组件根据一定的语义规则(即第三方类库ANTLR)将SparkSql字符串解析为一个抽象语法树AST。</p>\n<p style=\"margin-left:0;text-align:left;\"><strong>Analyze</strong><strong>组件:</strong>该组件会遍历整个AST,并对AST上的每个节点进行数据类型的绑定以及函数绑定，然后根据元数据信息Catalog对数据表中的字段进行解析。</p>\n<p style=\"margin-left:0;text-align:left;\"><strong>Optimizer</strong><strong>组件:</strong>该组件是Catalyst的核心，主要分为RBO和CBO两种优化策略，其中RBO是基于规则优化,CBO是基于代价优化。</p>\n<p style=\"margin-left:0;text-align:left;\"><strong>SparkPlanner</strong><strong>组件</strong>:优化后的逻辑执行计划OptimizedLogicalPlan依然是逻辑的， 并不能被Spark系统理解，此时需要将OptimizedLogicalPlan转换成physical plan(物理计划)。</p>\n<p style=\"margin-left:0;text-align:left;\"><strong>CostModel</strong><strong>组件</strong>:主要根据过去的性能统计数据，选择最佳的物理执行计划。</p>\n<p style=\"margin-left:0;text-align:left;\"><img alt=\"\" height=\"359\" src=\"image\\05dd2bef4ca841e7aab1b4200ecb7930.png\" width=\"865\"/></p>\n<p style=\"margin-left:0;text-align:left;\"></p>\n<p style=\"margin-left:0;text-align:left;\">Spark SQL工作流程:</p>\n<p style=\"margin-left:0;text-align:left;\">(1)在解析SQL语句之前会创建SparSession,涉及表名、字段名称和字段类型的元数据都将保存在Catalog中;</p>\n<p style=\"margin-left:0;text-align:left;\"></p>\n<p style=\"margin-left:0;text-align:left;\">(2)当调用SparkSession的sq|(O方法时就会使用SparkSqlParser进行解析SQL语 句,解析过程中使用的ANTLR进行词法解析和语法解析:</p>\n<p style=\"margin-left:0;text-align:left;\"></p>\n<p style=\"margin-left:0;text-align:left;\">(3)使用Analyzer分析器绑定逻辑计划，在该阶段Analyer会使用Analyzer Rules,并结合Catalog,对未绑定的逻辑计划进行解析,生成已绑定的逻辑计划;</p>\n<p style=\"margin-left:0;text-align:left;\"></p>\n<p style=\"margin-left:0;text-align:left;\">(4) Optimizer根据预先定义好的规则(RBO)对Resolved Logical Plan 进行优化并生 成Optimized Logical Plan(最优逻辑计划);</p>\n<p style=\"margin-left:0;text-align:left;\"></p>\n<p style=\"margin-left:0;text-align:left;\">(5)使用SparkPlanner对优化后的逻辑计划进行转换，生成多个可以执行的物理计划 Physical Plan;</p>\n<p style=\"margin-left:0;text-align:left;\">(6) CBO优化策略会根据Cost Model算出每个Physical Plan的代价，并选取代价最 小的Physical Plan作为最终的Physical Plan;</p>\n<p style=\"margin-left:0;text-align:left;\"></p>\n<p style=\"margin-left:0;text-align:left;\">(7)使用QueryExecuion执行物理计划，此时则调用SparkPlan的execute()方法,返回RDD.</p>\n<p style=\"margin-left:0;text-align:left;\"><img alt=\"\" height=\"359\" src=\"image\\5b0dfdcf62544d2c98110d06e7e4aa53.png\" width=\"865\"/></p>\n<p style=\"margin-left:0;text-align:left;\"></p>\n</div>\n</div>"}