{"blogid": "125728708", "writerAge": "码龄5年", "writerBlogNum": "301", "writerCollect": "861", "writerComment": "159", "writerFan": "14641", "writerGrade": "5级", "writerIntegral": "3696", "writerName": "LiBiGo", "writerProfileAdress": "writer_image\\profile_125728708.jpg", "writerRankTotal": "4632", "writerRankWeekly": "267", "writerThumb": "423", "writerVisitNum": "1388262", "blog_read_count": "12522", "blog_time": "已于 2022-07-11 20:13:21 修改", "blog_title": "【完美解决】RuntimeError: one of the variables needed for gradient computation has been modified by an inp", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<p>【就看这一篇就行】RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [256]] is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</p>\n<p>针对网上搜到的以下办法均无效的情况：<br/> 1）找到网络模型中的 inplace 操作，将inplace=True改成 inplace=False，例如torch.nn.ReLU(inplace=False)<br/> 2）将代码中的“a+=b”之类的操作改为“c = a + b”<br/> 3）将loss.backward()函数内的参数retain_graph值设置为True, loss.backward(retain_graph=True)，如果retain_graph设置为False，计算过程中的中间变量使用完即被释放掉。</p>\n<p><span style=\"background-color:#fe2c24;\">↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑以上方案无效↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑</span></p>\n<p><span style=\"background-color:#ffd900;\">↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓正确解决方案如下↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓</span></p>\n<hr/>\n<h1>1、问题描述：</h1>\n<p><img alt=\"\" height=\"921\" src=\"image\\320e9e131a5c4794adbcad29a3cde798.png\" width=\"1200\"/></p>\n<p> 提示在 loss.backward()报错</p>\n<p><span style=\"background-color:#a5a5a5;\">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [256]] is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</span></p>\n<h1>2、问题分析</h1>\n<p>在用PyTorch进行分布式训练时，遇到以上错误。</p>\n<p><span style=\"color:#fe2c24;\"><span style=\"background-color:#0d0016;\">日志的大概意思是用于梯度计算的变量通过</span><code><span style=\"background-color:#0d0016;\">inplace</span></code><span style=\"background-color:#0d0016;\">操作被修改。网上的一些解决方法基本是检查模型定义中是否有</span><code><span style=\"background-color:#0d0016;\">inplace=True</span></code><span style=\"background-color:#0d0016;\"> 设置以及</span><code><span style=\"background-color:#0d0016;\">+=</span></code><span style=\"background-color:#0d0016;\">操作符。但是这两种方案都不能解决遇到的问题。</span></span></p>\n<p>经过一些调试发现，只有当某些特定情况下才会触发此报错。下面结合一个对比学习的例子（并不是完整的脚本）来简单描述：</p>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\n\nfrom torchvision.models import resnet50\n\ndef main():\n    model = resnet50(num_classes=256).cuda()\n    model = nn.parallel.DistributedDataParallel(model, \n                                                device_ids=[args.local_rank], \n                                                find_unused_parameters=True)\n    criterion = nn.MSELoss()\n    \n    optimizer = torch.optim.SGD(model.parameters(),\n                                lr=0.001,\n                                momentum=0.99,\n                                weight_decay=1e-4)\n\n    for i in range(10):\n        input0 = torch.randn((4, 3, 224, 224), dtype=torch.float32).cuda()\n        input2 = torch.randn((4, 3, 224, 224), dtype=torch.float32).cuda()\n\n        out1 = model(input0)\n        out2 = model(input1)\n\n        loss = criterion(out1, out2)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\nif __name__ == '__main__':\n    main()</code></pre>\n<p>经过调试发现，当使用<code>nn.DataParallel</code>并行训练或者单卡训练均可正常运行；另外如果将两次模型调用集成到model中，即通过<code>out1, out2 = model(input0, input1)</code> 的方式在分布式训练下也不会报错。</p>\n<p>由此可以猜测：<strong>在分布式训练中，如果对同一模型进行多次调用</strong>则会触发以上报错，即</p>\n<blockquote>\n<p><code>nn.parallel.DistributedDataParallel</code>方法封装的模型，<code>forword()</code>函数和<code>backward()</code>函数必须交替执行，如果执行多个（次）<code>forward()</code>然后执行一次<code>backward()</code>则会报错。</p>\n</blockquote>\n<p>那么解决此问题的入手点则可以聚焦到<code>nn.parallel.DistributedDataParallel</code>接口上。 通过查询PyTorch官方文档发现此接口下的两个参数：</p>\n<pre><code class=\"language-python\">- find_unused_parameters: 如果模型的输出有不需要进行反向传播的，此参数需要设置为True；\n若你的代码运行后卡住不动，基本上就是该参数的问题。\n\n- broadcast_buffers: 该参数默认为True，设置为True时，在模型执行forward之前，gpu0会把\nbuffer中的参数值全部覆盖到别的gpu上。\n\n</code></pre>\n<p>问题基本可以定位出来了，即<code>broadcast_buffers=True</code>导致参数被覆盖修改。</p>\n<h1>3、解决办法</h1>\n<pre><code class=\"language-python\"># 在该出错文件上找到被调用的DistributedDataParallel()，将broadcast_buffers设置为False\nmodel = nn.parallel.DistributedDataParallel(model, \n                                             device_ids=[args.local_rank], \n                                             broadcast_buffers=False,\n                                             find_unused_parameters=True)</code></pre>\n<p></p>\n<p>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2048]] is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</p>\n</div>\n</div>"}