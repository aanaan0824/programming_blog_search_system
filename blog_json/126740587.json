{"blogid": "126740587", "writerAge": "码龄2年", "writerBlogNum": "151", "writerCollect": "170", "writerComment": "48", "writerFan": "1068", "writerGrade": "5级", "writerIntegral": "1748", "writerName": "灰太狼的羊羊", "writerProfileAdress": "writer_image\\profile_126740587.jpg", "writerRankTotal": "11444", "writerRankWeekly": "3688", "writerThumb": "194", "writerVisitNum": "50350", "blog_read_count": "11", "blog_time": "于 2022-09-07 11:18:55 发布", "blog_title": "机器学习——线性判别分析", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<p id=\"main-toc\"><strong>目录</strong></p>\n<p id=\"-toc\" style=\"margin-left:0px;\"></p>\n<p id=\"%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%C2%A0-toc\" style=\"margin-left:0px;\"><a href=\"#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%C2%A0\">线性判别分析 </a></p>\n<p id=\"LDA%E7%9A%84%E9%99%8D%E7%BB%B4%E8%BF%87%E7%A8%8B-toc\" style=\"margin-left:40px;\"><a href=\"#LDA%E7%9A%84%E9%99%8D%E7%BB%B4%E8%BF%87%E7%A8%8B\">LDA的降维过程</a></p>\n<p id=\"%E6%A1%88%E4%BE%8B%EF%BC%9A%E9%B8%A2%E5%B0%BE%E8%8A%B1(Iris)-toc\" style=\"margin-left:40px;\"><a href=\"#%E6%A1%88%E4%BE%8B%EF%BC%9A%E9%B8%A2%E5%B0%BE%E8%8A%B1%28Iris%29\">案例：鸢尾花(Iris)</a></p>\n<p id=\"%E4%BB%A3%E7%A0%81%E6%BC%94%E7%A4%BA-toc\" style=\"margin-left:80px;\"><a href=\"#%E4%BB%A3%E7%A0%81%E6%BC%94%E7%A4%BA\">代码演示</a></p>\n<p id=\"%E6%95%B0%E6%8D%AE%E9%9B%86-toc\" style=\"margin-left:80px;\"><a href=\"#%E6%95%B0%E6%8D%AE%E9%9B%86\">数据集</a></p>\n<p id=\"%E5%B1%80%E9%83%A8%E7%BA%BF%E6%80%A7%E5%B5%8C%E5%85%A5-toc\" style=\"margin-left:40px;\"><a href=\"#%E5%B1%80%E9%83%A8%E7%BA%BF%E6%80%A7%E5%B5%8C%E5%85%A5\">局部线性嵌入</a></p>\n<hr id=\"hr-toc\"/>\n<p></p>\n<h1 id=\"%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%C2%A0\"><strong>线性判别分析</strong> </h1>\n<blockquote>\n<p><strong>线性判别分析</strong>（LDA）是一种<strong>有监督</strong>的<strong>线性降维算法</strong>。与PCA不同，LDA是为了使降维后的数据点尽可能地容易被区分。</p>\n</blockquote>\n<blockquote>\n<p><strong>线性判别分析</strong>（LDA）的原理是<strong>对于给定的训练集</strong>，设法将样本投影到一条直线上，使得同类的投影点尽可能接近，异类样本的投影点尽可能远离；在对新样本进行分类时，将其投影到这条直线上，再根据投影点的位置来确定新样本的类别。<strong>PCA主要是从特征的协方差角度</strong>，去找到比较好的投影方式。LDA更多地考虑了标注，即希望投影后不同类别之间数据点的距离更大，同一类别的数据点更紧凑</p>\n</blockquote>\n<h2 id=\"LDA%E7%9A%84%E9%99%8D%E7%BB%B4%E8%BF%87%E7%A8%8B\"><strong>LDA的降维过程</strong></h2>\n<p><br/> 1、计算数据集中每个类别下所有样本的<strong>均值向量</strong><br/> 2、通过<strong>均值向量</strong>，计算<strong>类间散布矩阵</strong>和<strong>类内散布矩阵</strong><br/> 3、依据公式进行<strong>特征值求解</strong>，计算的特征向量和特征值<br/> 4、按照<strong>特征值排序</strong>， 选择前k个特征向量构成投影矩阵U<br/> 5、通过的特征值矩阵将所有样本<strong>转换到新的子空间中</strong></p>\n<p><img alt=\"\" height=\"249\" src=\"image\\9be00d3c32504e759ac34a09cb3aea5e.png\" width=\"618\"/></p>\n<h2 id=\"%E6%A1%88%E4%BE%8B%EF%BC%9A%E9%B8%A2%E5%B0%BE%E8%8A%B1(Iris)\"><strong>案例：</strong>鸢尾花(Iris)</h2>\n<p>应用LDA技术对鸢尾花(Iris)的样本数据进行分析，鸢尾花数据集是20世纪30年代的经典数据集，它由Fisher收集整理，数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性。可通过<strong>花萼长度、花萼宽度、花瓣长度和花瓣宽度</strong>4个属性预测鸢尾花卉属于山鸢尾（Iris Setosa）、杂色鸢尾（Iris Versicolour）、维吉尼亚鸢尾（Iris Virginica）中的哪种类别，将类别文字转化为数字类别。</p>\n<p><img alt=\"\" height=\"86\" src=\"image\\7b800af5fc12415bb4ff96b278044d35.png\" width=\"646\"/></p>\n<p> 数据集中有4个特征，<strong>萼片长、萼片宽、花瓣长和花瓣宽</strong>，总共150行，每一行是一个样本，这就构成了一个4x150的输入矩阵，输出是1列，即花的类别，构成了1x150的矩阵。分析的目标就是通过LDA算法将输入<strong>矩阵映射到低维空间</strong>中进行分类。</p>\n<h3 id=\"%E4%BB%A3%E7%A0%81%E6%BC%94%E7%A4%BA\"><strong>代码演示</strong></h3>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\n\ndf = pd.read_csv('iris.data.txt', header=None)\ndf[4] = df[4].map({'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2})\n\nprint(df.tail())\n\ny, X = df.iloc[:, 4].values, df.iloc[:, 0:4].values\nX_cent = X - X.mean(axis=0)\nX_std = X_cent / X.std(axis=0)\n\nimport numpy as np\n\ndef comp_mean_vectors(X, y):\n    class_labels = np.unique(y)\n    n_classes = class_labels.shape[0]\n    mean_vectors = []\n    for cl in class_labels:\n        mean_vectors.append(np.mean(X[y==cl], axis=0))\n    return mean_vectors\n\ndef scatter_within(X, y):\n    class_labels = np.unique(y)\n    n_classes = class_labels.shape[0]\n    n_features = X.shape[1]\n    mean_vectors = comp_mean_vectors(X, y)\n    S_W = np.zeros((n_features, n_features))\n    for cl, mv in zip(class_labels, mean_vectors):\n        class_sc_mat = np.zeros((n_features, n_features))                 \n        for row in X[y == cl]:\n            row, mv = row.reshape(n_features, 1), mv.reshape(n_features, 1)\n            class_sc_mat += (row-mv).dot((row-mv).T)\n        S_W += class_sc_mat                           \n    return S_W\n\ndef scatter_between(X, y):\n    overall_mean = np.mean(X, axis=0)\n    n_features = X.shape[1]\n    print(\"n_features\",n_features)\n    mean_vectors = comp_mean_vectors(X, y)\n    S_B = np.zeros((n_features, n_features))\n    for i, mean_vec in enumerate(mean_vectors):\n        n = X[y==i+1,:].shape[0]\n        mean_vec = mean_vec.reshape(n_features, 1)\n        overall_mean = overall_mean.reshape(n_features, 1)\n        S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n    print(\"S_B:\",S_B)\n    return S_B\n\ndef get_components(eig_vals, eig_vecs, n_comp=1):\n    n_features = X.shape[1]\n    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n    eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n    print(\"eig_pairs:\",eig_pairs)\n    W = np.hstack([eig_pairs[i][1].reshape(4, 1) for i in range(0, n_comp)])\n    return W\n\n\nS_W, S_B = scatter_within(X, y), scatter_between(X, y)\nprint(S_W)\neig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\nW = get_components(eig_vals, eig_vecs, n_comp=2)\nprint('EigVals: %s\\n\\nEigVecs: %s' % (eig_vals, eig_vecs))\nprint('\\nW: %s' % W)\n\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nX_lda = X.dot(W)\nfor label,marker,color in zip(np.unique(y),('^', 's', 'o'),('blue', 'red', 'green')):\n    plt.scatter(X_lda[y==label, 0], X_lda[y==label, 1], c=color,edgecolors='black', marker=marker,s=640)\nplt.show()                                         #LD数\n\n</code></pre>\n<h3 id=\"%E6%95%B0%E6%8D%AE%E9%9B%86\"><strong>数据集</strong></h3>\n<pre><code class=\"language-python\">5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n5.0,3.6,1.4,0.2,Iris-setosa\n5.4,3.9,1.7,0.4,Iris-setosa\n4.6,3.4,1.4,0.3,Iris-setosa\n5.0,3.4,1.5,0.2,Iris-setosa\n4.4,2.9,1.4,0.2,Iris-setosa\n4.9,3.1,1.5,0.1,Iris-setosa\n5.4,3.7,1.5,0.2,Iris-setosa\n4.8,3.4,1.6,0.2,Iris-setosa\n4.8,3.0,1.4,0.1,Iris-setosa\n4.3,3.0,1.1,0.1,Iris-setosa\n5.8,4.0,1.2,0.2,Iris-setosa\n5.7,4.4,1.5,0.4,Iris-setosa\n5.4,3.9,1.3,0.4,Iris-setosa\n5.1,3.5,1.4,0.3,Iris-setosa\n5.7,3.8,1.7,0.3,Iris-setosa\n5.1,3.8,1.5,0.3,Iris-setosa\n5.4,3.4,1.7,0.2,Iris-setosa\n5.1,3.7,1.5,0.4,Iris-setosa\n4.6,3.6,1.0,0.2,Iris-setosa\n5.1,3.3,1.7,0.5,Iris-setosa\n4.8,3.4,1.9,0.2,Iris-setosa\n5.0,3.0,1.6,0.2,Iris-setosa\n5.0,3.4,1.6,0.4,Iris-setosa\n5.2,3.5,1.5,0.2,Iris-setosa\n5.2,3.4,1.4,0.2,Iris-setosa\n4.7,3.2,1.6,0.2,Iris-setosa\n4.8,3.1,1.6,0.2,Iris-setosa\n5.4,3.4,1.5,0.4,Iris-setosa\n5.2,4.1,1.5,0.1,Iris-setosa\n5.5,4.2,1.4,0.2,Iris-setosa\n4.9,3.1,1.5,0.1,Iris-setosa\n5.0,3.2,1.2,0.2,Iris-setosa\n5.5,3.5,1.3,0.2,Iris-setosa\n4.9,3.1,1.5,0.1,Iris-setosa\n4.4,3.0,1.3,0.2,Iris-setosa\n5.1,3.4,1.5,0.2,Iris-setosa\n5.0,3.5,1.3,0.3,Iris-setosa\n4.5,2.3,1.3,0.3,Iris-setosa\n4.4,3.2,1.3,0.2,Iris-setosa\n5.0,3.5,1.6,0.6,Iris-setosa\n5.1,3.8,1.9,0.4,Iris-setosa\n4.8,3.0,1.4,0.3,Iris-setosa\n5.1,3.8,1.6,0.2,Iris-setosa\n4.6,3.2,1.4,0.2,Iris-setosa\n5.3,3.7,1.5,0.2,Iris-setosa\n5.0,3.3,1.4,0.2,Iris-setosa\n7.0,3.2,4.7,1.4,Iris-versicolor\n6.4,3.2,4.5,1.5,Iris-versicolor\n6.9,3.1,4.9,1.5,Iris-versicolor\n5.5,2.3,4.0,1.3,Iris-versicolor\n6.5,2.8,4.6,1.5,Iris-versicolor\n5.7,2.8,4.5,1.3,Iris-versicolor\n6.3,3.3,4.7,1.6,Iris-versicolor\n4.9,2.4,3.3,1.0,Iris-versicolor\n6.6,2.9,4.6,1.3,Iris-versicolor\n5.2,2.7,3.9,1.4,Iris-versicolor\n5.0,2.0,3.5,1.0,Iris-versicolor\n5.9,3.0,4.2,1.5,Iris-versicolor\n6.0,2.2,4.0,1.0,Iris-versicolor\n6.1,2.9,4.7,1.4,Iris-versicolor\n5.6,2.9,3.6,1.3,Iris-versicolor\n6.7,3.1,4.4,1.4,Iris-versicolor\n5.6,3.0,4.5,1.5,Iris-versicolor\n5.8,2.7,4.1,1.0,Iris-versicolor\n6.2,2.2,4.5,1.5,Iris-versicolor\n5.6,2.5,3.9,1.1,Iris-versicolor\n5.9,3.2,4.8,1.8,Iris-versicolor\n6.1,2.8,4.0,1.3,Iris-versicolor\n6.3,2.5,4.9,1.5,Iris-versicolor\n6.1,2.8,4.7,1.2,Iris-versicolor\n6.4,2.9,4.3,1.3,Iris-versicolor\n6.6,3.0,4.4,1.4,Iris-versicolor\n6.8,2.8,4.8,1.4,Iris-versicolor\n6.7,3.0,5.0,1.7,Iris-versicolor\n6.0,2.9,4.5,1.5,Iris-versicolor\n5.7,2.6,3.5,1.0,Iris-versicolor\n5.5,2.4,3.8,1.1,Iris-versicolor\n5.5,2.4,3.7,1.0,Iris-versicolor\n5.8,2.7,3.9,1.2,Iris-versicolor\n6.0,2.7,5.1,1.6,Iris-versicolor\n5.4,3.0,4.5,1.5,Iris-versicolor\n6.0,3.4,4.5,1.6,Iris-versicolor\n6.7,3.1,4.7,1.5,Iris-versicolor\n6.3,2.3,4.4,1.3,Iris-versicolor\n5.6,3.0,4.1,1.3,Iris-versicolor\n5.5,2.5,4.0,1.3,Iris-versicolor\n5.5,2.6,4.4,1.2,Iris-versicolor\n6.1,3.0,4.6,1.4,Iris-versicolor\n5.8,2.6,4.0,1.2,Iris-versicolor\n5.0,2.3,3.3,1.0,Iris-versicolor\n5.6,2.7,4.2,1.3,Iris-versicolor\n5.7,3.0,4.2,1.2,Iris-versicolor\n5.7,2.9,4.2,1.3,Iris-versicolor\n6.2,2.9,4.3,1.3,Iris-versicolor\n5.1,2.5,3.0,1.1,Iris-versicolor\n5.7,2.8,4.1,1.3,Iris-versicolor\n6.3,3.3,6.0,2.5,Iris-virginica\n5.8,2.7,5.1,1.9,Iris-virginica\n7.1,3.0,5.9,2.1,Iris-virginica\n6.3,2.9,5.6,1.8,Iris-virginica\n6.5,3.0,5.8,2.2,Iris-virginica\n7.6,3.0,6.6,2.1,Iris-virginica\n4.9,2.5,4.5,1.7,Iris-virginica\n7.3,2.9,6.3,1.8,Iris-virginica\n6.7,2.5,5.8,1.8,Iris-virginica\n7.2,3.6,6.1,2.5,Iris-virginica\n6.5,3.2,5.1,2.0,Iris-virginica\n6.4,2.7,5.3,1.9,Iris-virginica\n6.8,3.0,5.5,2.1,Iris-virginica\n5.7,2.5,5.0,2.0,Iris-virginica\n5.8,2.8,5.1,2.4,Iris-virginica\n6.4,3.2,5.3,2.3,Iris-virginica\n6.5,3.0,5.5,1.8,Iris-virginica\n7.7,3.8,6.7,2.2,Iris-virginica\n7.7,2.6,6.9,2.3,Iris-virginica\n6.0,2.2,5.0,1.5,Iris-virginica\n6.9,3.2,5.7,2.3,Iris-virginica\n5.6,2.8,4.9,2.0,Iris-virginica\n7.7,2.8,6.7,2.0,Iris-virginica\n6.3,2.7,4.9,1.8,Iris-virginica\n6.7,3.3,5.7,2.1,Iris-virginica\n7.2,3.2,6.0,1.8,Iris-virginica\n6.2,2.8,4.8,1.8,Iris-virginica\n6.1,3.0,4.9,1.8,Iris-virginica\n6.4,2.8,5.6,2.1,Iris-virginica\n7.2,3.0,5.8,1.6,Iris-virginica\n7.4,2.8,6.1,1.9,Iris-virginica\n7.9,3.8,6.4,2.0,Iris-virginica\n6.4,2.8,5.6,2.2,Iris-virginica\n6.3,2.8,5.1,1.5,Iris-virginica\n6.1,2.6,5.6,1.4,Iris-virginica\n7.7,3.0,6.1,2.3,Iris-virginica\n6.3,3.4,5.6,2.4,Iris-virginica\n6.4,3.1,5.5,1.8,Iris-virginica\n6.0,3.0,4.8,1.8,Iris-virginica\n6.9,3.1,5.4,2.1,Iris-virginica\n6.7,3.1,5.6,2.4,Iris-virginica\n6.9,3.1,5.1,2.3,Iris-virginica\n5.8,2.7,5.1,1.9,Iris-virginica\n6.8,3.2,5.9,2.3,Iris-virginica\n6.7,3.3,5.7,2.5,Iris-virginica\n6.7,3.0,5.2,2.3,Iris-virginica\n6.3,2.5,5.0,1.9,Iris-virginica\n6.5,3.0,5.2,2.0,Iris-virginica\n6.2,3.4,5.4,2.3,Iris-virginica\n5.9,3.0,5.1,1.8,Iris-virginica\n\n</code></pre>\n<h2 id=\"%E5%B1%80%E9%83%A8%E7%BA%BF%E6%80%A7%E5%B5%8C%E5%85%A5\"><strong>局部线性嵌入</strong></h2>\n<blockquote>\n<p><strong>局部线性嵌入（LLE）</strong> 是一种非线性降维算法，它能够使降维后的数据较好地保持原有流形结构，每一个数据点都可以由其近邻点的线性加权组合构造得到。</p>\n<p><br/><strong>局部线性嵌入</strong>寻求数据的低维投影，保留本地邻域内的距离。它可以被认为是一系列局部主成分分析，被全局比较以找到最佳的非线性嵌入。</p>\n<p><br/><strong>算法的主要步骤分为三步</strong></p>\n<p><br/><strong>首先，</strong>寻找每个样本点的k个近邻点。</p>\n<p><br/><strong>然后</strong>，由每个样本点的近邻点计算出该样本点的局部重建权值矩阵。</p>\n<p><br/><strong>最后</strong>，由该样本点的局部重建权值矩阵和近邻点计算出该样本点的输出值。</p>\n<p><br/><strong>LLE</strong>在有些情况下也并不适用，例如数据分布在整个封闭的球面上，LLE则不能将它映射到二维空间，且不能保持原有的数据流形。因此在处理数据时，需要确保数据不是分布在用合的球面或者椭球面上。</p>\n</blockquote>\n<p><img alt=\"\" height=\"297\" src=\"image\\8f36b93070b24a90b7f1499534ccfa8b.png\" width=\"719\"/></p>\n<p> </p>\n<p></p>\n<p> </p>\n</div>\n</div>"}