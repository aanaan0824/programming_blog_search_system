{"blogid": "123504222", "writerAge": "码龄4年", "writerBlogNum": "192", "writerCollect": "2399", "writerComment": "457", "writerFan": "373", "writerGrade": "6级", "writerIntegral": "5757", "writerName": "白色小靴", "writerProfileAdress": "writer_image\\profile_123504222.jpg", "writerRankTotal": "34085", "writerRankWeekly": "117471", "writerThumb": "532", "writerVisitNum": "437301", "blog_read_count": "6073", "blog_time": "已于 2022-03-15 15:57:32 修改", "blog_title": "onnxruntime (C++/CUDA) 编译安装及部署", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"markdown_views prism-tomorrow-night\" id=\"content_views\">\n<svg style=\"display: none;\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M5,0 0,2.5 5,5z\" id=\"raphael-marker-block\" stroke-linecap=\"round\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"></path>\n</svg>\n<p>前几天使用了LibTorch对模型进行C++转换和测试，发现速度比原始Python的Pytorch模型提升了将近2倍。现在尝试以下另一种跨平台的模型转换方式——Onnx，可实现跨X86/ARM架构的迁移应用。</p>\n<p>本文主要介绍C++版本的onnxruntime使用，Python的操作较容易就不再提及了。</p>\n<h1><a id=\"_4\"></a>一、克隆及编译</h1>\n<pre><code class=\"prism language-bash\"><span class=\"token function\">git</span> clone --recursive https://github.com/Microsoft/onnxruntime\n<span class=\"token function\">cd</span> onnxruntime/\n<span class=\"token function\">git</span> checkout v1.8.0\n</code></pre>\n<p>这里建议checkout到旧tag，否则容易因为版本过新而编译失败，比如Cmake版本要求过高、CUDA版本不匹配等问题。若跟随网上其他教程，大概率会因为版本过新而导致后续编译失败。</p>\n<p>接下来编译：</p>\n<pre><code class=\"prism language-bash\">./build.sh --skip_tests --use_cuda --config Release --build_shared_lib --parallel --cuda_home /usr/local/cuda-11.3 --cudnn_home /usr/local/cuda-11.3\n</code></pre>\n<p>其中的<code>use_cuda</code>表示你要使用CUDA的onnxruntime，<code>cuda_home</code>和<code>cudnn_home</code>均指向你的CUDA安装目录即可。</p>\n<p>最后就编译成功了：</p>\n<pre><code class=\"prism language-bash\"><span class=\"token punctuation\">[</span>100%<span class=\"token punctuation\">]</span> Linking CXX executable onnxruntime_test_all\n<span class=\"token punctuation\">[</span>100%<span class=\"token punctuation\">]</span> Built target onnxruntime_test_all\n<span class=\"token punctuation\">[</span>100%<span class=\"token punctuation\">]</span> Linking CUDA shared module libonnxruntime_providers_cuda.so\n<span class=\"token punctuation\">[</span>100%<span class=\"token punctuation\">]</span> Built target onnxruntime_providers_cuda\n2022-03-15 13:49:03,260 util.run <span class=\"token punctuation\">[</span>DEBUG<span class=\"token punctuation\">]</span> - Subprocess completed. Return code: 0\n2022-03-15 13:49:03,260 build <span class=\"token punctuation\">[</span>INFO<span class=\"token punctuation\">]</span> - Build complete\n</code></pre>\n<h1><a id=\"onnxruntime_29\"></a>二、onnxruntime使用实例</h1>\n<h2><a id=\"21_onnx_31\"></a>2.1 onnx模型保存</h2>\n<p>重点关注<code>example</code>的设置（输入）和如何<code>export</code>出onnx模型。</p>\n<pre><code class=\"prism language-bash\"><span class=\"token function\">import</span> numpy as np\nfrom modules.feature_extracter_without_delta_layer <span class=\"token function\">import</span> featureExtracter\n\n\ncheckpoint <span class=\"token operator\">=</span> torch.load<span class=\"token punctuation\">(</span><span class=\"token string\">\"./amodel.pth.tar\"</span><span class=\"token punctuation\">)</span>\namodel <span class=\"token operator\">=</span> featureExtracter<span class=\"token punctuation\">(</span>channels<span class=\"token operator\">=</span>1<span class=\"token punctuation\">)</span>\namodel.load_state_dict<span class=\"token punctuation\">(</span>checkpoint<span class=\"token punctuation\">[</span><span class=\"token string\">'state_dict'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> \namodel.cuda<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\namodel.eval<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nexample <span class=\"token operator\">=</span> torch.rand<span class=\"token punctuation\">(</span>1, 1, 32, 900<span class=\"token punctuation\">)</span>\nexample <span class=\"token operator\">=</span> example.cuda<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ntorch.onnx.export<span class=\"token punctuation\">(</span>amodel,\n                 <span class=\"token punctuation\">(</span>example<span class=\"token punctuation\">)</span>,\n                 <span class=\"token string\">'overlapTransformer.onnx'</span>,\n                 input_names <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'input'</span><span class=\"token punctuation\">]</span>,\n                 output_names <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'output'</span><span class=\"token punctuation\">]</span>,\n                 opset_version<span class=\"token operator\">=</span>11,\n                 verbose <span class=\"token operator\">=</span> True<span class=\"token punctuation\">)</span>\n</code></pre>\n<h2><a id=\"22_onnxC_56\"></a>2.2 onnx模型的C++调用</h2>\n<p>其中的<code>cuda_options</code>与<code>AppendExecutionProvider_CUDA</code>的联合使用保证了CUDA加速。</p>\n<pre><code class=\"prism language-bash\"><span class=\"token comment\">#include &lt;iostream&gt;</span>\n<span class=\"token comment\">#include &lt;vector&gt;</span>\n<span class=\"token comment\">#include &lt;chrono&gt;</span>\n<span class=\"token comment\">#include &lt;string&gt;</span>\n<span class=\"token comment\">#include &lt;vector&gt;</span>\n<span class=\"token comment\">#include &lt;onnxruntime_cxx_api.h&gt;</span>\n\nusing namespace std<span class=\"token punctuation\">;</span>\n\n\nint main<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{<!-- --></span>\n    Ort::Env env<span class=\"token punctuation\">(</span>ORT_LOGGING_LEVEL_WARNING, <span class=\"token string\">\"test\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    Ort::SessionOptions session_options<span class=\"token punctuation\">;</span>\n\n    OrtCUDAProviderOptions cuda_options<span class=\"token punctuation\">{<!-- --></span>\n          0,\n          OrtCudnnConvAlgoSearch::EXHAUSTIVE,\n          std::numeric_limits<span class=\"token operator\">&lt;</span>size_t<span class=\"token operator\">&gt;</span>::max<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>,\n          0,\n          <span class=\"token boolean\">true</span>\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span>\n\n    session_options.AppendExecutionProvider_CUDA<span class=\"token punctuation\">(</span>cuda_options<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    const char* model_path <span class=\"token operator\">=</span> <span class=\"token string\">\"./overlapTransformer.onnx\"</span><span class=\"token punctuation\">;</span>\n\n    int width <span class=\"token operator\">=</span> 900<span class=\"token punctuation\">;</span>\n    int height <span class=\"token operator\">=</span> 32<span class=\"token punctuation\">;</span>\n    int len_arr <span class=\"token operator\">=</span> width*height<span class=\"token punctuation\">;</span>\n    float virtual_image<span class=\"token punctuation\">[</span>len_arr<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>int i<span class=\"token operator\">=</span>0<span class=\"token punctuation\">;</span> i<span class=\"token operator\">&lt;</span>height<span class=\"token punctuation\">;</span> i++<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>int j<span class=\"token operator\">=</span>0<span class=\"token punctuation\">;</span> j<span class=\"token operator\">&lt;</span>width<span class=\"token punctuation\">;</span> j++<span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">{<!-- --></span>\n            virtual_image<span class=\"token punctuation\">[</span>int<span class=\"token punctuation\">(</span>i*width+j<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> 1<span class=\"token punctuation\">;</span> // range\n        <span class=\"token punctuation\">}</span>\n\n\n    Ort::Session session<span class=\"token punctuation\">(</span>env, model_path, session_options<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    // print model input layer <span class=\"token punctuation\">(</span>node names, types, shape etc.<span class=\"token punctuation\">)</span>\n    Ort::AllocatorWithDefaultOptions allocator<span class=\"token punctuation\">;</span>\n\n    // print number of model input nodes\n    size_t num_input_nodes <span class=\"token operator\">=</span> session.GetInputCount<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    std::vector<span class=\"token operator\">&lt;</span>const char*<span class=\"token operator\">&gt;</span> input_node_names <span class=\"token operator\">=</span> <span class=\"token punctuation\">{<!-- --></span><span class=\"token string\">\"input\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span>\n    std::vector<span class=\"token operator\">&lt;</span>const char*<span class=\"token operator\">&gt;</span> output_node_names <span class=\"token operator\">=</span> <span class=\"token punctuation\">{<!-- --></span><span class=\"token string\">\"output\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span>\n\n    std::vector<span class=\"token operator\">&lt;</span>int64_t<span class=\"token operator\">&gt;</span> input_node_dims <span class=\"token operator\">=</span> <span class=\"token punctuation\">{<!-- --></span>1, 1, 32, 900<span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span>\n    size_t input_tensor_size <span class=\"token operator\">=</span> 32 * 900<span class=\"token punctuation\">;</span>\n    std::vector<span class=\"token operator\">&lt;</span>float<span class=\"token operator\">&gt;</span> input_tensor_values<span class=\"token punctuation\">(</span>input_tensor_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>unsigned int i <span class=\"token operator\">=</span> 0<span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;</span> input_tensor_size<span class=\"token punctuation\">;</span> i++<span class=\"token punctuation\">)</span>\n        input_tensor_values<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> float<span class=\"token punctuation\">(</span>virtual_image<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    // create input tensor object from data values ！！！！！！！！！！\n    auto memory_info <span class=\"token operator\">=</span> Ort::MemoryInfo::CreateCpu<span class=\"token punctuation\">(</span>OrtArenaAllocator, OrtMemTypeDefault<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    Ort::Value input_tensor <span class=\"token operator\">=</span> Ort::Value::CreateTensor<span class=\"token operator\">&lt;</span>float<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">(</span>memory_info, input_tensor_values.data<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>,\n                                                            input_tensor_size, input_node_dims.data<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>, 4<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    std::vector<span class=\"token operator\">&lt;</span>Ort::Value<span class=\"token operator\">&gt;</span> ort_inputs<span class=\"token punctuation\">;</span>\n    ort_inputs.push_back<span class=\"token punctuation\">(</span>std::move<span class=\"token punctuation\">(</span>input_tensor<span class=\"token punctuation\">))</span><span class=\"token punctuation\">;</span>\n\n    auto output_tensors <span class=\"token operator\">=</span> session.Run<span class=\"token punctuation\">(</span>Ort::RunOptions<span class=\"token punctuation\">{<!-- --></span>nullptr<span class=\"token punctuation\">}</span>, input_node_names.data<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>, ort_inputs.data<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>,\n                                    ort_inputs.size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>, output_node_names.data<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>, 1<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    float* floatarr <span class=\"token operator\">=</span> output_tensors<span class=\"token punctuation\">[</span>0<span class=\"token punctuation\">]</span>.GetTensorMutableData<span class=\"token operator\">&lt;</span>float<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>int i<span class=\"token operator\">=</span>0<span class=\"token punctuation\">;</span> i<span class=\"token operator\">&lt;</span>256<span class=\"token punctuation\">;</span> i++<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">{<!-- --></span>\n        std::cout<span class=\"token operator\">&lt;&lt;</span>floatarr<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token operator\">&lt;&lt;</span>std::endl<span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n\n    <span class=\"token keyword\">return</span> 0<span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span>\n</code></pre>\n<p>相关教程，可供参考：<br/> https://github.com/microsoft/onnxruntime<br/> https://blog.csdn.net/znsoft/article/details/114583048<br/> https://ask.csdn.net/questions/7619412<br/> https://blog.csdn.net/XCCCCZ/article/details/110356437</p>\n</div>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-22a2fefd3b.css\" rel=\"stylesheet\"/>\n<link href=\"https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-4f8fbf9108.css\" rel=\"stylesheet\"/>\n</div>"}