{"blogid": "126743174", "writerAge": "码龄39天", "writerBlogNum": "169", "writerCollect": "3", "writerComment": "1", "writerFan": "70", "writerGrade": "5级", "writerIntegral": "1860", "writerName": "幸福的小肥熊", "writerProfileAdress": "writer_image\\profile_126743174.jpg", "writerRankTotal": "14133", "writerRankWeekly": "9012", "writerThumb": "0", "writerVisitNum": "16332", "blog_read_count": "11", "blog_time": "于 2022-09-07 12:27:27 发布", "blog_title": "神经网络图像效果图解析,图神经网络 图像处理", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<h2>如何用visio画卷积神经网络图。图形类似下图所示</h2>\n<p>。</p>\n<p>大概试了一下用visio绘制这个图，除了最左面的变形图片外其余基本可以实现（那个图可以考虑用其它图像处理软件比如Photoshop生成后插入visio），visio中主要用到的图形可以在更多形状-常规-具有透视效果的块中找到块图形，拖入绘图区后拉动透视角度调节的小红点进行调整直到合适为止，其余的块可以按住ctrl+鼠标左键进行拉动复制，然后再进行大小、位置仔细调整就可以了，大致绘出图形示例如下图所示：</p>\n<p><strong>谷歌人工智能写作项目：神经网络伪原创</strong></p>\n<p><img alt=\"\" src=\"image\\1343ebe46ee142b89e7d10fa2aee3b80.png\"/></p>\n<h2>如何通过人工神经网络实现图像识别</h2>\n<p><strong><a href=\"http://www.maoxiezuo.com/\" title=\"写作猫\">写作猫</a></strong>。</p>\n<p>人工神经网络（ArtificialNeuralNetworks）（简称ANN）系统从20世纪40年代末诞生至今仅短短半个多世纪，但由于他具有信息的分布存储、并行处理以及自学习能力等优点，已经在信息处理、模式识别、智能控制及系统建模等领域得到越来越广泛的应用。</p>\n<p>尤其是基于误差反向传播（ErrorBackPropagation）算法的多层前馈网络（Multiple-LayerFeedforwardNetwork）(简称BP网络)，可以以任意精度逼近任意的连续函数，所以广泛应用于非线性建模、函数逼近、模式分类等方面。</p>\n<p>目标识别是模式识别领域的一项传统的课题，这是因为目标识别不是一个孤立的问题，而是模式识别领域中大多数课题都会遇到的基本问题，并且在不同的课题中，由于具体的条件不同，解决的方法也不尽相同，因而目标识别的研究仍具有理论和实践意义。</p>\n<p>这里讨论的是将要识别的目标物体用成像头(红外或可见光等)摄入后形成的图像信号序列送入计算机，用神经网络识别图像的问题。</p>\n<p>一、BP神经网络BP网络是采用Widrow-Hoff学习算法和非线性可微转移函数的多层网络。一个典型的BP网络采用的是梯度下降算法，也就是Widrow-Hoff算法所规定的。</p>\n<p>backpropagation就是指的为非线性多层网络计算梯度的方法。一个典型的BP网络结构如图所示。我们将它用向量图表示如下图所示。</p>\n<p>其中：对于第k个模式对，输出层单元的j的加权输入为该单元的实际输出为而隐含层单元i的加权输入为该单元的实际输出为函数f为可微分递减函数其算法描述如下：（1）初始化网络及学习参数，如设置网络初始权矩阵、学习因子等。</p>\n<p>（2）提供训练模式，训练网络，直到满足学习要求。（3）前向传播过程：对给定训练模式输入，计算网络的输出模式，并与期望模式比较，若有误差，则执行（4）；否则，返回（2）。</p>\n<p>（4）后向传播过程：a.计算同一层单元的误差；b.修正权值和阈值；c.返回（2）二、BP网络隐层个数的选择对于含有一个隐层的三层BP网络可以实现输入到输出的任何非线性映射。</p>\n<p>增加网络隐层数可以降低误差，提高精度，但同时也使网络复杂化，增加网络的训练时间。误差精度的提高也可以通过增加隐层结点数来实现。一般情况下，应优先考虑增加隐含层的结点数。</p>\n<p>三、隐含层神经元个数的选择当用神经网络实现网络映射时，隐含层神经元个数直接影响着神经网络的学习能力和归纳能力。</p>\n<p>隐含层神经元数目较少时，网络每次学习的时间较短，但有可能因为学习不足导致网络无法记住全部学习内容；隐含层神经元数目较大时，学习能力增强，网络每次学习的时间较长，网络的存储容量随之变大，导致网络对未知输入的归纳能力下降，因为对隐含层神经元个数的选择尚无理论上的指导，一般凭经验确定。</p>\n<p>四、神经网络图像识别系统人工神经网络方法实现模式识别，可处理一些环境信息十分复杂，背景知识不清楚，推理规则不明确的问题，允许样品有较大的缺损、畸变，神经网络方法的缺点是其模型在不断丰富完善中，目前能识别的模式类还不够多，神经网络方法允许样品有较大的缺损和畸变，其运行速度快，自适应性能好，具有较高的分辨率。</p>\n<p>神经网络的图像识别系统是神经网络模式识别系统的一种，原理是一致的。一般神经网络图像识别系统由预处理，特征提取和神经网络分类器组成。预处理就是将原始数据中的无用信息删除，平滑，二值化和进行幅度归一化等。</p>\n<p>神经网络图像识别系统中的特征提取部分不一定存在，这样就分为两大类：①有特征提取部分的：这一类系统实际上是传统方法与神经网络方法技术的结合，这种方法可以充分利用人的经验来获取模式特征以及神经网络分类能力来识别目标图像。</p>\n<p>特征提取必须能反应整个图像的特征。但它的抗干扰能力不如第2类。</p>\n<p>②无特征提取部分的：省去特征抽取，整副图像直接作为神经网络的输入，这种方式下，系统的神经网络结构的复杂度大大增加了，输入模式维数的增加导致了网络规模的庞大。</p>\n<p>此外，神经网络结构需要完全自己消除模式变形的影响。但是网络的抗干扰性能好，识别率高。当BP网用于分类时，首先要选择各类的样本进行训练，每类样本的个数要近似相等。</p>\n<p>其原因在于一方面防止训练后网络对样本多的类别响应过于敏感，而对样本数少的类别不敏感。另一方面可以大幅度提高训练速度，避免网络陷入局部最小点。</p>\n<p>由于BP网络不具有不变识别的能力，所以要使网络对模式的平移、旋转、伸缩具有不变性，要尽可能选择各种可能情况的样本。</p>\n<p>例如要选择不同姿态、不同方位、不同角度、不同背景等有代表性的样本，这样可以保证网络有较高的识别率。</p>\n<p>构造神经网络分类器首先要选择适当的网络结构：神经网络分类器的输入就是图像的特征向量；神经网络分类器的输出节点应该是类别数。隐层数要选好，每层神经元数要合适，目前有很多采用一层隐层的网络结构。</p>\n<p>然后要选择适当的学习算法，这样才会有很好的识别效果。</p>\n<p>在学习阶段应该用大量的样本进行训练学习，通过样本的大量学习对神经网络的各层网络的连接权值进行修正，使其对样本有正确的识别结果，这就像人记数字一样，网络中的神经元就像是人脑细胞，权值的改变就像是人脑细胞的相互作用的改变，神经网络在样本学习中就像人记数字一样，学习样本时的网络权值调整就相当于人记住各个数字的形象，网络权值就是网络记住的内容，网络学习阶段就像人由不认识数字到认识数字反复学习过程是一样的。</p>\n<p>神经网络是按整个特征向量的整体来记忆图像的，只要大多数特征符合曾学习过的样本就可识别为同一类别，所以当样本存在较大噪声时神经网络分类器仍可正确识别。</p>\n<p>在图像识别阶段，只要将图像的点阵向量作为神经网络分类器的输入，经过网络的计算，分类器的输出就是识别结果。五、仿真实验1、实验对象本实验用MATLAB完成了对神经网络的训练和图像识别模拟。</p>\n<p>从实验数据库中选择0～9这十个数字的BMP格式的目标图像。图像大小为16×8像素，每个目标图像分别加10％、20％、30％、40％、50％大小的随机噪声，共产生60个图像样本。</p>\n<p>将样本分为两个部分，一部分用于训练，另一部分用于测试。实验中用于训练的样本为40个，用于测试的样本为20个。随机噪声调用函数randn(m,n)产生。</p>\n<p>2、网络结构本试验采用三层的BP网络，输入层神经元个数等于样本图像的象素个数16×8个。隐含层选24个神经元，这是在试验中试出的较理想的隐层结点数。</p>\n<p>输出层神经元个数就是要识别的模式数目，此例中有10个模式，所以输出层神经元选择10个，10个神经元与10个模式一一对应。</p>\n<p>3、基于MATLAB语言的网络训练与仿真建立并初始化网络% ================S1 = 24;% 隐层神经元数目S1 选为24[R,Q] = size(numdata);[S2,Q] = size(targets);F = numdata;P=double(F);net = newff(minmax(P),[S1 S2],{'logsig''logsig'},'traingda','learngdm')这里numdata为训练样本矩阵，大小为128×40，targets为对应的目标输出矩阵，大小为10×40。</p>\n<p>newff(PR,[S1S2…SN],{TF1TF2…TFN}，BTF,BLF,PF)为MATLAB函数库中建立一个N层前向BP网络的函数，函数的自变量PR表示网络输入矢量取值范围的矩阵[Pminmax];S1~SN为各层神经元的个数；TF1~TFN用于指定各层神经元的传递函数；BTF用于指定网络的训练函数；BLF用于指定权值和阀值的学习函数；PF用于指定网络的性能函数，缺省值为‘mse’。</p>\n<p>设置训练参数net.performFcn = 'sse'; %平方和误差性能函数 = 0.1; %平方和误差目标 = 20; %进程显示频率net.trainParam.epochs = 5000;%最大训练步数 = 0.95; %动量常数网络训练net=init(net);%初始化网络[net,tr] = train(net,P,T);％网络训练对训练好的网络进行仿真D=sim(net,P);A = sim(net,B);B为测试样本向量集,128×20的点阵。</p>\n<p>D为网络对训练样本的识别结果，A为测试样本的网络识别结果。实验结果表明：网络对训练样本和对测试样本的识别率均为100％。如图为64579五个数字添加50%随机噪声后网络的识别结果。</p>\n<p>六、总结从上述的试验中已经可以看出，采用神经网络识别是切实可行的，给出的例子只是简单的数字识别实验，要想在网络模式下识别复杂的目标图像则需要降低网络规模，增加识别能力，原理是一样的。</p>\n<h2>卷积神经网络怎么生成图片？</h2>\n<p>需要使用类似GAN的生成模型去做。望采纳GAN的基本原理其实非常简单，这里以生成图片为例进行说明。假设我们有两个网络，G（Generator）和D（Discriminator）。</p>\n<p>正如它的名字所暗示的那样，它们的功能分别是：G是一个生成图片的网络，它接收一个随机的噪声z，通过这个噪声生成图片，记做G(z)。D是一个判别网络，判别一张图片是不是“真实的”。</p>\n<p>它的输入参数是x，x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。</p>\n<p>在训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量把G生成的图片和真实的图片分别开来。这样，G和D构成了一个动态的“博弈过程”。请点击输入图片描述。</p>\n<h2>CNN（卷积神经网络）是什么？</h2>\n<p>在数字图像处理的时候我们用卷积来滤波是因为我们用的卷积模版在频域上确实是高通低通带通等等物理意义上的滤波器。</p>\n<p>然而在神经网络中，模版的参数是训练出来的，我认为是纯数学意义的东西，很难理解为在频域上还有什么意义，所以我不认为神经网络里的卷积有滤波的作用。接着谈一下个人的理解。</p>\n<p>首先不管是不是卷积神经网络，只要是神经网络，本质上就是在用一层层简单的函数（不管是sigmoid还是Relu）来拟合一个极其复杂的函数，而拟合的过程就是通过一次次backpropagation来调参从而使代价函数最小。</p>\n<h2>深度学习与神经网络有什么区别</h2>\n<p>深度学习与神经网络关系2017-01-10最近开始学习深度学习，基本上都是zouxy09博主的文章，写的蛮好，很全面，也会根据自己的思路，做下删减，细化。</p>\n<p>五、DeepLearning的基本思想假设我们有一个系统S，它有n层（S1,…Sn），它的输入是I，输出是O，形象地表示为：I=&gt;S1=&gt;S2=&gt;…..=&gt;Sn=&gt;O，如果输出O等于输入I，即输入I经过这个系统变化之后没有任何的信息损失（呵呵，大牛说，这是不可能的。</p>\n<p>信息论中有个“信息逐层丢失”的说法（信息处理不等式），设处理a信息得到b，再对b处理得到c，那么可以证明：a和c的互信息不会超过a和b的互信息。这表明信息处理不会增加信息，大部分处理会丢失信息。</p>\n<p>当然了，如果丢掉的是没用的信息那多好啊），保持了不变，这意味着输入I经过每一层Si都没有任何的信息损失，即在任何一层Si，它都是原有信息（即输入I）的另外一种表示。</p>\n<p>现在回到我们的主题DeepLearning，我们需要自动地学习特征，假设我们有一堆输入I（如一堆图像或者文本），假设我们设计了一个系统S（有n层），我们通过调整系统中参数，使得它的输出仍然是输入I，那么我们就可以自动地获取得到输入I的一系列层次特征，即S1，…,Sn。</p>\n<p>对于深度学习来说，其思想就是对堆叠多个层，也就是说这一层的输出作为下一层的输入。通过这种方式，就可以实现对输入信息进行分级表达了。</p>\n<p>另外，前面是假设输出严格地等于输入，这个限制太严格，我们可以略微地放松这个限制，例如我们只要使得输入与输出的差别尽可能地小即可，这个放松会导致另外一类不同的DeepLearning方法。</p>\n<p>上述就是DeepLearning的基本思想。六、浅层学习（ShallowLearning）和深度学习（DeepLearning）浅层学习是机器学习的第一次浪潮。</p>\n<p>20世纪80年代末期，用于人工神经网络的反向传播算法（也叫BackPropagation算法或者BP算法）的发明，给机器学习带来了希望，掀起了基于统计模型的机器学习热潮。这个热潮一直持续到今天。</p>\n<p>人们发现，利用BP算法可以让一个人工神经网络模型从大量训练样本中学习统计规律，从而对未知事件做预测。这种基于统计的机器学习方法比起过去基于人工规则的系统，在很多方面显出优越性。</p>\n<p>这个时候的人工神经网络，虽也被称作多层感知机（Multi-layerPerceptron），但实际是种只含有一层隐层节点的浅层模型。</p>\n<p>20世纪90年代，各种各样的浅层机器学习模型相继被提出，例如支撑向量机（SVM，SupportVectorMachines）、Boosting、最大熵方法（如LR，LogisticRegression）等。</p>\n<p>这些模型的结构基本上可以看成带有一层隐层节点（如SVM、Boosting），或没有隐层节点（如LR）。这些模型无论是在理论分析还是应用中都获得了巨大的成功。</p>\n<p>相比之下，由于理论分析的难度大，训练方法又需要很多经验和技巧，这个时期浅层人工神经网络反而相对沉寂。深度学习是机器学习的第二次浪潮。</p>\n<p>2006年，加拿大多伦多大学教授、机器学习领域的泰斗GeoffreyHinton和他的学生RuslanSalakhutdinov在《科学》上发表了一篇文章，开启了深度学习在学术界和工业界的浪潮。</p>\n<p>这篇文章有两个主要观点：1）多隐层的人工神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；2）深度神经网络在训练上的难度，可以通过“逐层初始化”（layer-wisepre-training）来有效克服，在这篇文章中，逐层初始化是通过无监督学习实现的。</p>\n<p>当前多数分类、回归等学习方法为浅层结构算法，其局限性在于有限样本和计算单元情况下对复杂函数的表示能力有限，针对复杂分类问题其泛化能力受到一定制约。</p>\n<p>深度学习可通过学习一种深层非线性网络结构，实现复杂函数逼近，表征输入数据分布式表示，并展现了强大的从少数样本集中学习数据集本质特征的能力。</p>\n<p>（多层的好处是可以用较少的参数表示复杂的函数）深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。</p>\n<p>因此，“深度模型”是手段，“特征学习”是目的。</p>\n<p>区别于传统的浅层学习，深度学习的不同在于：1）强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；2）明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。</p>\n<p>与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。</p>\n<p>七、Deeplearning与NeuralNetwork深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。</p>\n<p>深度学习是无监督学习的一种。深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。</p>\n<p>Deeplearning本身算是machinelearning的一个分支，简单可以理解为neuralnetwork的发展。</p>\n<p>大约二三十年前，neuralnetwork曾经是ML领域特别火热的一个方向，但是后来确慢慢淡出了，原因包括以下几个方面：1）比较容易过拟合，参数比较难tune，而且需要不少trick；2）训练速度比较慢，在层次比较少（小于等于3）的情况下效果并不比其它方法更优；所以中间有大约20多年的时间，神经网络被关注很少，这段时间基本上是SVM和boosting算法的天下。</p>\n<p>但是，一个痴心的老先生Hinton，他坚持了下来，并最终（和其它人一起Bengio、Yann.lecun等）提成了一个实际可行的deeplearning框架。</p>\n<p>Deeplearning与传统的神经网络之间有相同的地方也有很多不同。</p>\n<p>二者的相同在于deeplearning采用了神经网络相似的分层结构，系统由包括输入层、隐层（多层）、输出层组成的多层网络，只有相邻层节点之间有连接，同一层以及跨层节点之间相互无连接，每一层可以看作是一个logisticregression模型；这种分层结构，是比较接近人类大脑的结构的。</p>\n<p>而为了克服神经网络训练中的问题，DL采用了与神经网络很不同的训练机制。</p>\n<p>传统神经网络（这里作者主要指前向神经网络）中，采用的是backpropagation的方式进行，简单来讲就是采用迭代的算法来训练整个网络，随机设定初值，计算当前网络的输出，然后根据当前输出和label之间的差去改变前面各层的参数，直到收敛（整体是一个梯度下降法）。</p>\n<p>而deeplearning整体上是一个layer-wise的训练机制。</p>\n<p>这样做的原因是因为，如果采用backpropagation的机制，对于一个deepnetwork（7层以上），残差传播到最前面的层已经变得太小，出现所谓的gradientdiffusion（梯度扩散）。</p>\n<p>这个问题我们接下来讨论。</p>\n<p>八、Deeplearning训练过程8.1、传统神经网络的训练方法为什么不能用在深度神经网络BP算法作为传统训练多层网络的典型算法，实际上对仅含几层网络，该训练方法就已经很不理想。</p>\n<p>深度结构（涉及多个非线性处理单元层）非凸目标代价函数中普遍存在的局部最小是训练困难的主要来源。</p>\n<p>BP算法存在的问题：（1）梯度越来越稀疏：从顶层越往下，误差校正信号越来越小；（2）收敛到局部最小值：尤其是从远离最优区域开始的时候（随机值初始化会导致这种情况的发生）；（3）一般，我们只能用有标签的数据来训练：但大部分的数据是没标签的，而大脑可以从没有标签的的数据中学习；8.2、deeplearning训练过程如果对所有层同时训练，时间复杂度会太高；如果每次训练一层，偏差就会逐层传递。</p>\n<p>这会面临跟上面监督学习中相反的问题，会严重欠拟合（因为深度网络的神经元和参数太多了）。</p>\n<p>2006年，hinton提出了在非监督数据上建立多层神经网络的一个有效方法，简单的说，分为两步，一是每次训练一层网络，二是调优，使原始表示x向上生成的高级表示r和该高级表示r向下生成的x'尽可能一致。</p>\n<p>方法是：1）首先逐层构建单层神经元，这样每次都是训练一个单层网络。2）当所有层训练完后，Hinton使用wake-sleep算法进行调优。</p>\n<p>将除最顶层的其它层间的权重变为双向的，这样最顶层仍然是一个单层神经网络，而其它层则变为了图模型。向上的权重用于“认知”，向下的权重用于“生成”。然后使用Wake-Sleep算法调整所有的权重。</p>\n<p>让认知和生成达成一致，也就是保证生成的最顶层表示能够尽可能正确的复原底层的结点。</p>\n<p>比如顶层的一个结点表示人脸，那么所有人脸的图像应该激活这个结点，并且这个结果向下生成的图像应该能够表现为一个大概的人脸图像。Wake-Sleep算法分为醒（wake）和睡（sleep）两个部分。</p>\n<p>1）wake阶段：认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层间的下行权重（生成权重）。</p>\n<p>也就是“如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的”。2）sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。</p>\n<p>也就是“如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念”。</p>\n<p>deeplearning训练过程具体如下：1）使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）：采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作是一个无监督训练过程，是和传统神经网络区别最大的部分（这个过程可以看作是featurelearning过程）：具体的，先用无标定数据训练第一层，训练时先学习第一层的参数（这一层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），由于模型capacity的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第n-1层后，将n-1层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数；2）自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）：基于第一步得到的各层参数进一步fine-tune整个多层模型的参数，这一步是一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于DL的第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果；所以deeplearning效果好很大程度上归功于第一步的featurelearning过程。</p>\n<h2>BP神经网络训练生成的图片解释，急求。</h2>\n<p>那这张呢，到了最大迭代次数了，可是还是收敛不到指定的精度。出现的情况就是像图上一样，均方误差达到0.00128左右的时候就无法继续下去了，误差梯度总是反复，先下降，一会又缩回去了。</p>\n<p>即使我把迭代次数设置到10000次均方误差也就稳定在0.00128左右了，主要是误差梯度总是不停的反复，这是为什么呢？是收敛失败吗？</p>\n<p> </p>\n</div>\n</div>"}