{"blogid": "126674046", "writerAge": "码龄130天", "writerBlogNum": "43", "writerCollect": "720", "writerComment": "216", "writerFan": "734", "writerGrade": "3级", "writerIntegral": "769", "writerName": "人工智能算法研究院", "writerProfileAdress": "writer_image\\profile_126674046.jpg", "writerRankTotal": "20775", "writerRankWeekly": "4031", "writerThumb": "99", "writerVisitNum": "61182", "blog_read_count": "560", "blog_time": "已于 2022-09-03 10:55:34 修改", "blog_title": "YOLOv7改进之二十五：引入Swin Transformer", "content": "<div class=\"article_content clearfix\" id=\"article_content\">\n<link href=\"style.css\" rel=\"stylesheet\"/>\n<div class=\"htmledit_views\" id=\"content_views\">\n<p><strong> ​前 言：</strong>作为当前先进的深度学习目标检测算法YOLOv7，已经集合了大量的trick，但是还是有提高和改进的空间，针对具体应用场景下的检测难点，可以不同的改进方法。此后的系列文章，将重点对YOLOv7的如何改进进行详细的介绍，目的是为了给那些搞科研的同学需要创新点或者搞工程项目的朋友需要达到更好的效果提供自己的微薄帮助和参考。由于出到YOLOv7，YOLOv5算法2020年至今已经涌现出大量改进论文，这个不论对于搞科研的同学或者已经工作的朋友来说，研究的价值和新颖度都不太够了，为与时俱进，以后改进算法以YOLOv7为基础，此前YOLOv5改进方法在YOLOv7同样适用，所以继续YOLOv5系列改进的序号。另外改进方法在YOLOv5等其他算法同样可以适用进行改进。希望能够对大家有帮助。</p>\n<p><strong>具体改进办法请关注后私信留言！</strong></p>\n<p><strong>解决问题：</strong></p>\n<p>目前Transformer应用到图像领域主要有两大挑战：</p>\n<ul><li>视觉实体变化大，在不同场景下视觉Transformer性能未必很好</li><li>图像分辨率高，像素点多，Transformer基于全局自注意力的计算导致计算量较大</li></ul>\n<p>针对上述两个问题，我们提出了一种<strong>包含滑窗操作，具有层级设计</strong>的Swin Transformer。</p>\n<p>其中滑窗操作包括<strong>不重叠的local window，和重叠的cross-window</strong>。将注意力计算限制在一个窗口中，<strong>一方面能引入CNN卷积操作的局部性，另一方面能节省计算量</strong></p>\n<p><strong>原理：</strong></p>\n<p>       将 Transformer 从语言适应到视觉方面的挑战来自 两个域之间的差异，例如视觉实体的规模以及相比于文本单词的高分辨率图像像素的巨大差异。为解决这些差异，我们提出了一种 层次化 (hierarchical) Transformer，其表示是用 移位窗口 (Shifted Windows) 计算的。移位窗口方案通过 将自注意力计算限制在不重叠的局部窗口的同时，还允许跨窗口连接来提高效率。这种分层架构具有在各种尺度上建模的灵活性，并且 相对于图像大小具有线性计算复杂度。Swin Transformer 的这些特性使其与广泛的视觉任务兼容，包括图像分类（ImageNet-1K 的 87.3 top-1 Acc）和 密集预测任务，例如 目标检测（COCO test dev 的 58.7 box AP 和 51.1 mask AP）和语义分割（ADE20K val 的 53.5 mIoU）。它的性能在 COCO 上以 +2.7 box AP 和 +2.6 mask AP 以及在 ADE20K 上 +3.2 mIoU 的大幅度超越了 SOTA 技术，证明了基于 Transformer 的模型作为视觉主干的潜力。分层设计和移位窗口方法也证明了其对全 MLP 架构是有益的。 </p>\n<p></p>\n<p><img alt=\"\" height=\"274\" src=\"image\\d052452ac4594c65bb2c930a967ab87f.png\" width=\"888\"/></p>\n<p> 项目部分代码如下：</p>\n<pre><code>class SwinTransformer(nn.Module):\n    def __init__(...):\n        super().__init__()\n        ...\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n            \n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(...)\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x</code></pre>\n<p><strong>结 果：</strong>本人暂时由于比较忙，暂时还未做实验，该改进确实可行的话，可以尝试投稿sci期刊。需要请关注留言。</p>\n<p><strong>预告一下：下一篇内容将继续分享深度学习算法相关改进方法。</strong>有兴趣的朋友可以关注一下我，有问题可以留言或者私聊我哦</p>\n<p>PS：该方法不仅仅是适用改进YOLOv5，也可以改进其他的YOLO网络以及目标检测网络，比如YOLOv7、v6、v4、v3，Faster rcnn ，ssd等。</p>\n<p>最后，希望能互粉一下，做个朋友，一起学习交流。</p>\n</div>\n</div>"}